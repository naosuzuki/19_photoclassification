%%% Notice: This file contains a large number of \verb's 
%%%         or verbatim environments in order to display command names
%%%         or examples.  But the use of \verb/verbatim is *not* recommended. 
%%% ver.6 2015/01/05 
\documentclass[proof]{pasj01}
\let\square\relax
%\documentclass[useamsfonts]{pasj01}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{comment}
%\usepackage{color}
%\newcommand{\mt}{\textcolor{blue}}

%
%\draft 
\Received{$\langle$reception date$\rangle$}
\Accepted{$\langle$acception date$\rangle$}
\Published{$\langle$publication date$\rangle$}
%% \SetRunningHead{Astronomical Society of Japan}{Usage of \texttt{pasj00.cls}}

\begin{document}

\title{Photometric classification of the HSC transients through machine learning}
\author{
Ichiro \textsc{Takahashi}\altaffilmark{1,2,*},
Nao \textsc{Suzuki}\altaffilmark{1},
Naoki \textsc{Yasuda}\altaffilmark{1},
Akisato \textsc{Kimura}\altaffilmark{3},
Naonori \textsc{Ueda}\altaffilmark{3},
Masaomi \textsc{Tanaka}\altaffilmark{4,1},
Nozomu \textsc{Tominaga}\altaffilmark{5,1},
and
Naoki \textsc{Yoshida}\altaffilmark{1,2,6,7}
%
}%
\altaffiltext{1}{Kavli Institute for the Physics and Mathematics of the Universe (WPI), The University of Tokyo Institutes for Advanced Study, The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba 277-8583, Japan}
\altaffiltext{2}{CREST, JST, 4-1-8 Honcho, Kawaguchi, Saitama 332-0012, Japan}
\altaffiltext{3}{NTT Communication Science Laboratories, 2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan}
\altaffiltext{4}{Astronomical Institute, Tohoku University, Aoba, Sendai, Miyagi 980-8578, Japan}
\altaffiltext{5}{Department of Physics, Faculty of Science and Engineering, Konan University, 8-9-1 Okamoto, Kobe, Hyogo 658-8501, Japan}
\altaffiltext{6}{Department of Physics, Graduate School of Science, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan}
\altaffiltext{7}{Institute for Physics of Intelligence, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan}

\email{ichiro.takahashi@ipmu.jp}

\KeyWords{supernovae: general --- methods: statistical --- surveys}

\maketitle
%
\begin{abstract}
The progress of observation technology in recent years has brought the rapid increase in the number of discovered supernovae (SNe). In total of 1824 SN candidates from the transient survey, conducted from 2016 to 2017, with the Subaru Hyper Suprime-Cam (HSC). In order to select follow-up candidates efficiently among SNe discovered in the survey, we study type classification of SNe using Deep Neural Network with Highway layers. Our classifier receives photometric data directly, without interpolating them or extracting features. The evaluation of our classifier with simulated classification challenge dataset for LSST yields the performance as area under the curve (AUC) of 0.996 in binary classification, and accuracy of 95.3\% in three-class classification respectively. When we apply this classifier to the observation data of HSC-SSP Transient Survey, the binary classification AUC score is 0.925 for SNe labeled by the light curve fitting using SALT2. We also investigated the dependence of the classification performance on the number of photometric points in HSC observation, and found out the followings: 1) The performance of type classification improves as input increases, and accuracy exceeds 0.8 at 22 photometric points. 2) When considered based on the SN phase, our classifier can classify with 78.1\% accuracy by inputting two weeks of data from the first detection. These results and findings show that our classifier has sufficient classification performance to select the follow-up targets even in the actual survey.
\end{abstract}

\section{Introduction}
Time domain science becomes one of the major fields of astronomical study today.  The discovery of accelerating universe \citep{perlmutter99a,riess98a} evoked a series of large supernova (SN) surveys in the last decades 
\citep{betoule14a,scolnic18a,brout19a}.  
These surveys revealed that there exist a whole new family of transients and the search for unknown populations is of great interest today \citep{howell06a,phillips07a,quimby07b}. 
For the precision cosmology, it is very important to keep the purity of Type Ia supernova (SN~Ia) while having uniform sampling from bright to faint SNe~Ia.  Spectroscopic follow-up has been essential to distinguish faint SN~Ia from Type Ib supernova (SN~Ib) and Type Ic supernova (SN~Ic) which have similar light curve behavior \citep{scolnic14a}.   A large amount of precious telescope time is dedicated to the follow-up programs, and we would like to make an efficient use of those telescopes.

The scale of survey is getting larger, and it becomes impossible to trigger spectroscopic follow-up for all of the candidates in real time, and we are in need of developing new classification scheme.  It is a natural path to perform photometric classification \citep{sako11a,jonesl8a}.  With the rise of Machine Learning Technologies, it is getting common that an astronomical big data is being analyzed through machine learning.   

Neural Network has been used for photometric redshift studies from the early stage.  Today, many Machine Learning methods are applied to the photometric redshift studies \citep{collister04a,carliles10a,pasquet19a}.
Deep Neural Network (DNN) is now being used for imaging data for finding strong lens system \citep{petrillo17a} or for galaxy morphology classifications \citep{hausen19a}.  Now, machine learning is being introduced for transient survey for detection \citep{goldstein15a} and classification \citep{charnock17a}.

In this paper, we introduce our attempt to apply machine learning (DNN) to the actual transient survey data.
Hyper Suprime-Cam (HSC) \citep{miyazaki18a,Komiyama2018,kawanomoto18a,Furusawa2018} on Subaru Telescope enables us to probe wide filed (1.77 deg$^2$ Field of View) and deep space (26th mag in $i-$band / epoch).  Our primary scientific goals are SN~Ia cosmology, Type II supernova (SN~II) cosmology, super-luminous supernova (SLSN) studies as well as probing unknown population of transients.  
As is reported in \citet{yasuda19a}, more than 1800 SNe are discovered in a 6-month campaign. 
We deployed a machine learning (AUC boosting) for transient detection \citep{morii16a}, where 'real' vs. 'bogus' is determined by a machine.
In \citet{Kimura17}, we adopted DNN for SN type classification from 2 dimensional image, and highway block is introduced for the optimization of layers.
This paper is an extension of \citet{Kimura17} and applies DNN to the photometric classification of transients.   
Our unique attempt is to make use of the observed data as 'raw' as possible so that we can directly plug them into machine without any fitting to the data or extracting characteristics.

The structure of this paper is as follows. We introduce our methods in section 2, and data in section 3. The design of our DNN model and its application to pseudo-real LSST simulation data is described in section 4. Section 5 shows the application of our model to the actual Subaru/HSC data. We discuss the results in section 6 and conclusion is given in section 7.
%
%

\section{Methods}
\subsection{Tasks in HSC survey}
\label{sec:tasks}
Subaru HSC Transient survey is a part of Subaru Strategic Project (SSP) which is a five-year program with 300 dark nights \citep{aihara18a,miyazaki18a}.
HSC-SSP Transient Survey is composed of two seasons, and the first season is executed for a consecutive 6 months from November 2016 through April 2017.
HSC is mounted on the prime focus for 2 weeks in dark time.   
Weather permitted, we aim to observe 2 data points per filter per lunation.
A detailed survey strategies and the observing logs are reported in \citet{yasuda19a}, and we use the observed photometric data described in \citet{yasuda19a}.

One of our primary goals of HSC-SSP Transient Survey is SN~Ia Cosmology which aims to perform the most precise measurement of dark energy at high-redshift and test if dark energy is changing in time or not \citep{linder03b}.
We are awarded 96 orbits of the Hubble Space Telescope time (WFC3 Camera) to execute a precise photometry on IR at the time of maximum.
Our HST program uses non-disrupted ToO which means we send the observing request by Wednesday, 2 weeks prior to the observation.
In other words, we are in need of finding good candidates 2-3 weeks prior to the maximum.  High-redshift (z$>$1) time dilation factor helps, but it is always a challenge to identify SN~Ia on the rise.

Our international collaboration team executes spectroscopic follow-up using large telescopes in the world.  
Our target, high-redshift SN~Ia, is faint ($\sim$ 24th mag in $i-$band) for spectroscopic identification even with the powerful large telescopes: GMOS Gemini \citep{hook04a}, GTC, Keck LRIS \citep{oke95a}, VLT FORS, Subaru FOCAS \citep{kashikawa02a} and AAT AAOMega Spectrograph.  
Thus, it is critical to hit the target at the time of maximum either by ToO (GTC), queue mode (VLT) or classical scheduled observation (Keck, Subaru, ATT).

For SN~Ia, it takes about 18 days from its explosion to the maximum in rest frame \citep{conley06a,papadogiannakis19a}.   Given the fact, we are looking at high-redshift SN~Ia (z$>$1), we have about a month in observed frame for a SN~Ia from explosion to the maximum.   However, our task is to identify them 2 weeks prior to the maximum, meaning we have only 2 data points per a filter.   In addition to that, sky condition keeps changing, and we may not have the data as we originally had planned.   In reality, our identification has to be done through missing data points on the rise.

In parallel to SN~Ia Cosmology program, our sister projects also needs identification and classification of HSC transient.   
SN~II cosmology program needs a timely spectroscopic follow-up to measure the expansion velocity of photosphere from H$\beta$ line \citep{dejaeger17a}.
SLSN is of great interest today, it is relatively a rare event\citep{quimby11a} and its mechanism has quite a diversity \citep{galyam12a,Moriya18SLSN}, and it can probe high redshift universe \citep{cooke12a}.    
For SLSN project, timely spectroscopic follow-up is also critical \citep{moriya19a,curtin19a}.

Early phase SN~Ia gives us clues on explosion mechanism \citep{maeda18a} and progenitors \citep{cao15a}, and it is a hot topic to study.
HSC has an advantage of surveying a large volume and has revealed long standing theory prediction of Helium-shell detonation \citep{Jiang2017}.
Finding early phase SN~Ia is not trivial but HSC is yielding a new set of early phase SN~Ia \citep{Jiang_2020}.
Observations of the early phase Core-Collapse SN provides us with a crucial information on the size of the progenitors \citep{thompson03a,tominaga11a} and Circumstellar Medium \citep{forster18a}.  

%Shock Breakout lasts only for a few $\sim$ several hours at the time of explosion and it becomes as luminous as at the time of the maximum \citep{gezari08a}. 
%Shock Breakout contains the information on projenitor radius \citep{nakar10a}, and HSC is expected to find these candidates as well \citep{tominaga11a}.
%
%Tidal Disruption Event \citep[TDE]{gezari08b,holoien14a} is also expected to be observed by HSC-SSP Transient Survey.  The light curve is similar to
%the supernova, but it monotonically drops at a specific rate \citep[$t^{-5/3}$]{lodato11a} and the event happens on the core of the galaxy.  
%We expect to have a few events per season \citep{kochanek16a}.
%
%It is always possible to discover something new when we explore a new parameter space which is the depth for the case
%of Subaru HSC.   Rapid transients are discovered by HSC, but their identities are yet to be known (Tampo et al.  2020 in prep).  
%Similar objects are also reported by DES team \citep{pursianinen18a}.
%
%In many science cases, it is essential to have a timely classification at the early phase of transient so that we can trigger follow-up observations.
%\citep{maeda18a,jiang18a}
%\citep{moriya19b}
%
%

\subsection{Classification method for HSC-SSP transient survey}
We designed two models of machine learning with emphasis on identifying SN~Ia which requires time sensitive trigger for HST IR follow-up.
One model is in a binary mode and classifies if a transient is SN~Ia or not.
It has been known that the majority of high-redshift transients are SN~Ia, and we seek for other unknown transients from the ones labeled as non-SN~Ia.   
The other model classifies a transient into 3 classes: SN~Ia, SN~Ibc and SN~II.  
We choose three classes for simplicity and in fact, the majority of SNe goes to one of these three categories.
SN~Ia is a thermonuclear explosion, and its brightness can be calibrated empirically.
SN~Ib, SN~Ic and SN~II are all Core-Collapse SNe and classified by the spectral features \citep{filippenko97a}.
The light curves of SN~Ib and SN~Ic are similar to that of SN~Ia and it always contaminates SN~Ia cosmology program, and together we call them SN~Ibc.  They are fainter than SN~Ia and redder in general.
One of our challenges in this paper is if we can segregate SN~Ibc from SN~Ia.
%
%

\section{Data}
In this section, we present dataset we use for our study.
We first show our SN datset from HSC-SSP transient survey (subsection \ref{sec:hscdata}).
Then we describe simulated photometric data to train the machine (subsection \ref{sec:training}).
Lastly, we explained the pre-processing of the above data input to the machine (subsection \ref{sec:preproc}).

\subsection{Observed data from Subaru/HSC-SSP transient survey}
\label{sec:hscdata}
The goal of this project is to classify Subaru/HSC observed light curves. 
The discovery of 1824 SNe from a six-month HSC-SSP Transient Survey in the period of
November 2016 through April 2017 are reported and described in \citet{yasuda19a}.
The survey is composed of Ultra-Deep (UD) and Deep layer in COSMOS \citep{scoville07a}.
The median 5$\sigma$ limiting magnitudes per epoch are 26.4, 26.3, 26.0, 25.6, and 24.6 mag (AB)
for $g$-, $r2$-, $i2$-, $z$- and $y$-band respectively for UD. 
For Deep layer, the depth is 0.6 mag shallower.

The SN dataset consists of time series photometric data (flux, magnitude and their errors) in each band for each SN.
Because part of the $y$ band photometric data has residuals due to improper background subtraction influenced by scattered light \citep{aihara18dr},
we exclude the $y$ band data in our study, considering the impact on classification performance.

%redshift info
The redshift information for HSC SNe is combination of our follow-up observation results and catalogs from multiple surveys of those host galaxies.
The spectral redshifts (spec-z) are adopted from the results of the follow-up spectrum observations by AAT/AAOmega performed in 2018 and that from DEIMOS \citep{DEIMOS2018}, FMOS-COSMOS \citep{FMOS-COSMOS2015}, C3R2 \citep{C3R2_2017}, PRIMUS \citep{PRIMUS2011} and COSMOS catalogs.
For those without spec-z, the photometric redshifts (photo-z) are adopted from the COSMOS catalog and those calculated from the HSC-SSP survey data \citep{HSCSSP_photo-z2018}.



\subsection{Simulated data for training}
\label{sec:training}
We are in need of simulating observed photometric data to train the machine.  
For normal SN~Ia, we use SALT2 \citep{guy10b} model ({\it ver} 2.4) which requires 2 input parameters: c for color and x1 for stretch.
We adopt the asymmetric Gaussian distribution of c, and x1 from \citet{mosher14a}, and generate
light curves and simulate photometric data points based on the observation schedule. 
Besides SN~Ia, we use spectral time series from \citet{kessler19b} which contains both observed Core-Collapse SN data and light curves from simulation.
We mix equal number of SN~Ia and Non-SN~Ia to approximate the observed fractions. 
These fraction does not need to be exact for our purpose of classification, but it is important
that the minority is not buried in the majority.
For three class classification, we set the ratio of SN~Ia:SN~Ibc:SN~II=10:3:7.
The redshfit distribution of galaxies is taken from COSMOS survey \citep{laigle16a}, and we distributed simulated SN accordingly from $z$=0.1 through $z=$2.0.
Throughout this paper, we use $\Lambda$CDM cosmology with $\Omega_{m}$=0.3 and h=0.7.
We use filter response including system throughput from \citet{kawanomoto18a}. 
Examples of simulated photometric data is shown in figure \ref{fig:simLCsamples}.

One complication in simulating realistic data is that the machine does not accept expected errors, or we have not identified a good method to include errors.   
For this study, we take a path of brute force, namely, we place expected photometric error on top of the simulated data so that the simulated data point behaves like one of the many realizations.
For HSC simulation, the size of error varies night to night due to sky condition. 
We measure and derive flux vs. error relationship from the actual observed data at simulating epoch and apply that relationship to the simulated photometric data.
%For LSST simulation, we find the flux-error relation from the Deep Drilling Field (DDF) of the one used in Kaggle Competition \citep{malz19a} and applied that to the simulated photometric data.

We have tested how many light curves are needed for training.  We use 'accuracy' (subsection \ref{hyperparametersearch}) as a guide on how many light curves to use.  
Based on the our convergence test, we conclude we need to generate more than 100,000 light curves for training as shown figure \ref{fig:size_convergence_test}.
We dropped ones which has less than 3$\sigma$ detection at the maximum since our detection criterion is 5$\sigma$. %Prior to the modeling, we did the convergence test
For training, 514,954 light curves are generated for HSC observed data. 
Their class ratio is SN~Ia:Ibc:II=0.59:0.07:0.34, and the peak timings of them are randomly shifted by 450 days.
%
%
\begin{figure}[htbp]
  \begin{center}
     %\includegraphics[width=\columnwidth]{figures/SimLCsamples.eps}
     \includegraphics[width=130mm]{figures/SimLCsamples.eps}
  \end{center}
  \vspace{-6mm}
  \caption{%
  Overlay plots of simulated light curves with z between 0.1 and 1.2.
  Each panel shows the case of Ia, Ibc, II from the left, and $g$-, $r2$-, $i2$-, $z$-, $y$-band from the bottom.
  No noise component is added to these light curves.
  }%
  
  \label{fig:simLCsamples}
\end{figure}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/size_accuracy.eps}
  \end{center}
  \caption{%
A convergence test: We tested how many light curves needs to be generated to train a machine. 
The solid line shows the mean accuracy of five classifiers. The shade area shows the standard deviation of the classifiers. The classifiers are trained with 5 fold cross validation using the training dataset.  We conclude we need more than 100,000 light curves for training.
  }%
  \label{fig:size_convergence_test}
\end{figure}
%

%Non-Ia light curves are generated based on the PLAsTiCC template and using redshift as a parameter.
%The number of Ia and non Ia light curves has been adjusted to a ratio of 1:1.
%The ratio of Ibc and II is set to Ibc:II=3:7 with reference to that of the simulated dataset for PLAsTiCC \citep{plasticc_models}.
%In other words, the numbers of the three classes have a ratio of Ia:Ibc:II=0.5:0.15:0.35.
%The selection weights of individual SED time-series models in each of the Ibc and II classes are also matched to those in the PLAsTiCC dataset.
%The redshift is selected in the range of 0.1 to 2.0, and the event rate depends on the number distribution of galaxies in the COSMOS2015 catalog \citep{2016ApJS..224...24L}.
%In addition, the timing at which SNs occur is shifted by randomly setting an offset that indicates the difference between the SN phase and the observation schedule.
%Of all the light curves that are finally created, faint events with a maximum signal-to-noise ratio of less than three are rejected.
%
\subsection{Preprocessing of input data}
\label{sec:preproc}
%We found that in experiments using simulation data, it is better to be input x follow as,
Based on our pre-experiment with simulated dataset, we found that the machines performs best if we 
chose a combination of normalized flux ($f$) and pseudo-absolute magnitude ($M$):
\begin{equation}
    x = \left( M_1^\mathrm{abs}, \ldots, M_P^\mathrm{abs}, f_{1}^{\mathrm{scale}}, \ldots, f_{P}^{\mathrm{scale}} \right)^T,
\end{equation}
where $f_{i}^{\mathrm{scale}}$ is $i$-th raw observed flux normalized by its maximum flux:
\begin{equation}
    f_{i}^{\mathrm{scale}} = \frac{f_i}{\max \left(f_1, \ldots, f_P \right)},    \label{eq:scaled_flux}
\end{equation}
and $M_i^\mathrm{abs}$ is $i$-th pseudo-absolute observed magnitude.
For a simplicity, we ignore K-correction and use distance modulus (DM(z)) based on $\Lambda$CDM with photometric redshift
from its host galaxy.
\begin{eqnarray}
    M_i^\mathrm{abs} = m_i - \mathrm{DM}\left(z\right),
\end{eqnarray}
We can justify this operation because we treat the training set and the observed dataset in the same way.
It there existed K-correction offset, both dataset would experience in the same way.
%where $m_i$ is i-th magnitude and $\mathrm{DM}\left(z\right)$ is distance modulus of the redshift value $z$.
In addition, since the observed flux could go to a negative value due to statistical fluctuation, 
we adopt hyperbolic sine to imitate the magnitude system we use \citep{lupton99a}.  
%the calculation of the magnitude was dealt with using Luptitude (Lupton magnitude). %simplified Lupton magnitude 
\begin{eqnarray}
    m_i = 27.0 - \frac{2.5}{\log 10} \sinh^{-1} \frac{f_i}{2}. \label{eq:mag} 
\end{eqnarray}
In fact, the combination of the flux and magnitudes are redundant.   If we know one, we can calculate the other
in an explicit way.   
However, based on our experiment, the score of machine gets better if we use both.  
We suspect the distribution in flux (linear) and magnitude space (log) are different, 
and it gives an extra clue to the machine.
Thus, we use pseudo-absolute magnitude ($M$) and normalized flux ($f$) as an input.

\section{Deep neural network classifier}
\label{sec:DNN}
On the rise of the era of Big Data, the use of machine learning technique plays a critical role on the analysis of astronomical data.  Techniques such as Random Forest, Support Vector Machine, and Convolution Neural Network have been used for photometric data analysis \citep{pasquet19a}, galaxy classifications \citep{hausen19a} and spectral 
classifications \citep{garciadias18a,muthukrishna19c,sharma20a}.

We are in need of classifying SN from photometric data.
We attempt to make use of the observed data as it is without parameterization.
It is the Deep Learning that makes this attempt possible.
We test how good deep learning can do without extracting features such as color, light curve width and the peak magnitude on this paper.
%It has been proven among PLAsTiCC competition that a combination of features can perform very well \citep{boone19a}, but it needs the data augmentation, interpolation or extrapolation to extract features.

We push one step further and use the observed data as raw as possible, meaning our input is a simple array of magnitudes.  Such attempt would not be possible ten years ago, but thanks for the advancement of computing and deep learning technique, now it becomes a reality.  Among many other machine learning methods, deep neural network is our choice which enables us to classify astronomical objects from the raw observed data. 

\subsection{Model design}
\label{sec:model} %added by IT
In this section, we describe our design of DNN model
\footnote{The code for our model is available at $\langle$https://github.com/ichiro-takahashi/snclass$\rangle$.}
which takes an array of observed magnitudes as an input and outputs SN classification with probabilities.  We adopted Highway layer \citep{srivastava15a} as a core part of our network which is also called 'layer in layer'.  Compared to the plain DNN, Highway layer can perform better when the network is deep in terms of parameter optimization \citep{srivastava15b}.   
Just like other DNN models, this model goes through training, validation process to optimize parameters, and we describe the steps below.  The terminology we use is common in the world of DNN, but this is a new introduction to astronomical community, we spell out each step one by one.  The architecture of our model is summarized in figure \ref{fig:dnn_model}.
At the end, each SN is given a probability of being a certain astrophysical class, in our case, SN types. 
%
\begin{figure}[htbp]
  \begin{center}
     %\includegraphics[width=\columnwidth]{figures/model_all.eps}
     \includegraphics[width=130mm]{figures/model_all.eps}
  \end{center}
  \caption{\label{dnnmodel}
  The architecture of the deep neural network classifier. 
  The green boxes are parameters which are optimized by gradient descent method during training. The red boxes are hyper-parameters which are optimized by the hyper-parameter search. 
  Batch Normalization layer has four variables ($\mu, \sigma^2, \gamma, \beta$). $\mu$ and $\sigma^2$ are to learn the statistics (mean and variance) of the value through the layer respectively.  $\gamma$ and $\beta$ are scale parameter and shift parameter to adjust the output. $\mu$ and $\sigma^2$ are not updated by gradient descent method but are updated by moving average. We omit them from the figure for simplicity.
  }%
  \label{fig:dnn_model}
\end{figure}
%


{\bf Input}: Our input is an array of magnitudes and normalized fluxes of the $i$th SN in the training data set:
\begin{equation}
      x_i = \left( M_{i1}, M_{i2}, \ldots M_{ij} \ldots , M_{iN}, f_{i1}, f_{i2}, \ldots, f_{iN} \right)^T
\end{equation}
We do not use time or filter explicitly but it is recorded as an order inside the array.  The philosophy here is that the training set which is composed of the simulated data in the same array length holds the information on filter and dates.  For example, the $j$th magnitude in the array is the data taken on a certain date and by a certain filter.  The combination of the date and filter is identical to the ones in the training set.  Therefore, the $j$th component implicitly contains unique information about dates and filters.  
%{\bf Memo Distance Modulus \& Normalized Flux needs to be written for preprocessing}
%Based on our experiments (table \ref{tab:p_test}), the combination of absolute magnitude and normalized flux performs best, and therefore, our
If input is a combination of magnitude and normalized flux,
our input array has a size of $1\times2N$ per a SN where $N$ is the number of data points.

%The DNN model receives the total of P brightnesses observed in multiple bands as an input and outputs $C$ values, where $C$ is the number of classes in classification task, and $C = 1$ in regression task.

{\bf First Fully Connected layer:}
We would like to make use of $D$ neurons which is also known as the number of 'hidden layer', and $D$ is greater than the number of input components ($2N$).  However, the dimension $D$ is not known in advance, and this parameter is one of the hyperparameters we optimize later in this section.  Since the dimension of input ($2N$) and the number of optimized neurons $D$ could be different, we are in need of adjusting the dimensions and that is the role of this First Fully Connected layer $F(x)$.    
\begin{equation}
    F \left(x, \left\{W_{fc1},b_{fc1}\right\}\right) = W_{fc1} x + b_{fc1} \in \mathbb{R}^D.
\end{equation}
$F(x)$ is given by a linear combination of matrix $W_{fc} \in \mathbb{R}^{D\times 2N}$ and a vector $b_{fc} \in \mathbb{R}^D$. The initial value of $W_{fc}$ is generated by Gaussian distribution and $b_{fc}$ is initialized by $\mathbf{0}$, which is a $D$ dimentional vector and the all elements are zero.  We use a python wrapper library called {\it dm-sonnet} and its function called {\it linear} supplies the $F(x)$ when we plug in '$2N$' and '$D$'.  
%The initial value of the matrix $W_{fc}$ is generated by Gaussian distribution and b is initialized by zero. 
Later on, $W_{fc}$ and $b_{fc}$ are optimised by an open source Machine Learning package, {\it Tensorflow}.  Unless we mention otherwise, we use libraries from {\it Tensorflow}.

{\bf Dropout layer:}
In order to have a robust result, it is always best to train all of the neurons as an ensemble and avoid one of the neurons drags the result.  Dropout is a process which randomly drops some of the neurons from the training and the rate of dropout can be optimized as one of the hyperparameters \citep{dropout}. 

{\bf Highway layer:}
We adopted Highway layer \citep{srivastava15a} and the number of layers is optimized through hyperparameter search.   In theory, we can design a very deep layer and the number of layers can be more than we need.   In fact, it is not trivial to find the optimized number of layers.   
%In this study, we adopted Highway Layer \citep{highway} as the basic part of the network structure.
The depth and/or layer size of the DNN model is directly related to the complexity of the features that we input, and greatly affect the performance of the task.  
However, if a model is too deep, the learning process becomes difficult and causes performance degradation.
%There are many studies to solve the problem such as optimization, initialization, and network architecture.
Highway layer is a technique that stabilizes learning process by devising the network structure.
In \citet{Kimura17}, we have used Highway layer and tested on 2D image, and it proved a good performance.  The details of the use and the advantages of Highway layer is described in \citet{Kimura17}.  We continue to adopt Highway layer scheme for this analysis.
%\begin{equation}
%Highway\left(h\right) = H\left(h\right) \otimes T \left(h\right) + h \otimes C\left(h\right)
%\end{equation}
%Highway layer, shown in Fig.\ref{fig:dnn_model}, has multiple paths from the input to the output.
The output of Highway layer is calculated from the values of the multiple paths.
The output, ${\bf Highway} \left(x\right)$, is formulated as
\begin{equation}
    {\bf Highway} \left(x\right) = G \left(x\right) \otimes H \left(x\right) + C \left(x\right) \otimes x \in \mathbb{R}^D,
\end{equation}
where $H$ is a non-linear transformation layer. $G$ is the transformation gate function layer and controls the transformation of input.  $C$ is the carry gate function layer. The symbol, $\otimes$, is element-wise product, also known as Hadamard product.
%The two gate functions $T$ and $C$ control how much of the outputs which are transforming the input and carrying it, respectively.
%One gate functions, $T$, controls the transformation of input and the other gate function, $C$, controls  
Highway layer includes other layers inside.  This structure is called layer in layer.  Each function is defined as follows: 
\begin{eqnarray}
    H \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_{H_{fc}}, b_{H_{fc}}\right\}\right) \right) \in \mathbb{R}^D, \\
    G \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_{G_{fc}}, b_{G_{fc}}\right\}\right) \right) \in \mathbb{R}^D, \\
    C \left(x\right) &=& \mathbf{1} - G \left(x\right) \in \mathbb{R}^D,
\end{eqnarray}
where $a$ is an activation function, namely sigmoid.
\begin{eqnarray*}
    \mathrm{a} \left(p\right) &=& \left( \sigma\left(p_1\right),\sigma\left(p_2\right), \ldots, \sigma\left(p_D\right) \right)^T, \; p \in \mathbb{R}^D, \\
    \sigma \left(p_i\right) &=& \frac{1}{1 + e^{-p_i}},
\end{eqnarray*}
Note $D$ is the number of neurons.  Each element of $G(x)$ always takes a value between 0 and 1.  At the end values, Highway layer behaves as follows:
\begin{equation}
    {\bf Highway(x)}=\left\{
    \begin{array}{@{}ll@{}}
    %\begin{cases}
      x, & \mathrm{if} \ G \left(x\right)= \mathbf{0} \\
      H(x), & \mathrm{if} \ G \left(x\right)= \mathbf{1} 
    %\end{cases}
    \end{array}\right.
\end{equation}
%We define a highway block which is composed of three layers: Highway Layer, Batch Normalization Layer and Activation Layer.
%and adjusted the size of the network by changing the number of highway blocks.
%The highway block is composed of a maximum of three layers including a highway layer as shown in figure \ref{fig:dnn_model}.
%However, layers other than Highway layer are determined by a hyperparameter search described later.
%This block itself is laid multiple times.
%and this number of blocks $N_{bloack}$ is one of the hyperparameters and optimized later. 
%The optimal structure of the network was determined by hyperparameter search.
Along with the hidden layer dimensions $D$, the dropout ratio, batch normalization, and the types of activation function, the number of repetition $T$ is one of the hyperparameters and optimized through hyperparameter search. The details are described in subsection \ref{hyperparametersearch}.
%The subjects of hyperparameter search are the number of highway blocks $N_{block}$, the number of hidden layer dimensions $d$ of highway blocks, the dropout ratio, the presence of batch normalization, and the type of activation function.

{\bf Batch Normalization layer:}
We adopt Batch Normalization \citep{batch_norm} to accelerate and stabilize the optimization. Even if it may be a large number of parameters we need to train, Batch Normalization helps converge the training, reduce errors on the slope when we apply entropy minimization, prevents the average and dispersion goes exponentially large in deep layers, and minimize the biases on outputs \citep{understanding_batch_norm}.
Batch Normalization works well in many cases. However, if both Batch Normalization and Dropout are in the model, the performance may degrade (\cite{dropout_and_batch_norm}).   

{\bf Activation layer:}
Each neuron is activated through a non-linear transformation.  Non-linearity is an important component for DNN which gives us a wide variety of expression.   Note the majority of the layers, including Fully Connected Layers, are a linear transformation and even if we have a number of layers, it is equivalent of one single linear transformation.   Thus, it is essential to have a non-linear transformation so that each neuron can have a freedom to take any values necessary.
For the first iteration, we do not know what kind of transformation is the best, the transformation itself is taken as one of the hyperparameters.  The functions, 'tf.nn', in {\it Tensorflow} is being used.

{\bf Second Fully Connected layer:}
After $T$ reptitions of Highway layer, Batch Normalization layer, and Activation layer,
we are in need of converting the number of neurons to the number of SNe times the number of SN classes.   This is an opposite operation of the First Fully Connected layers.

{\bf Softmax layer:}
Softmax layer normalize the input value of this layer, which is denoted by $h \in \mathbb{R}^D$. The output value is $\hat{y} \in \mathbb{R}^D$ and each element $\hat{y}_k$ is expressed as
\begin{equation}
    \hat{y}_k = \frac{\exp \left( h_k \right)}{\sum_{k'=1}^K \exp \left( h_{k'} \right)}.
\end{equation}
The normalized value $\hat{y}$ satisfies $\hat{y}_k \geq 0$ and $\sum_k^K \hat{y}_k =1$.
We can interpret $\hat{y}_k$ as a probability that the input $x$ belongs to class $k$.
However, we note that this is a pseudo probability and it differs from the one we use in statistics.
  
\subsection{Hyperparameter search}\label{hyperparametersearch}
We perform a hyperparameter search (red boxes in figure \ref{fig:dnn_model}) by combining grid search and Tree-structured Parzen Estimator (TPE) algorithm.
Grid search is not suitable for high-dimensional space search, but has an advantage of searching for multiple points in parallel.
In addition, since the algorithm is simple, we can convey the knowledge of preliminary experiments for parameter initialization.
On the other hand, the TPE algorithm is suitable for searching a high-dimensional space, but has a disadvantage of not knowing where to start for the first time.
Therefore, this time, we search around the hyperparameter values that were good in the preliminary experiment by grid search, and then send the grid search results to TPE algorithm.  
The ranges of hyperparameters that we searched are given in table \ref{tb:hp}.
%By combining them, there are merits that the knowledge obtained in the preliminary experiment can be used and the time required for the search can be shortened.

% We optimized the hyperparameters of the DNN model using the library, optuna(\cite{optuna}).
As a usual strategy, we divide the dataset into a training data and a validation dataset.
We use the training data for optimizing DNN, and use the validation data for measuring the accuracy.
In optimizing the hyperparameters, we evaluated the hyperparameters with validation dataset so
that the accuracy of validation dataset becomes maximum.
We iterate this process for 100 times so that the accuracy converges to its maximum (figure \ref{fig:hp_test}).

%We trained the DNN model using the hyper parameters obtained by optimization, and evaluated the performance of the DNN model.
%
\begin{table}[htbp]
  \tbl{Ranges of hyperparameter search for Type Classification}{
      \begin{tabular}{lcc}
        \noalign{\vskip 2mm}
        \hline
        hyper parameter     & value (grid)  & range (TPE)\\ \hline 
        $D$                 & \{100, 300\}  & 50, \ldots, 1000   \\
        $T$                 & \{1, 3, 5\}   & 1, \ldots, 5       \\
        $bn$                & \{true\}      & \{true, false\}    \\
        $drop\mbox{ }rate$      & [5e-3, 0.035] & [5e-4, 0.25]       \\
        $type$              & \multicolumn{2}{c}{\{identity, relu, sigmoid, tanh\}} \\
        \hline
      \end{tabular}
  }\label{tb:hp}
\end{table}

\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/hp_iterations_accuracy.eps}
  \end{center}
  \caption{%
  We iterate hyperparameter search for 100 times where it converges to its maximum performance in terms of accuracy.   What is shown is the case of a binary classification.
  }%
  \label{fig:hp_test}
\end{figure}
%
%In this section, we describe the method used for SN type classification and regression of the value of redshift.
%Next, we will explain how to learn the model and how to determine the hyperparameters of the model.
%
%DNN is one of machine learning methods inspired by the biological brain system.
%DNN combines a large number of elements that mimic neurons, and each element has only a simple function, but it can obtain complex functions as a whole.
%DNN model can obtain the features needed to solve the target task from the given input without explicitly giving the features.
%Our goal is to classify the type of the celestial body or to estimate the redshift value from the observed light curve.
%We used DNN model for them.
%The DNN model receives the total of P brightnesses observed in multiple bands as an input and outputs $C$ values, where $C$ is the number of classes in classification task, and $C = 1$ in regression task.
%We used the structural model shown in Fig.\ref{fig:dnn_model}.
%
%In this study, we adopted Highway Layer \citep{highway} as the basic part of the network structure.
%The depth and/or layer size of the DNN model are directly related to the complexity of the features that can be obtained, and greatly affect the %performance for the task.
%However, a too deep model is difficult to learn and rather causes performance degradation.
%There are many studies to solve the problem, such as optimization, initialization, and network architecture.
%Highway layer is a technique that stabilizes learning by devising the network structure.
%
%Highway layer, shown in Fig.\ref{fig:dnn_model}, has multiple paths from the input to the output.
%The output of Highway Layer is calculated from the values of the multiple paths.
%The output, ${\bf Highway} \left(x\right)$, is formulated as
%\begin{equation}
%    {\bf Highway} \left(x\right) = T \left(x\right) \cdot H \left(x\right) + C \left(x\right) \cdot x,
%\end{equation}
%where $H$ is a non-linear transformation layer, $T$ is the transformation-gate function layer, and $C$ is the carry-gate function layer.
%The two gate functions $T$ and $C$ control how much of the outputs which are transforming the input and carrying it, respectively.
%Highway layer includes other layers inside.
%This structure is called layer in layer.
%
%We defined the functions as 
%\begin{eqnarray}
%    H \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_H, b_H\right\}\right) \right), \\
%    T \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_T, b_T\right\}\right) \right), \\
%    C \left(x\right) &=& 1 - T \left(x\right),
%\end{eqnarray}
%where $a$ is an activation function, that is,
%\begin{eqnarray*}
%    \mathrm{a} \left(z\right) &=& \left( \sigma\left(z_1\right),\sigma\left(z_2\right), \ldots, %\sigma\left(z_d\right) \right)^T, \; z \in \mathbb{R}^d, \\
%    \sigma \left(z_i\right) &=& \frac{1}{1 + e^{-z_i}},
%\end{eqnarray*}
%$d$ is the size of the dimension of the input and the output of Highway Layer, and
%$F$ is a fully connected layer parameterized by $W \in \mathbb{R}^{d \times d}$ and $b \in \mathbb{R}^d$, is formulated as
%\begin{equation}
%    F \left(x, \left\{W,b\right\}\right) = W x + b.
%\end{equation}
%Fully connected layer is a popular and basic layer of DNN.
%This formulation of Highway layer is same as (\cite{highway}).
%
%
We can train the DNN classifier in the same way regardless of the number of classes.
In the case of multi-type classification, the number of classes is $K = 3$ in our experiment, so the number of outputs of the DNN classifier is also three.
In binary classification (SN~Ia or Non-SN~Ia), the number of the outputs is two.

We train the model by minimizing the cross-entropy error: 
\begin{equation}
\mathrm{CE} \left(y, \hat{y} \right) =　-\sum_{k = 1}^K y_k \log \hat{y}_k,
\end{equation}
where $y$ is a ground truth vector which is one-hot encoding of $K$ dimension, and $\hat{y}$ is the DNN output vector.
We deploy {\it Adam optimizer} which uses a stochastic gradient method to optimize the model parameters.

We introduce a data augmentation to prevent overfitting at the time of training.
By increasing the number of input data by augmentation, we prevent DNN from memorizing all of the training dataset.
We apply two ways of data augmentation to the training dataset.
One is adding Gaussian noise (based on the expected observed uncertainty) to the simulated flux.
The other is mixup (\cite{mixup}).

%To add noise to flux, as follows, we replaced $f_i$ with $f_i + \epsilon_i$ in eq.\ref{eq:scaled_flux} and eq.\ref{eq:mag},
%where $\epsilon_i$ is a random noise drawn from the Gaussian distribution with zero mean and $e_i^2$ variance, and $e_i$ is the flux error corresponding to $f_i$.
Mixup generates a new virtual training dataset as follows:
\begin{eqnarray*}
    \tilde{x} &=& \lambda x_u + \left( 1-\lambda \right) x_v, \\
    \tilde{y} &=& \lambda y_u + \left( 1-\lambda \right) y_v,
\end{eqnarray*}
where $\left(x_u, y_u\right)$ and $\left(x_v, y_v\right)$ are samples drawn at random from the training dataset, $x$ is input vector, $y$ is one-hot label vector, 
and a mixing ratio $\lambda \in \left[0, 1\right]$ is drawn from a random distribution which is low density around 0 and 1 and higher density around 0.5. 
The generated datasets are suitable for DNN to learn the classification boundaries.

As described above, the hyperparameters (red boxes in figure \ref{fig:dnn_model}) of the model are optimized by maximizing the accuracy, while 
the model parameters (green boxes in figure \ref{fig:dnn_model}) are optimized by minimizing cross-entropy error.
%Table\ref{tb:hp_grid} shows the grid search settings, and Table\ref{tb:hp_search} shows the search settings for the TPE algorithm.
%Table\ref{tb:hp} shows the grid search settings and the search settings for the TPE algorithm.
% We determined the optimal DNN model size for classification by hyperparameter search.
% The scale for measuring the goodness of the model was the accuracy of the validation data, which was not used for training.
% As the first step of hyperparameter search, we performed a grid search for each element created by Cartesian product of the values in table \ref{tb:hp_grid}.
% Subsequently, we searched in the range in table \ref{tb:hp_search} using the TPE algorithm.
%
% The performance evaluation of the models was performed by 5-fold cross validation.
% Let the class corresponding to the element with the highest value among the prediction vector be the class predicted by DNN.
% The accuracy is evaluated by the matching rate of DNN predicted class and the ground truth.
%

\subsection{Testing DNN model with PLAsTiCC dataset} 
\label{sec:p}
%
%We verify the performance of the classifier described in the previous section using the PLAsTiCC dataset \citep{plasticc_dataset}.
Before we apply our model to the HSC observed data, we test it with LSST simulated classification challenge dataset, PLAsTiCC dataset \citep{allam18a,malz19a} which is composed of realistic photometric dataset with errors on time-variable objects.
To evaluate our model, we are in need of dataset with labels of true identity. 
PLAsTiCC Deep Drilling Field (DDF) dataset is similar to HSC-SSP Transient Survey, and we took advantage of it.
However, %our primary purpose is to extract SN~Ia, and 
we generate the training set by ourselves 
%(subsection \ref{sec:training}) 
and do not use the training set PLAsTiCC team has provided because we knew that the number of training dataset is not good enough to achieve the maximum performance (figure \ref{fig:size_convergence_test}).

%For training, 370,345 light curves are generated for LSST DDF.
%We use filter response including system throughput from \citet{ivezic19a}. 
For training, we generated 370,345 light curves based on the method described in subsection \ref{sec:training} using filter response and photometric zero-point for LSST \citep{ivezic19a}.
These light curves are composed of SN types by SN~Ia:Ibc:II=0.60:0.06:0.34, and their peaks are randomly shifted in time.
%In addition, the timing at which SNs occur is shifted by randomly setting an offset that indicates the difference between the SN phase and the observation schedule.
%
For test dataset, we extract 2,297 light curves from PLAsTiCC dataset.
%, and applied our model to it.
These are labeled three types of SNe (Ia, Ibc and II), and simulated to occur in COSMOS.
%, the target region of HSC survey.

We use the area under the curve (AUC) of receiver operating characteristic (ROC) curve, precision-recall curve in two-class classification, and the accuracy from confusion matrix in three-class classification as a metric to evaluate our model.
We test which combination of inputs performs best for PLAsTiCC dataset. Our input could be a combination of arrays of normalized flux ($f$), magnitude ($m$), pseudo-absolute magnitude ($M$).
Table \ref{tab:p_test} shows the AUC in two-class classification and accuracy in three-class classification in PLAsTiCC data, respectively.
We find that the combination of normalized flux ($f$) and pseudo-absolute magnitude ($M$) performs best, although the information is redundant, we suspect the different distribution of the data gives extra clue to the machine.
%As with the case of the simulated data, the classifier with absolute magnitudes and normalized flux provides the best classification results for PLAsTiCC data.
The AUC for the ROC curve and precision-recall curve are 0.996 and 0.995 respectively.
%, and these curves in this model are shown in figure\ \ref{fig:plasticc_2class_ROC} and figure\ \ref{fig:plasticc_2class_PreRec}.
Figure\ \ref{fig:plasticc_3class_CM} shows the confusion matrix in the three-class classification, and the total accuracy is calculated as 95.3\%.
As is always the case in the real world, it is hard to classify SN~Ibc, but the effect on total accuracy is relatively small.
%PLAsTiCC, test(v1.2.1)
%
\begin{table}[htbp]
\tbl{Classification performance of each input for the PLAsTiCC dataset.}{
\begin{tabular}{cccccc}
\noalign{\vskip 2mm}
\hline
\multicolumn{3}{c}{Input\footnotemark[$*$]}   & \multicolumn{2}{c}{AUC} & Accuracy \\
\hline
$M$ & $m$ & $f$ &  ROC &  Pre.-Rec. &\\
\hline
\checkmark &            & \checkmark &    0.996 &       0.995 &     0.953\\
\checkmark &            &            &    0.995 &       0.993 &     0.952\\
           & \checkmark & \checkmark &    0.995 &       0.993 &     0.948\\
           & \checkmark &            &    0.995 &       0.991 &     0.940\\
\hline
\end{tabular}
}\label{tab:p_test}
\begin{tabnote}
\footnotemark[$*$] Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table}
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/06_CM_abs-mag_scaled-flux_w-mixup_predictions_test_2.eps}
  \end{center}
  \caption{%
  Normalized confusion matrix in three-class classification of PLAsTiCC dataset. The input to the classifier is pseudo-absolute magnitude and normalized flux. The proportions in each row sum to 1. The numbers in parentheses represent the raw numbers.
  }%
  \label{fig:plasticc_3class_CM}
\end{figure}
%

In the three-class classification of the PLAsTiCC dataset,
107 SNe is misclassified and have the following characteristics:
\begin{itemize}
\item 54\% (58/107) of them are 'incomplete events' that do not include the SN peak phase, the period (observed frame) of 10 days before and 20 days after peak, in the photometric data, while they are only 38\% of all events.
\item Of the rest, 29\% (14/49) are on the boundary where the difference in probability between the correct class and the predicted class is less than 0.1.
\item More than half of the remaining 35 events misclassify Ibc as Ia, II
\end{itemize}
Figure\ \ref{fig:misclass_rate_3class} shows the accuracy against redshift for the PLAsTiCC dataset.
%We found that 
The accuracy for Ibc is lower than that of other classes.
%and that the accuracy dropped significantly in the vicinity of redshift 1.0, and the accuracy also decreased for nearby Ibc SNe.
%The accuracy for SN~Ibc is 
It is greatly reduce at redshifts beyond 1.0, and also decreased at redshift z of 0.1 to 0.2.
As a result of individual check, while some misclassified SNe are so dark and indeterminate that they are difficult to classify even with conventional methods, there are also bright SNe that are completely misclassified.
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/misclass_rate_plastic_3class.eps}
  \end{center}
  \caption{%
  Accuracy against redshift in the three-class classification of the PLAsTiCC dataset.
  }%
  \label{fig:misclass_rate_3class}
\end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to HSC-SSP transient survey}
\label{sec:h}
%
%observation frequencies,
We apply the developed classifier to the observed photometric data set of 1824 SNe.
As described in \citet{yasuda19a}, the survey %is part of HSC-SSP survey, where observations were performed in two areas with different 
is conducted in two kinds of layers with different depths and cadence, 'Deep' and 'Ultra-Deep'.
Therefore, the number of photometric data points of SN in each layer could be different.
%There are also SNe in which part of the light curve data is missing.
%To correspond these SNe, we prepared five input cases of light curve data and classifiers for each case.
Our DNN model requires the exactly the same number of data points as an input, and here we divide
our data set into five cases based on the number of photometric data points.
The number of SNe for each case is summarized in table\ \ref{tab:class_flag}.
For example, the number of SNe in Case 0 is 709, and they are in the Ultra-Deep field.  Each SN has a total of 
42 epochs of photometric data in four bands ($g$-, $r2$-, $i2$- and $z$-band).
The number of epochs and filter schedule for Case 0 SNe are summarized in table \ref{tab:HSCsurvey_schedule}.
By introducing five cases, the machine can classify 1812 HSC SNe which corresponds to 99.3\% of 1824 SNe.
The remaining 12 SNe are excluded due to the missing data. 
%in all of the schedule patterns from Case 0 to Case 4, and there is no new pattern common to them.
%
%v190522
\begin{table}[htbp]
\tbl{Number of SNe for each Case.}{
\begin{tabular}{lrrrr}
\noalign{\vskip 1mm}
\hline
Case & Epoch & Number & Fraction \\
\hline
0     & 42      & 709        & 0.391 \\
1     & 26      & 646        & 0.357 \\
2     & 19      & 271        & 0.150 \\
3     & 10      & 122        & 0.067 \\
4     & 4        & 64        & 0.035 \\
\hline
\end{tabular}
}\label{tab:class_flag}
\end{table}
%
%
\begin{table}[htbp]
\tbl{Number of input epochs and the schedule for Case 0 SNe.}{
\begin{tabular}{llp{15em}}
\noalign{\vskip 2mm}
\hline
Filter & Epochs & Elapsed day\\
\hline
$g$ & 8 & 2, 40, 63, 70, 92, 119, 126, 154\\
$r2$ & 9 & 5, 32, 61, 71, 92, 103, 122, 129, 151\\
$i2$ & 13 & 2, 6, 32, 40, 61, 68, 71, 94, 101, 120, 127, 154, 155\\
$z$ & 12 & 0, 6, 30, 40, 59, 68, 90, 101, 119, 126, 151, 157\\
%Y  & 10 & -59, -26, -17, 4, 13, 37, 45, 59, 68, 89 \\
\hline
\end{tabular}
}\label{tab:HSCsurvey_schedule}
\end{table}
%
%
%training set 
We apply both two-class and three-class classification to the above five cases of HSC observed data.
For each case, the machine needs to be trained independently with a dedicated training data set.
Thus, hyperparameters are optimized for each case and reported in table \ref{tb:searched_hp_class}.
%The optimized hyperparameters for each cases are listed in table \ref{tb:searched_hp_class}.
The following subsections (subsection \ref{sec:h2} and \ref{sec:h3}) describe the performance evaluation for each classification.
%
%
\begin{table*}[t]
  \tbl{Optimized hyperparameters for classification}{
      \begin{tabular}{lcccllllllllll}
      \noalign{\vskip 1mm}
\hline
%      & $M$        & $m$        & $f$        & the number of blocks T & hidden size D & drop rate & bn & type    \\ \hline
      & \multicolumn{3}{l}{Input\footnotemark[$*$]}            & \multicolumn{5}{l}{Two-Class}      &  \multicolumn{5}{l}{Three-Class}   \\ \hline
      & $M$        & $m$        & $f$        & $T$ & $D$   & $drop\mbox{ }rate$ & $bn$ & $type$    & $T$ & $D$   & $drop\mbox{ }rate$ & $bn$ & $type$    \\ \hline      
Case 0& \checkmark &            & \checkmark & 5 & 178 & 9.47e-3   & 1  & sigmoid & 4 & 429 & 1.20e-3   & 0  & linear  \\
      & \checkmark &            &            & 3 & 247 & 9.68e-4   & 1  & sigmoid & 4 & 516 & 2.54e-3   & 0  & tanh    \\
      &            & \checkmark & \checkmark & 4 & 531 & 6.43e-3   & 0  & linear  & 4 & 608 & 1.72e-2   & 0  & linear  \\
      &            & \checkmark &            & 4 & 411 & 9.00e-2   & 1  & sigmoid & 4 & 838 & 1.36e-3   & 0  & tanh    \\ \hline
Case 1& \checkmark &            & \checkmark & 5 & 734 & 8.75e-4   & 0  & tanh    & 4 & 915 & 1.03e-2   & 0  & linear  \\
      & \checkmark &            &            & 2 & 389 & 7.17e-2   & 1  & sigmoid & 5 & 698 & 2.79e-2   & 0  & linear  \\
      &            & \checkmark & \checkmark & 2 & 647 & 7.92e-4   & 0  & tanh    & 4 & 540 & 1.45e-3   & 0  & linear  \\
      &            & \checkmark &            & 2 & 342 & 1.42e-2   & 1  & sigmoid & 2 & 520 & 9.99e-4   & 1  & sigmoid \\ \hline
Case 2& \checkmark &            & \checkmark & 2 & 368 & 1.79e-3   & 1  & sigmoid & 4 & 698 & 9.08e-4   & 0  & linear  \\
      & \checkmark &            &            & 4 & 920 & 1.44e-3   & 0  & sigmoid & 4 & 614 & 7.03e-3   & 0  & linear  \\
      &            & \checkmark & \checkmark & 5 & 572 & 4.27e-3   & 1  & sigmoid & 4 & 896 & 8.58e-3   & 0  & linear  \\
      &            & \checkmark &            & 4 & 640 & 1.12e-1   & 1  & sigmoid & 5 & 300 & 5.00e-1   & 1  & sigmoid \\ \hline
Case 3& \checkmark &            & \checkmark & 5 & 893 & 1.42e-3   & 1  & sigmoid & 4 & 522 & 5.02e-4   & 0  & tanh    \\
      & \checkmark &            &            & 4 & 880 & 1.98e-2   & 1  & sigmoid & 5 & 841 & 4.96e-2   & 1  & sigmoid \\
      &            & \checkmark & \checkmark & 3 & 300 & 5.00e-3   & 1  & linear  & 3 & 462 & 9.28e-4   & 1  & sigmoid \\
      &            & \checkmark &            & 3 & 930 & 9.77e-2   & 1  & sigmoid & 3 & 300 & 5.00e-3   & 1  & sigmoid \\ \hline
Case 4& \checkmark &            & \checkmark & 5 & 379 & 4.21e-3   & 1  & sigmoid & 3 & 484 & 2.13e-3   & 0  & linear  \\
      & \checkmark &            &            & 2 & 631 & 1.77e-2   & 0  & sigmoid & 4 & 243 & 3.21e-3   & 1  & sigmoid \\
      &            & \checkmark & \checkmark & 5 & 140 & 4.04e-3   & 1  & sigmoid & 5 & 389 & 1.23e-4   & 0  & tanh    \\
      &            & \checkmark &            & 4 & 567 & 5.77e-2   & 0  & sigmoid & 3 & 354 & 2.14e-1   & 1  & sigmoid \\ \hline
\end{tabular}
  }\label{tb:searched_hp_class}
\begin{tabnote}
\footnotemark[$*$] Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Binary classification}
\label{sec:h2}
%
In binary classification, we prepare four versions of classifiers with different inputs as in PLAsTiCC dataset, and compare their performance.
%The performance for each input of binary classification are summarized in table\ \ref{tab:h2_validation},\ref{tab:h2_test_gold} and \ref{tab:h2_test_all}.
The performance for each input of binary classification are summarized in table\ \ref{tab:h2_AUC}.

For the validation dataset, which is a part of the simulated dataset, a higher number of input dimensions results in the better the results, and any classifier can classify them with very high AUCs.
The best AUCs for all classified events are 0.993 and 0.995 for the ROC curve and precision-recall curve, respectively.
%Figure\ \ref{fig:h2_validation} show ROC curves and precision-recall curves in the best performing classifier for the validation set.

For test dataset, the classification performance is verified using 1332 HSC SNe (1256 with redshift) labeled by the SALT2 light curve fitter \citep{guy2007,guy10b}, one of the conventional classification methods.
The verification label for HSC SNe conforms to \citet{yasuda19a}, which defines SN~Ia as SNe that satisfy all the following four criteria for the SALT2 fitting results:
(1)color ($c$), and stretch ($x_1$) within the $3\sigma$ range of \citet{scolnickessler2016} 'All G10' distribution, 
(2)absolute magnitude in B band $M_B$ brighter than $-18.5$ mag, 
(3)reduced $\chi ^{2}$ of less than 10,
(4)number of degrees of freedom (dof) is greater than or equal to 5.
Other candidates that satisfy the looser set of conditions above are labeled 'Ia?'.
Specifically, the range of (1) is expanded to within 5 sigma, and the thresholds of (2) and (3) are set to $-17.5$ mag and 20 respectively.
On the other hand, we define non Ia in the HSC classification as SNe that do not satisfy the conditions of 'Ia' and 'Ia?', and whose number of dof is 5 or more.
The numbers of labeled Ia, Ia? and non Ia are 429, 251 and 908 (410, 240 and 850, with redshift), respectively.
Besides the above, 215 SNe with the number of dof less than 5 are labeled as 'unclassified', and fitting of the remaining 21 SNe fails.
In this performance evaluation, we use 428 Ia and 904 non Ia SNe classified by our machine.
%The numbers of labeled Ia, Ia? and non Ia are 428, 251 and 904 (409, and 847, with redshift), respectively.
%Those with the number of dof less than 5 are labeled as 'unclassified', which is not classified by SALT2 fit, and remaining 21 SNe are labeled 'fail' which does not even fit. 

We also extracted 441 'light curve verified SNe' that have spec-z and photometric information before and after peak, and verified their classification results.
Figures\ \ref{fig:h2_test_all} and \ref{fig:h2_test_gold} show the AUCs of the best classifier for all labeled HSC SNe and the light curve verified HSC SNe respectively.
The confusion matrices for each case are shown in figure\ \ref{fig:h2_test_CM}.
%need reference
The best performing classifier shows the same classification results as the conventional method in 84.2\% of 1256 labeled SNe, with 91.8\% accuracy for 441 light curve verified SNe.
%The final classification result of the HSC SNe is the combination of outputs from the classifier using the absolute magnitude for those with redshift information, and for that using the magnitude for those without redshift information.
%The numbers of SNe with Ia probabilities above 0.5, 0.7 and 0.9 are 530, 386 and 170 out of 1332, respectively.
%The numbers of SNe with Ia probabilities above 0.5, 0.7 and 0.9 are 613, 437 and 186 out of 1540, respectively. (in old ver.)
%
%
%
%
%2class AUC
%Ver. 1.2.0 
\begin{table*}[htbp]
\tbl{AUC of each input in the HSC binary classification.}{
\begin{tabular}{c|ccc|p{3em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}|p{3em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}}
\noalign{\vskip 1mm}
\hline
Dataset & \multicolumn{3}{c}{Input\footnotemark[$*$]} & \multicolumn{6}{|c|}{ROC} & \multicolumn{6}{c}{Precision-Recall} \\
\hline
 & $M$ & $m$ & $f$ & Case 0 & 1 & 2 & 3 & 4 & All & Case 0 & 1 & 2 & 3 & 4 & All \\
\hline
Validation &\checkmark &            & \checkmark &       1.000&       0.990 &       0.987 &       0.976 &       0.887 &        0.993 &          1.000 &          0.993 &          0.991 &          0.983 &          0.917 &           0.995 \\
& \checkmark &            &            &       0.999&       0.980 &       0.975 &       0.959 &       0.845 &        0.987 &          0.999 &          0.986 &          0.982 &          0.972 &          0.886 &           0.991 \\
&           & \checkmark & \checkmark &       0.999&       0.983 &       0.979 &       0.963 &       0.817 &        0.988 &          0.999 &          0.988 &          0.985 &          0.973 &          0.863 &           0.992 \\
&           & \checkmark &            &       0.996&       0.971 &       0.966 &       0.938 &       0.790 &        0.980 &          0.997 &          0.980 &          0.976 &          0.955 &          0.841 &           0.985 \\
\hline
Test& \checkmark &            & \checkmark &       0.975 &       0.978 &       0.931 &       0.844 &       1.000 &        0.966 &          0.931 &          0.955 &          0.773 &          0.761 &          1.000 &           0.909 \\
(Light curve verified)& \checkmark &            &            &       0.986 &       0.967 &       0.942 &       0.967 &       0.964 &        0.971 &          0.965 &          0.935 &          0.849 &          0.966 &          0.944 &           0.934 \\
&           & \checkmark & \checkmark &       0.947 &       0.926 &       0.887 &       0.756 &       1.000 &        0.923 &          0.836 &          0.860 &          0.803 &          0.632 &          1.000 &           0.820 \\
&           & \checkmark &            &       0.945 &       0.864 &       0.854 &       0.756 &       0.643 &        0.896 &          0.826 &          0.769 &          0.752 &          0.612 &          0.687 &           0.787 \\
\hline
Test& \checkmark &            & \checkmark &       0.945 &       0.922 &       0.914 &       0.863 &       0.844 &        0.925 &          0.855 &          0.840 &          0.809 &          0.788 &          0.650 &           0.832 \\
(All labeled)& \checkmark &            &            &       0.957 &       0.909 &       0.879 &       0.908 &       0.864 &        0.922 &          0.901 &          0.814 &          0.714 &          0.885 &          0.702 &           0.817 \\
&           & \checkmark & \checkmark &       0.915 &       0.889 &       0.885 &       0.718 &       0.711 &        0.885 &          0.780 &          0.778 &          0.768 &          0.543 &          0.363 &           0.749 \\
&           & \checkmark &            &       0.911 &       0.837 &       0.855 &       0.713 &       0.656 &        0.862 &          0.773 &          0.685 &          0.712 &          0.523 &          0.385 &           0.712 \\
\hline
\end{tabular}
}\label{tab:h2_AUC}
\begin{tabnote}
\footnotemark[$*$] Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table*}
%
% all samples
%
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_absolute-magnitude-scaled-flux-remove-y_SNdata_test_190522_ROC_all.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_absolute-magnitude-scaled-flux-remove-y_SNdata_test_190522_PreRec_all.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \vspace{2mm}
    \caption{%
    ROC curves and Precision Recall curves in two-class classification of all labeled HSC SNe.
    The input to the classifier is pseudo-absolute magnitude and normalized flux.
    Each colored line represents the performance for each of the five classifiers with different input cases, and the that for all of their outputs.
    }
    \label{fig:h2_test_all}
\end{figure*}
%
%
%spec-z \& w/o incomplete samples
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_absolute-magnitude-scaled-flux-remove-y_SNdata_test_190522_ROC_noedge_spec.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_absolute-magnitude-scaled-flux-remove-y_SNdata_test_190522_PreRec_noedge_spec.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \vspace{2mm}
    \caption{%
  As in figure \ref{fig:h2_test_all}, but for the light curve verified HSC SNe. 
  %Recall curves in two-class classification of light curve verified HSC SNe. 
  %The input to the classifier is absolute magnitude and normalized flux.
  %Each colored line represents the performance for each of the five classifiers with different input cases, and the that for all of their outputs.
}%
    \label{fig:h2_test_gold}
\end{figure*}
%
%
%
%
%2class Test CM
%Ver. 1.2.0
%
%% spec-z & w/o incomplete samples
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_CM_absolute-magnitude-scaled-flux-remove-y_SNdata_test_190522_2_Flagall_all.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_CM_absolute-magnitude-scaled-flux-remove-y_SNdata_test_190522_2_Flagall_noedge_spec.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \caption{%
  Normalized confusion matrices in binary classification of 1256 labeled HSC SNe (left) and 441 light curve verified SNe (right).
  The inputs for both classifications are pseudo-absolute magnitude and normalized flux.
}%
    \label{fig:h2_test_CM}
\end{figure*}
%

In the binary classification of 1256 labeled HSC SNe, 198 of them are misclassified.
The misclassification rate for each case is different, and tends to increase as the number of input dimensions decreases.
Although the rate is 13\% for Case 0, it is 23\% for Case 4.
As with the PLAsTiCC data, incomplete events without peak phase are the majority of misclassified events in HSC data, accounting for 47\% (93/198) of them.
The second most common cause of misclassification is outlier value or systematic flux offset in photometric data, accounting for 34\%(67/198) of misclassifications.
Of the remaining 38 SNe, 17 are boundary events with Ia probability of 40 to 60\%, and the rest are events where SALT2 fitting does not work well.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Multi-type classification}
\label{sec:h3}
%
In this paper, we show the classification performance only for the validation dataset with three-class classifier, because three type classification label is not available for the HSC transients.
%because we are currently labeling more than three classes for all SNe detected in HSC-SSP Transient Survey using conventional light curve fitting.
%It is necessary to write the reason why there is no answer of multi-class classification.
%
% Classification performance of each model (3class, HSC, validation)
The accuracy for each input in the three-class classification of the validation dataset are summarized in table\ \ref{tab:h3_validation}.
The best accuracy for the validation dataset is 94.0\%.
The confusion matrix of the best classifier is shown in figure \ref{fig:h3_validation_CM}.
It represents that our classifier has a very high sensitivity to SN~Ia while it is not good at classifying SN~Ibc.

In addition, we describe the predicted classes of actual HSC SNe classified by the three-class classifier.
Figure \ref{fig:hsc3_type_frac_alongz} shows the fractions of each type predicted by the classifier in each redshift from 0.1 to 1.5.
All 1812 classified HSC SNe are used to calculate the fraction.
These SN types are a combination of the outputs from the two classifiers with different inputs depending on the presence of redshift information: (1)pseudo-absolute magnitude and normalized flux, (2)magnitude and normalized flux.
%Figure \ref{fig:h3_class_ratio} shows the predicted class ratio.
%Our machine classifies 1812 SN classes with a ratio of SN~Ia:Ibc:II=0.43:0.06:0.51, which is an integrated value throughout the redshift and includes incomplete sampling of faint SNe.

%
%
%%However, it should be noted that this ratio is calculated including misclassificaion.
%Table\  summarizes the classification results of all the actual SN.
%
%Need to write that 'ALL' accuracy is weighted
%
%3class Validation
%Ver. 1.2.1
\begin{table}[htbp]
\tbl{Accuracy of each input in the HSC three class classification for validation dataset.}{
\begin{tabular}{ccc|p{3em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}}
\noalign{\vskip 2mm} 
\hline
\multicolumn{3}{c}{Input\footnotemark[$*$]} & \multicolumn{6}{|c}{Accuracy\footnotemark[${\dagger}$]} \\
\hline
$M$ & $m$ & $f$  &  Case 0 & 1 & 2 & 3 & 4 &  All \\
\hline
\checkmark &            & \checkmark &        0.985 &        0.926 &        0.920 &        0.890 &        0.774 &         0.940 \\
\checkmark &            &            &        0.971 &        0.894 &        0.886 &        0.844 &        0.729 &         0.914 \\
           & \checkmark & \checkmark &        0.970 &        0.907 &        0.897 &        0.860 &        0.724 &         0.921 \\
           & \checkmark &            &        0.952 &        0.871 &        0.861 &        0.818 &        0.701 &         0.892 \\
\hline
\end{tabular}
}\label{tab:h3_validation}
\begin{tabnote}
\footnotemark[$*$] Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.

\footnotemark[$\dagger$] %Accuracy for samples extracted from each case according to the number ratio in the actual HSC dataset.
Accuracy of samples extracted from each case according to the fractions in table \ref{tab:class_flag}.
\end{tabnote}
\end{table}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/13_CM_abs-mag_scaled-flux_w-mixup_remove-y_predictions_validation_2_Flagall_weighted.eps}
  \end{center}
  \caption{%
  Normalized confusion matrix for validation dataset in the HSC three-class classification.
  The input is pseudo-absolute magnitude and normalized flux.
  The proportions in each row sum to 1 (within rounding error).
  }%
  \label{fig:h3_validation_CM}
\end{figure}
%
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/SNfrac_alongz.eps}
  \end{center}
  \caption{%
  Type fractions along redshift in HSC three-class classification.
  }%
  \label{fig:hsc3_type_frac_alongz}
\end{figure}
%

%
\subsection{Classification of HSC SNe}
%
We here publish the classification results of 1824 HSC SNe, obtained by the proposed classifiers, in e-table 1.\footnote{ E-table 1 is available on the online edition as a supplementary table. }
A part of this classification list is shown in table\ \ref{tab:h_results} as an example.
This list summarizes the probabilities predicted by the two-class and three-class classifiers for each SN, along with redshifts of host galaxies and classification labels by SALT2 fit.
The probabilities in this list are calculated from the output of the classifier with the normalized flux added to the input.
Each classification performance shown in subsections \ref{sec:h2} and \ref{sec:h3} is calculated based on the probabilities in this list.
\begin{table*}[htbp]
\tbl{Example of classification result list for HSC SNe.}{
%\tiny
\scriptsize
\begin{tabular}{p{4.5em}p{1.2em}p{4.0em}p{2.1em}|p{0.6em}p{1.8em}p{3.0em}|p{2.9em}|p{1.2em}p{1.2em}p{1.2em}p{0.6em}|p{2.9em}|p{1.2em}p{1.2em}p{1.2em}p{0.6em}}
\noalign{\vskip 1mm}
%\multicolumn{6}{|c}{Accuracy}
\hline
Name  &  Case &        z &  z\_src\footnotemark[$*$] &  \multicolumn{3}{p{6.0em}}{SALT2 fitting} &\multicolumn{5}{|p{13.5em}}{Classifier (Input\footnotemark[$\dagger$]: $M+f$)} & \multicolumn{5}{|p{12.0em}}{Classifier (Input\footnotemark[$\dagger$]: $m+f$)}\\
\hline
      &       &          &          &  dof & Type\footnotemark[$\ddagger$] & F\_cover\footnotemark[$\S$]  & 2-class &\multicolumn{4}{p{10.0em}|}{3-class}&2-class &    \multicolumn{4}{p{10.0em}}{3-class}\\
\hline
      &       &          &          &      &        &        &    Ia &    Ia &   Ibc &    II  &Type &   Ia &    Ia &   Ibc &    II &Type\\
\hline
HSC16aaau &     1 &    $0.370_{-0.072}^{+0.110}$ &         3 &    7 &    Ia? &   False &    0.556 &    0.554 &    0.022 &    0.424 &      Ia &    0.517 &    0.649 &    0.008 &    0.342 &      Ia \\
HSC16aaav &     1 &    $3.280_{-2.423}^{+0.167}$ &         4 &   17 &  nonIa &     True &    0.134 &    0.049 &    0.002 &    0.949 &      II &    0.279 &    0.356 &    0.048 &    0.596 &      II \\
HSC16aabj &     0 &    $0.361_{-0.008}^{+0.007}$ &         2 &    8 &  nonIa &   False &    0.630 &    0.667 &    0.001 &    0.331 &      Ia &    0.574 &    0.578 &    0.018 &    0.405 &      Ia \\
HSC16aabk &     1 &      NaN &         0 &    9 &    Ia? &   False &      NaN &      NaN &      NaN &      NaN &     NaN &    0.433 &    0.675 &    0.077 &    0.248 &      Ia \\
HSC16aabp &     1 &    $1.477_{-0.032}^{+0.037}$ &         2 &   19 &  nonIa &   False &    0.957 &    0.964 &    0.001 &    0.035 &      Ia &    0.807 &    0.871 &    0.039 &    0.090 &      Ia \\
\vdots & & & & & & & & & & & & & & & &\\
HSC17bjrb &     1 &    $0.560_{-0.036}^{+0.024}$ &         3 &    1 &  UC &   False &    0.003 &    0.007 &    0.004 &    0.989 &      II &    0.011 &    0.002 &    0.004 &    0.994 &      II \\
HSC17bjwo &     0 &    $1.449_{-0.063}^{+0.080}$ &         2 &   26 &     Ia &     True &    0.881 &    0.915 &    0.005 &    0.080 &      Ia &    0.891 &    0.935 &    0.010 &    0.055 &      Ia \\
HSC17bjya &     0 &    $1.128_{-0.000}^{+0.000}$ &         1 &   22 &  nonIa &     True &    0.130 &    0.145 &    0.039 &    0.816 &      II &    0.141 &    0.109 &    0.056 &    0.835 &      II \\
HSC17bjyn &     0 &    $0.626_{-0.000}^{+0.000}$ &         1 &   24 &     Ia &     True &    0.887 &    0.891 &    0.031 &    0.078 &      Ia &    0.965 &    0.918 &    0.007 &    0.075 &      Ia \\
HSC17bjza &     1 &    $1.350_{-0.156}^{+1.142}$ &         4 &   13 &  nonIa &     True &    0.016 &    0.041 &    0.016 &    0.943 &      II &    0.062 &    0.039 &    0.005 &    0.957 &      II \\
HSC17bkbn &     0 &    $0.863_{-0.012}^{+0.036}$ &         2 &   23 &  nonIa &     True &    0.031 &    0.025 &    0.002 &    0.973 &      II &    0.028 &    0.021 &    0.002 &    0.976 &      II \\
HSC17bkcz &     0 &    $0.795_{-0.000}^{+0.000}$ &         1 &   27 &     Ia &     True &    0.675 &    0.674 &    0.035 &    0.291 &      Ia &    0.661 &    0.789 &    0.019 &    0.191 &      Ia \\
HSC17bkef &     0 &    $2.940_{-0.087}^{+0.119}$ &         2 &    0 &   fail &    NaN &    0.219 &    0.443 &    0.000 &    0.556 &      II &    0.950 &    0.947 &    0.010 &    0.043 &      Ia \\
HSC17bkem &     2 &    $0.609_{-0.000}^{+0.000}$ &         1 &   17 &     Ia &     True &    0.889 &    0.858 &    0.001 &    0.141 &      Ia &    0.901 &    0.863 &    0.023 &    0.114 &      Ia \\
HSC17bkfv &     0 &    $0.670_{-0.035}^{+0.035}$ &         3 &   23 &     Ia &     True &    0.915 &    0.906 &    0.016 &    0.078 &      Ia &    0.961 &    0.926 &    0.011 &    0.063 &      Ia \\
\vdots & & & & & & & & & & & & & & & &\\
HSC17dskd &     0 &    $0.630_{-0.000}^{+0.000}$ &         1 &    3 &  UC &   False &    0.889 &    0.863 &    0.087 &    0.050 &      Ia &    0.873 &    0.873 &    0.072 &    0.054 &      Ia \\
HSC17dsng &     0 &    $1.331_{-0.048}^{+0.048}$ &         2 &    7 &    Ia? &   False &    0.951 &    0.967 &    0.006 &    0.027 &      Ia &    0.935 &    0.895 &    0.011 &    0.094 &      Ia \\
HSC17dsoh &     0 &    $1.026_{-0.000}^{+0.000}$ &         1 &    2 &  UC &   False &    0.968 &    0.968 &    0.011 &    0.020 &      Ia &    0.911 &    0.923 &    0.022 &    0.055 &      Ia \\
HSC17dsox &     0 &    $1.137_{-0.034}^{+0.041}$ &         2 &    2 &  UC &   False &    0.708 &    0.794 &    0.019 &    0.186 &      Ia &    0.721 &    0.738 &    0.040 &    0.222 &      Ia \\
HSC17dspl &     0 &    $0.624_{-0.000}^{+0.000}$ &         1 &    9 &  nonIa &   False &    0.180 &    0.065 &    0.114 &    0.821 &      II &    0.049 &    0.103 &    0.100 &    0.797 &      II \\
\hline
\end{tabular}
}\label{tab:h_results}
\begin{tabnote}
\footnotemark[$*$] Code for redshift source.
1: spec-z, 2: COSMOS photo-z, 3: HSC photo-z ultra-deep, 4: HSC photo-z deep, 0: hostless.

\footnotemark[$\dagger$] $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.

\footnotemark[$\ddagger$] SN type labeled by SALT2 fitting, UC: unclassified.

\footnotemark[$\S$] Flag indicating whether the photometric data covers the period of 10 days before and 20 days after the peak. SNe with this flag set to False are defined as 'incomplete events'.
\end{tabnote}
\end{table*}
%
%
\subsection{Dependence on the number of epochs}
%
When using our classification method, the number of photometric data points given to the classifier increases as the survey progresses.
Therefore, we investigated the transition of performance against the number of epochs.
We classified the HSC dataset by increasing the number of input data points one by one, and examined the relationship between the number of epochs and the classification performance.
Binary classifiers are used for classification, and the accuracy calculated from each confusion matrix is used for evaluation.
Figure\ \ref{fig:n_observations} shows the transition of classification performance for the Case 0 HSC dataset along with the number of epochs.
Although the Ia accuracy is as low as 0.6 to 0.7 in the early stage of the survey with less than five epochs, it exceeds 0.8 when the number of epochs increases to 22.
The partial decrease in accuracy is thought to be due to the new SN found at the added photometric point.
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/n_observations_v2_case0.eps}
  \end{center}
  \caption{%
  Relationship between the number of epochs and classification performance in binary classification for the Case 0 dataset. 
  The horizontal axis is the elapsed days of the HSC survey, and the vertical dot line shows the scale of input photometric point number. 
  The color of each mark in accuracy indicates the band of the added photometric point. 
  The blue horizontal dotted line indicates the accuracy when using all epochs.
  }%
  \label{fig:n_observations}
\end{figure}
%

We also investigated the classification performance during each SN phase by regrouping all events according to the length of period since the first detection.
%examples of probability
Figure\ \ref{fig:lcps} illustrates the light curves and Ia probability transitions since the first detection of three HSC SNe.
We define 'first detection' as the first day when the SN is detected with 5$\sigma$ confidence in flux, and flagged as a real object by the real-bogus classifier using convolutional neural network \citep{yasuda19a}.
The probability is updated at each new epoch.
While there are events where the probability increases as the SN phase progresses, there are also some events where the probabilities fluctuate around 0.5 even as the observation progresses and cannot be clearly classified.
Figure\ \ref{fig:n_observations_SNphase} shows the relationship between the number of days from first detection to classification, accuracy and the cumulative number of SNe classified as Ia with high probability.
The calculations for each performance are based on the classification results for 1161 SNe detected before the rising phase.
This figure presents how long SN photometric data is needed to classify with high accuracy using our classifier.
%The classification performance is low if only the initial rising phase before the SN peek is included in the input photometric data, but the accuracy increases to 78\% when we input two weeks of SN data from the first detection.
The classification accuracy is 78.1\% for the first two weeks of data, and after one month it increases to 82.7\%.
%Since the observation interval in the HSC survey is not constant, the number of inputs is different even for SNe with the same detected period.
%Due to this fact, there is a variation in the average input dimension number of samples in each phase, which causes a fluctuation in accuracy.
%However, due to events whose probabilities fluctuate with the progress of the SN phase illustrated in Figure \ref{fig:lcps}, there are some cases where the accuracy slightly decreases even if the period until classification becomes long.
%In addition, considering the cumulative number of SNe classified as Ia with high probability from six months data, it is estimated that our classifier can classify at least four or more Ia SNe candidate to follow-up in one month observation of the Ultra-deep area.
In addition, the number of follow-up candidates by the classifier can be estimated from the cumulative number in figure \ref{fig:n_observations_SNphase}.
Using data within one month from the first detection, 79 SNe with z $>$ 1 are classified with Ia probability of 95\% or more.
Since the number of these SNe is a cumulative number for six months observation, dividing this by six corresponds to the number of follow-up SNe classified during a one-month survey, which is 13 events.
%
\begin{figure*}[htbp]
    \begin{tabular}{ccc}
        \begin{minipage}{0.33\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/lcp_aqfi.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.33\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/lcp_apsw.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.33\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/lcp_akvr.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \vspace{3mm}
    \caption{%
    Examples of light curves and probability transitions. The title of each plot shows the name of the SN in the HSC survey and the label classified by SALT2 fitting.
    }%      
    \label{fig:lcps}
\end{figure*}
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/n_observations_SNphase_v200318.eps}
  \end{center}
  \caption{%
  Classification accuracy and cumulative number of type Ia SNe classified with high probability against SN phase.
  The orange line indicates the cumulative number of SNe with Ia probability $>$ 95\%, and green line is that for distant SNe at z $>$ 1. %Average number of observation points with red lines.
  }%
  \label{fig:n_observations_SNphase}
\end{figure}
%

Lastly, we study the transition of the Ia probability output by the classifier along SN phase.
Each time the number of input epochs increases, Ia probability, the output of the classifier, is updated.
For each of 100 SNe labeled Ia and non Ia with SALT2 fitting, we extracted its Ia probability for each duration from the various classification results, and plotted its transition as figure\ \ref{fig:visualized_Ia_prob}.
Although the Ia probability fluctuates greatly in the earliest phase, it begins to divide in about two weeks, and it can be clearly separated in one month.
%Figure \ref{fig:HSTIaprob} shows the transitions of Ia probability along the days from the peak for 16 SNe included in Case 0 data, among 26 SNe selected as HST targets in the HSC survey, and the average of these transitions.
Figure \ref{fig:HSTIaprob} shows the transitions of Ia probability along the days from the peak for 26 SNe selected as HST targets in the HSC survey, and the average of these transitions.
The Ia probability for the average of the candidates is greater than 0.8 three weeks before the peak.
This means that our classifier accomplishes the task described in subsection \ref{sec:tasks} to find good candidates only with information before the SN peak.
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/n_observations_visualized_Ia_probability_200319.eps}
  \end{center}
  \caption{%
  Transition of Ia probability with increasing elapsed time of input SNe.
  Each line corresponds to one of SNe.
  The difference in color indicates the difference in label, with red being Ia and blue being non Ia.
  }%
  \label{fig:visualized_Ia_prob}
\end{figure}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/HST_DaysFromPeak_vs_IaProbability_200319.eps}
  \end{center}
  \caption{%
  Ia probability transitions of 26 HST targets. Each of the lighter lines represents the variation for the individual HST targets, and the red line is those average.
  }%
  \label{fig:HSTIaprob}
\end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%
\subsection{Factors affecting classifier performance}
%\subsection{Simulated data versus observed data}
%
%In this paper we not only evaluate our classifier with simulated data, but also apply it to actual HSC survey data to evaluate classification performance.
In this paper, we apply our classifier to actual HSC survey data to evaluate its classification performance.
What we find out by classifying actual data with our method is that the performance difference between the validation and test set is larger than that for the PLAsTiCC data.
We discuss the factors that affect the classification performance of actual data.

One possible reason is the uncertainty in the labeling of actual data using conventional methods.
In the confusion matrix for all labeled SNe in figure \ref{fig:h2_test_CM}, 47\% of the 198 misclassified SNe are incomplete events.
%As with the classification of the PLAsTiCC data, many misclassified incomplete events, which should be only 27\% in all classified SNe, suggest that they reduce the performance of the classifier.
They are only 35\% of 1824 HSC SNe.
The high percentage of incomplete events among misclassified events suggests that they reduce classification performance. %, as with classification of PLAsTiCC data.
%However, the misclassification rate of incomplete events is clearly increased to 7\% (93/1256) in HSC data compared to 3\% (58/2297) in PLAsTiCC data.
However, the misclassification rate of incomplete events is clearly increased to 28\% (93/338) in HSC data compared to 7\% (58/871) in PLAsTiCC data.
SALT2 fitting is also not good at fitting incomplete events due to its specification, and errors in fitting parameters are significantly larger.
These facts suggests the existence of uncertainty in labeling with conventional classification methods.
Another source of uncertainty in labeling is due to the uncertainty of redshift information used in SALT2 fitting.
For at least 16 misclassified events that have no other misclassification factors, the fitting color parameters are far outside the criteria for Ia, and the wrong redshift is probably used for fitting.
In fact, as shown in the right panel of figure \ref{fig:h2_test_CM}, the performance for light curve verified SNe, except for incomplete events and whose spec-z is known, is better than for all classified events.

Another cause of the large difference in the performance of the validation and test set is the inability to completely simulate the observed data.
Outlier values and systematic flux offsets, which are considered to be one of the causes of misclassification described in subsection \ref{sec:h2}, are found only in observed data.
As described in \citet{yasuda19a}, the photometric data of the SN in the HSC survey is measured from the difference image obtained by subtracting the reference image from the observed image.
We believe that the unsimulated residual in this subtraction creates a difference between the photometric data for training and observation, and increases the misclassification rate of the classifier in the actual data.
For example, The light curve of SN HSC16akvr shown in the right panel of figure \ref{fig:lcps} fluctuates at its tail part, and the classification is not clear in that part.
It will be necessary to reduce or simulate these outlier values as much as possible to improve the performance of the classifier.
%The presence of anomalous events also reduces the performance of actual survey classifications.
%
%Of course, the labeling of the actual data class by the conventional method includes uncertainty, but the difference in the results should be minimized by making the learning data more similar to the actual data.
%
%
%\subsection{Comparison with the best classifier in PLAsTiCC}
\subsection{Comparison with other classifiers}
%
When comparing the classification results of the classifiers, it is difficult to make a direct comparison if there are differences in methods, training datasets, or number of inputs and outputs. 
Here, we simply compare the recent SN type classifier based on machine learning and our classifier with AUC of ROC in the binary classification of Ia or not.
For comparison, we use the classification results of the simulated SN light curves with redshift information from \citet{Lochner_2016}, \citet{charnock17a} and \citet{Muthukrishna_2019}.
\citet{Lochner_2016} obtained AUC of 0.984 by classification with boosted decision trees (BDTs) using the SALT2 fitting parameters as input features.
\citet{charnock17a} reported AUC of 0.986 for classification of SNPCC data using deep recurrent neural networks (RNN) with unidirectional long short-term memory (LSTM) units.
\citet{Muthukrishna_2019} used deep RNN with Gated Recurrent Units (GRUs) to classify simulated ZTF light curves and archived an AUC of 0.99 40 days after the trigger.
The AUC of our classifier, 0.996, is more than comparable to those of these recent classifiers in binary classification.

Next, we compare the results of our classifiers and those ranked first place in the PLAsTiCC Kaggle competition \citep{malz19a}.
The best classifier in the competition \citep{boone19a} is based on Light-GBM and trained with features extracted from photometric data modeled by Gaussian process regression.
The training set of this classifier includes a total of 591,410 light curves by augmenting 100 new light curves under different observation conditions and different redshifts for each light curve of the original PLAsTiCC training set.
It classifies events into 15 classes including variable objects other than SN such as microlensing events and active galactic nuclei.
We use 2,297 PLAsTiCC SN predictions, classified by both classifier, for comparison.
Since the number of output classes is different between our classifier and the best classifier, we divide the classification results into Ia and the other, and compare as binary classification.
Figure\ \ref{fig:comp_plasticc_1st} shows the confusion matrix of each classifier.
While our classifier and PLAsTiCC 1st classifier cannot be strictly compared because of the different training datasets and number of output classes,
%only for the compared SNe, our classifier is about 6\% better in total accuracy than the PLAsTiCC 1st classifier and 6\% better in Ia accuracy.
our classifier has a comparable capability to the PLAsTiCC 1st classifier.
%Considering that our classifier only uses the middle one year data out of the original three year data as input, it classifies PLAsTiCC data with better accuracy with less data than that of PLAsTiCC 1st.
However, since our method fixes the observation schedule to input, it is impossible to apply to all PLAsTiCC data with one classifier.
%How many patterns
Our method is not useful for surveys that sweep a wide area like LSST survey, but for surveys that keep observing at the same field like HSC-SSP Transient Survey.
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/07_CM_PLAsTiCC-1st_submission_aug22_2class_2.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/03_CM_abs-mag_scaled-flux_w-mixup_predictions_test_2.eps}
            \end{center}
        \end{minipage}
    \end{tabular}  \caption{%
    Normalized confusion matrices of two-class classification results for the PLAsTiCC test set by \citet{boone19a} (left) and our best classifier given pseudo-absolute magnitude and normalized flux (right).
    }%                                                                                           
    \label{fig:comp_plasticc_1st}
\end{figure*}
%
%
%
%
\subsection{Data uncertainty}
%
The flux error information is also important for the classification of distant SN Ia, which is the main target of the HSC survey and has a small signal-to-noise ratio.
Here, we discuss how to incorporate errors for classification, including our method.
\citet{charnock17a} adds the flux error itself as part of the input vector.
In the case of feature-based classification, the flux error is used in fitting for calculating the features, and affects those errors.
The flux error is also used to calculate statistical features such as 'Standard Deviation / Mean', which is one of the features shown in \citet{narayan18a} and \citet{Muthukrishna_2019}.
On the other hand, as described in subsection \ref{sec:training}, we calculate the flux error of the simulated training data based on the relationship between the flux and the flux error for each epoch of the observed data.
Then, random numbers that follow a normal distribution defined by the original fluxes and flux errors are input to the classifier as processed fluxes.

The processing of time series data with uncertainty is an issue in many application domains, not only in the astronomy field.
For example, in similarity matching of time series data with uncertainty, it is suggested as a promising method to take into account continuous correlation in time series such as measuring distance after filtering by Uncertain Moving Average (UMA)\citep{Dallachiesa_2012}.
UMA filtering uses the correlation of neighboring points and reduces the contribution of observation points with large errors.
We believe that the development of a method to efficiently incorporate such error information is necessary for improving the classification performance.
%
\subsection{Missing data}
%
Our method is specialized in the HSC survey, where each observed transient tends to have the same cadence by observing a certain field repeatedly.
It can use all photometric information and classify with less pre-processing, compared to the method of reducing dimensions and extracting features.
However, this method is not suitable for processing data with different observation schedules for each object, or samples with missing data.
In other classification methods, missing data is interpolated by linear or Gaussian processes \citep{Lochner_2016,Muthukrishna_2019}, or replaced with reference to the flux of adjacent observation points \citep{charnock17a}.
In the case of the HSC survey, most of all samples can be roughly classified into two types of observation schedules corresponding to Ultra-Deep and Deep layer.
Therefore, as shown in table \ref{tab:class_flag}, we can classify 99.3\% of SNe using only five classifiers with different input schedule cases, including those for data missing samples.
%To classify all objects with our method, we need to prepare multiple classifiers.
%In order to support ultra-wide-range surveys such as LSST, it is necessary to input features that are less affected by the magnitude of the cadence for each transient, and to add processing to interpolate missing values.
%
%
\subsection{Type fractions of HSC SNe}
The elemental abundance of the solar system \citep{grevesse98a} originates from the cosmic history of SNe \citep{maraston05a,kobayashi00a}.  
Recent studies show that the solar abundance pattern is observed in other systems \citep{ramirez09a} and clusters \citep{mernier18a}. 
It is important to investigate the origin of elements in the context of cosmic evolution \citep{fukugita04a}. 

It is now well established that star formation rate is peaking at z$\sim$2, and the chemical composition of our system is a mixture of SN~Ia and Core-Collapse SNe \citep{tsujimoto95a,kobayashi11a}.
Deriving SN rate needs a careful analysis \citep{dilday08a,brown19a,frohmaier19a} and it is beyond the scope of this paper, but at least we can check the consistency with previous work on relative fraction.
Lick Observatory Supernova Survey \citep{li11a} reports the relative ratio between SN~Ia:Ibc:II to be 0.24:0.19:0.57, while we have 0.22:0.19:0.59 even at z$\lesssim$0.2. 

Based on our survey depth, we have a complete sampling of SN~Ia up to z$\sim$1.1 while we lose completeness of Core-Collapse SN in much lower redshift given the fact that the magnitudes of SN~Ibc and SN~II are fainter by 2$\sim$3 mag at maximum.
The trend of SN~II fraction drops while SN~Ia fraction rises towards high redshift as shown in figure \ref{fig:hsc3_type_frac_alongz}. 
It is simply because of this completeness effect due the the magnitude difference and does not reflect the cosmological SN rates.
If we adopt the SN~Ia rate from \citet{graur14a} and Core-Collapse SN rate from \citet{strolger15a}, we can estimate the completeness of Core-Collapse SN. 
%At z$\sim$0.3 Core-Collapse SN completeness is 72\%, and it drops to be 28\% at z$\sim$0.5. 
At z$\sim$0.3 Core-Collapse SN completeness is 78\%, and it drops to be 49\% at z$\sim$0.5. 
The reason why SN~II fraction does not go to zero at z$\sim$1 in Figure \ref{fig:hsc3_type_frac_alongz} is that the magnitude dispersion of Core-Collapse SN \citep[$\sigma$ $\sim$ 1.2 mag]{li11a,kessler19b} is much larger than that of SN~Ia \citep[$\sigma$ $\sim$ 0.5 mag]{rubin15a}, and they are factor of 4-5 more abundant at z$\sim$1 \citep{madau98a,hounsell18a}. 
More careful investigation is needed, but we deduce completeness %of Core-Collapse SN at z$\sim$1 is 16\%.
of Core-Collapse SN at z$\sim$1 is 12\%.

%Cosmological origin of elements depends of the cosmic history of SNe.
%show solar abundance 
%
%
%
\section{Conclusions}
%\begin{itemize}
%\item 
%\item 
%\item our new method(2DGP+LGBM)
%\end{itemize}
%
In this paper, we present a model of a classifier that classifies SN types by directly inputting photometric data, and its classification performance.
Our DNN classifier is trained with simulated SN photometric data and classifies PLAsTiCC data and actual HSC SNe data with high accuracy of 95.3\% and 84.2\% respectively.
The study on the number of input dimensions also shows that our classifier can classify the HSC survey data with sufficient accuracy even for two weeks of pre-peak data from the first detection.
From these facts, we conclude that this classifier has sufficient classification performance for subsequent type-specific studies and selection of follow-up targets even in actual HSC surveys.
%Furthermore, our proposed method is easy to apply to estimation models such as redshift because of simple input. The application of our model to redshift estimation is useful for the selection of host galaxies by comparing with the photo-z information of galaxies.
%
\begin{ack}
The Hyper Suprime-Cam (HSC) collaboration includes the astronomical communities of Japan, Taiwan, and Princeton University. The HSC instrumentation and software were developed by the National Astronomical Observatory of Japan (NAOJ), the Kavli Institute for the Physics and Mathematics of the Universe (Kavli IPMU), the University of Tokyo, the High Energy Accelerator Research Organization (KEK), the Academia Sinica Institute for Astronomy and Astrophysics in Taiwan (ASIAA), and Princeton University. Funding was contributed by the FIRST program from the Japanese Cabinet Office, the Ministry of Education, Culture, Sports, Science and Technology (MEXT), the Japan Society for the Promotion of Science (JSPS), the Japan Science and Technology Agency (JST), the Toray Science Foundation, NAOJ, Kavli IPMU, KEK, ASIAA, and Princeton University.

This paper makes use of software developed for the Large Synoptic Survey Telescope. We thank the LSST Project for making their code available as free software at  http://dm.lsst.org

The Pan-STARRS1 Surveys (PS1) have been made possible through contributions of the Institute for Astronomy, the University of Hawaii, the Pan-STARRS Project Office, the Max-Planck Society and its participating institutes, the Max Planck Institute for Astronomy, Heidelberg and the Max Planck Institute for Extraterrestrial Physics, Garching, The Johns Hopkins University, Durham University, the University of Edinburgh, Queen’s University Belfast, the Harvard-Smithsonian Center for Astrophysics, the Las Cumbres Observatory Global Telescope Network Incorporated, the National Central University of Taiwan, the Space Telescope Science Institute, the National Aeronautics and Space Administration under Grant No. NNX08AR22G issued through the Planetary Science Division of the NASA Science Mission Directorate, the National Science Foundation under Grant No. AST-1238877, the University of Maryland, and Eotvos Lorand University (ELTE) and the Los Alamos National Laboratory.

We thank Y. Imoto for his enormous contribution to the development of the classification model, and the data classification.
We also thank T. J. Moriya, J. Jiang and other members of HSC transient working group for helpful discussions and comments on the manuscript.

%IT, NS and NY acknowledge financial support from JST CREST (JPMHCR1414).
%This work is supported by JST CREST Grant Number JPMHCR1414, Japan.
%NY is supported by the Grant-in-Aid for Scientific Research program of MEXT (18H04345). NS is supported by JSPS (18K03696).
%MT is supported by the Grant-in-Aid for Scientific Research program of MEXT (17H06363) and JSPS (16H02183,19H00694).
This work is supported by JST CREST Grant Number JPMHCR1414, MEXT KAKENHI Grant Numbers 18H04345 (N.Ya.), 17H06363 (M.T.), and JSPS KAKENHI Grant Numbers 18K03696 (N.S.), 16H02183 (M.T.), 19H00694 (M.T.).

This research is based in part on data collected at the Subaru Telescope and retrieved from the HSC data archive system, which is operated by the Subaru Telescope and Astronomy Data Center at NAOJ.
\end{ack}
%
%
\appendix 
\section*{Redshift estimation}
%\subsection{Estimating Redshift}
\label{sec:est_redshift}
In HSC-SSP transient survey, there are SNe without redshift because their host galaxies are not clearly identified, called hostless SNe.
In the redshift list of HSC SNe used for type classification, 6\% (108/1824) correspond to them.
To cope with such SNe and the possibility of host galaxy misidentification,
we can estimate the redshift $z$ of a SN from the photometric data.
%There exists cases that we may not have a photometric redshift (hostless supernvoae) or photometric redshift is not accurate.
%For these cases, we can estimate the redshift $z$ of a SN from the observed brightness.
%We cannot use absolute magnitude to estimate the redshift value because redshift value is required to calculate absolute magnitude.
%Therefore, we used magnitude instead of absolute magnitude as the input of the DNN.

We use a model with the same structure as in figure \ref{fig:dnn_model} except that the DNN output is a scalar, and place no final softmax layer for redshift estimation.
The objective function to optimize is the squared error between the ground truth $z$ and the output value $\hat{y}$.
We measure the accuracy of the model with the coefficient of determination $R^2$, that is,
\begin{eqnarray*}
    R^2 = 1 - \frac{\sum_n \left| z_n - \hat{y}_n \right|^2}{\sum_n \left| z_n - \bar{z} \right|^2}, 
\end{eqnarray*}
where $z_n$ is the redshift value of $n$-th sample, $\hat{y}_n$ is the output of $n$-th sample, 
and $\bar{z}$ is the mean redshift of the dataset.
% $\mathrm{SE} \left(z, y\right) = \frac{1}{2} \left| z - y \right|^2 $
We perform the same hyperparameter search in the model as in subsection \ref{hyperparametersearch} 
including the data augmentation.
The accuracy of this estimator is discussed below.

We apply the standard deviation of normalized residual $(z_{\rm pred}-z_{\rm spec})/(1+z_{\rm spec})$ \citep{Salvato_2009,Salvato_2019}, used in galaxy photo-z estimation, to the performance criteria for redshift estimation.
In verification with Case 0 simulated data, that for Ia and non Ia objects are 0.022 and 0.076, respectively.
For the actual HSC SNe, we used observationally obtained redshifts of host galaxies (see section \ref{sec:h}) including spec-z and photo-z for comparison with estimates.
Figure\ \ref{fig:redshift_estimation} shows the comparison between the estimated redshifts and the observed redshifts for the HSC SNe classified as Ia.
The two classes labeled by SALT2 fitting have different distributions, and the events labeled non Ia are less accurate in redshift estimation than Ia.
The normalized residuals for each of Ia and non Ia Case 0 SNe labeled with SALT2 fitting are normally distributed with standard deviations of 0.066 and 0.138, respectively.
If the comparison is limited to SNe in the host galaxy with spec-z information, they are 0.056 and 0.130.
Although these estimation accuracy is worse than the template fitting accuracy using host galaxy photometric data, it is useful not only for hostless SNe, but also to select host galaxy by comparing the redshift estimated from the SN light curve itself with those of galaxies.
Furthermore, this distributional difference of residuals leads to the fact that the Ia accuracy can be further improved by excluding events with large residuals in the estimated redshift of the SN and host galaxy.
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/redshift_pred_Ia_w_true_label_flagall.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/redshift_pred_Ia_w_true_label_diff_flag0.eps}
            \end{center}
        \end{minipage}
    \end{tabular}  \caption{%
    (Left) Relation between the machine predictions and the observed redshifts for HSC SNe.
    The dashed line is the line of equality.
    The color difference represents SALT2 fit label.
    (Right) Normalized residual distribution for Case 0 SNe.
    }%
    \label{fig:redshift_estimation}
\end{figure*}

%
\bibliographystyle{myaasjournal}
\bibliography{hsc,archive}
%


\end{document}

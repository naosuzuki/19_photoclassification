%%% Notice: This file contains a large number of \verb's 
%%%         or verbatim environments in order to display command names
%%%         or examples.  But the use of \verb/verbatim is *not* recommended. 
%%% ver.6 2015/01/05 
%\documentclass[proof]{pasj01}
\documentclass[useamsfonts]{pasj01}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{color}
%\usepackage{amsmath}


\newcommand{\mt}{\textcolor{blue}}

%
%\draft 
\Received{$\langle$reception date$\rangle$}
\Accepted{$\langle$acception date$\rangle$}
\Published{$\langle$publication date$\rangle$}
%% \SetRunningHead{Astronomical Society of Japan}{Usage of \texttt{pasj00.cls}}

\begin{document}

\title{Photometric Classification of the HSC Transients through Machine Learning}
\author{
Ichiro Takahashi\altaffilmark{1,2,*},
Yasuhiro Imoto\altaffilmark{3},
Nao Suzuki\altaffilmark{1},
Naoki Yasuda\altaffilmark{1},
Akisato Kimura\altaffilmark{3},
Naonori Ueda\altaffilmark{3},
Naoki Yoshida\altaffilmark{1,2,4,5},
%
}%
\altaffiltext{1}{Kavli Institute for the Physics and Mathematics of the Universe (WPI), The University of Tokyo Institutes for Advanced Study, The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba 277-8583, Japan}
\altaffiltext{2}{CREST, JST, 4-1-8 Honcho, Kawaguchi, Saitama 332-0012, Japan}
\altaffiltext{3}{NTT Communication Science Laboratories, 2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan}
\altaffiltext{4}{Department of Physics, Graduate School of Science, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan}
\altaffiltext{5}{Institute for Physics of Intelligence, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan}


%\altaffiltext{}{Astronomical Society of Japan, c/o National Astoronomical Observatory of Japan, 2-21-1 Osawa, Mitaka, Tokyo 181-8588, Japan }
\email{ichiro.takahashi@ipmu.jp}

\KeyWords{supernovae: general --- methods: statistical --- surveys}

\maketitle
%

\begin{abstract}
The progress of observation technology in recent years has brought the rapid increase in the number of discovered supernovae (SNe). More than 1,800 SN candidates were discovered in transient survey with the Subaru Hyper Suprime-Cam (HSC). In order to select follow-up candidates efficiently among SNe discovered in the survey, we study type classification of SNe using Deep Neural Network with highway layers. Our classifier receives photometric data directly, without interpolating them or extracting features. 
%We evaluated it with LSST simulated classification challenge dataset.
%the published dataset of photometric classification challenge for LSST.
%“The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC)” to simulate the learning data. 
%As a result of classifier verification using PLAsTiCC dataset, 
%the classification performances in binary classification and three-class classification are measured as area under the curve (AUC) of 0.996 and accuracy of 95.3\% respectively. 
The evaluation of our classifier with simulated classification challenge dataset for LSST yields the classification performance as area under the curve (AUC) of 0.996 in binary classification, and accuracy of 95.3\% in three-class classification respectively.
When we apply this classifier to the observation data of HSC-SSP Transient Survey conducted from 2016 to 2017, %the AUC score in binary classification is 0.921.
the binary classification AUC score is 0.921 for SNe labeled by the template fitting using SALT2.
%\mt{We also investigated the dependence of the classification perfomance on the number of photometric points.}
We also investigated the dependence of the classification performance on the number of photometric points in HSC observation, and
%We also investigated the classification results while increasing the number of input dimensions only with the complete light curves in the HSC survey, and
%We 
found out the followings: 1) The performance of type classification improves as input increases, and accuracy exceeds 0.8 at 20 photometric points. 2) When considered based on the SN phase, our classifier can classify with 85\% accuracy by inputting two weeks of data from the first detection. These results and findings show that our classifier has sufficient classification performance to select the follow-up targets even in the actual survey.
\end{abstract}

\section{Introduction}
Time domain science becomes one of the major fields of astronomical study today.  The discovery of accelerating universe \citep{perlmutter99a,riess98a} evoked a series of large supernova (SN) surveys in the last decades 
\citep{betoule14a,scolnic18a,brout19a}.  
These surveys revealed that there exist a whole new family of transients and the search for unknown populations is of great interest today \citep{howell06a,phillips07a,quimby07b}. 
For the precision cosmology, it is very important to keep the purity of Type Ia supernova (SN~Ia) while having uniform sampling from bright to faint SNe~Ia.  Spectroscopic follow-up has been essential to distinguish faint SN~Ia from Type Ib supernova (SN~Ib) and Type Ic supernova (SN~Ic) which have similar light curve behavior \citep{scolnic14a}.   A large amount of precious telescope time is dedicated to the follow-up programs, and we would like to make an efficient use of those telescopes.

The scale of survey is getting larger, and it becomes impossible to trigger spectroscopic follow-up for all of the candidates in real time, and we are in need of developing new classification scheme.  It is a natural path to perform photometric classification \citep{sako11a,jonesl8a}.  With the rise of Machine Learning Technologies, it is getting common that an astronomical big data is being analyzed through Machine Learning.   

Neural Network has been used for photometric redshift studies from the early stage.  Today, many Machine Learning methods are applied to the photometric redshift studies \citep{collister04a,carliles10a,pasquet19a}.
Deep Neural Network (DNN) is now being used for imaging data for finding strong lens system \citep{petrillo17a} or for galaxy morphology classifications \citep{hausen19a}.  Now, machine learning is being introduced for transient survey for detection \citep{goldstein15a} and classification \citep{charnock17a}.

In this paper, we introduce our attempt to apply machine learning (DNN) to the real transient survey data.
%our attempt to apply the latest techniques to the real astronomical data is reported in this paper.  
%It is getting to common to see the applications of machine learning in the astronomical community.  (to be continued)
Hyper Suprime-Cam (HSC) on Subaru Telescope enables us to probe wide filed (1.77 deg$^2$ Field of View) and deep space (26th mag in $i-$band / epoch).  Our primary scientific goals are SN~Ia cosmology, Type II supernova (SN~II) cosmology, super-luminous supernova (SLSN) studies as well as probing unknown population of transients.  
As is reported in \citet{yasuda19a}, more than 1800 SNe are discovered in a 6-month campaign. 
We deployed a machine learning (AUC boosting) for transient detection \citep{morii16a}, where 'real' vs. 'bogus' is determined by a machine.
In \citet{kimura17a}, we adopted DNN for SN type classification from 2 dimensional image, and highway block is introduced for the optimization of layers.
This paper is an extension of \citet{kimura17a} and applies DNN to the photometric classification of transients.   
Our unique attempt is to make use of the observed data as 'raw' as possible so that we can directly plug them into machine without any fitting to the data or extracting characteristics.

%The structure of this paper is as follows. We introduce our methods in $\S$2, and data in $\S$3. The DNN Model we construct is described in $\S$4.  We first apply our DNN model to a pseudo-real LSST simulated data in $\S$5 and apply to the real Subaru/HSC data in $\S$5.  We discuss the results in $\S$6 and conclusion is given in $\S$7.
The structure of this paper is as follows. We introduce our methods in $\S$ 2, and data in $\S$ 3. The design of our DNN model and its application to pseudo-real LSST simulation data is described in $\S$ 4. $\S$ 5 shows the application of our model to the real Subaru/HSC data. We discuss the results in $\S$ 6 and conclusion is given in $\S$ 7.
%paper structure
%In $\S$2, 
 
%\subsection{Increase in the number of discovered supernovae}
%\begin{itemize}
%\item DES, LSST
%\item Necessity of machine learning
%\end{itemize}
%\subsection{Supernova survey}
%\begin{itemize}
%\item Past surveys
%\item Subaru/HSC survey
%\end{itemize}
%\subsection{Supernovae type classification}
%\begin{itemize}
%\item SCP
%\item SNPCC
%\item PLAsTiCC
%\end{itemize}
%
\section{Methods}
\subsection{Tasks in HSC Survey}
\label{sec:tasks}
Subaru Hyper-Suprime Cam (HSC) Transient survey is a part of Subaru Strategic Project (SSP) which is a five-year program with 300 dark nights \citep{aihara18a,miyazaki18a}.
HSC-SSP Transient Survey is composed of two seasons, and the first season is executed for a consecutive 6 months from November 2016 through April 2017.
HSC is mounted on the prime focus for 2 weeks in dark time.   
Weather permitted, we aim to observe 2 data points per filter per lunation.
A detailed survey strategies and the observing logs are reported in \citet{yasuda19a}, and we use the observed photometric data described in \citet{yasuda19a}.

One of our primary goals of HSC-SSP Transient Survey is SN~Ia Cosmology which aims to perform the most precise measurement of dark energy at high-redshift and test if dark energy is changing in time or not \citep{linder03b}.
We are awarded 96 orbits of the Hubble Space Telescope time (WFC3 Camera) to execute a precise photometry on IR at the time of maximum.
Our HST program uses non-disrupted ToO which means we send the observing request by Wednesday, 2 weeks prior to the observation.
In other words, we are in need of finding good candidates 2-3 weeks prior to the maximum.  High-redshift (z$>$1) time dilation factor helps, but it is always a challenge to identify SN~Ia on the rise.

Our international collaboration team executes spectroscopic follow-up using large telescopes in the world.  
Our target, high-redshift SN~Ia, is faint ($\sim$ 24th mag in $i-$band) for spectroscopic identification even with the powerful large telescopes: GMOS Gemini \citep{hook04a}, GTC, Keck LRIS \citep{oke95a}, VLT FORS, Subaru FOCAS \citep{kashikawa02a} and AAT AAOMega Spectrograph.  
Thus, it is critical to hit the target at the time of maximum either by ToO (GTC), queue mode (VLT) or classical scheduled observation (Keck, Subaru, ATT).

For SN~Ia, it takes about 18 days from its explosion to the maximum in rest frame \citep{conley06a,papadogiannakis19a}.   Given the fact, we are looking at high-redshift SN~Ia (z$>$1), we have about a month in observed frame for a SN~Ia from explosion to the maximum.   However, our task is to identify them 2 weeks prior to the maximum, meaning we have only 2 data points per a filter.   In addition to that, sky condition keeps changing, and we may not have the data as we originally had planned.   In reality, our identification has to be done through missing data points on the rise.

In parallel to SN~Ia Cosmology program, our sister projects also needs identification and classification of HSC transient.   
SN~II cosmology program needs a timely spectroscopic follow-up to measure the expansion velocity of photosphere from H$\beta$ line \citep{dejaeger17a}.
SLSN is of great interest today, it is relatively a rare event\citep{quimby11a} and its mechanism has quite a diversity \citep{galyam12a,Moriya18SLSN}, and it can probe high redshift universe \citep{cooke12a}.    
For SLSN project, timely spectroscopic follow-up is also critical \citep{moriya19a,curtin19a}.

Early phase SN~Ia gives us clues on explosion mechanism \citep{maeda18a} and progenitors \citep{cao15a}, and it is a hot topic to study.
HSC has an advantage of surveying a large volume and has revealed long standing theory prediction of Helium-shell detonation \citep{Jiang2017}.
Finding early phase SN~Ia is not trivial but HSC is yielding a new set of early phase SN~Ia \citep{jiang20a}.
Observations of the early phase Core Collapse SN provides us with a crucial information on the size of the progenitors \citep{thompson03a,tominaga11a} and Circumstellar Medium \citep{forster18a}.  

%Shock Breakout lasts only for a few $\sim$ several hours at the time of explosion and it becomes as luminous as at the time of the maximum \citep{gezari08a}. 
%Shock Breakout contains the information on projenitor radius \citep{nakar10a}, and HSC is expected to find these candidates as well \citep{tominaga11a}.

%Tidal Disruption Event \citep[TDE]{gezari08b,holoien14a} is also expected to be observed by HSC-SSP Transient Survey.  The light curve is similar to
%the supernova, but it monotonically drops at a specific rate \citep[$t^{-5/3}$]{lodato11a} and the event happens on the core of the galaxy.  
%We expect to have a few events per season \citep{kochanek16a}.

%It is always possible to discover something new when we explore a new parameter space which is the depth for the case
%of Subaru HSC.   Rapid transients are discovered by HSC, but their identities are yet to be known (Tampo et al.  2020 in prep).  
%Similar objects are also reported by DES team \citep{pursianinen18a}.

%In many science cases, it is essential to have a timely classification at the early phase of transient so that we can trigger follow-up observations.
%\citep{maeda18a,jiang18a}
%\citep{moriya19b}

%\begin{itemize}
%\item Classification of SN Ia
%\item Observation schedule
%\item Type classification before peak
%\end{itemize}
\subsection{Classification Method for HSC-SSP Transient Survey}
We designed two models of machine learning with emphasis on identifying SN~Ia which requires time sensitive trigger for HST IR follow-up.  One model is in a binary mode and classifies if a transient is SN~Ia or not.
It has been known that the majority of high-redshift transients are SN~Ia, and we seek for other unknown transients
from the ones labeled as non-SN~Ia.   
The other model classifies a transient into 3 classes: SN~Ia, SN~Ibc and SN~II.  
We choose three classes for simplicity and in fact, the majority of SNe goes to one of these three categories.
SN~Ia is a thermonuclear explosion, and its brightness can be calibrated empirically.
SN~Ib, SN~Ic and SN~II are all core collapse SNe and classified by the spectral features \citep{filippenko97a}.
The light curves of SN~Ib and SN~Ic are similar to that of SN~Ia and it always contaminates SN~Ia cosmology program, and together we call them SN~Ibc.  They are fainter than SN~Ia and redder in general.
One of our challenges in this paper is if we can segregate SN~Ibc from SN~Ia.
%
%We designed a SN type classifier using machine learning specialized for HSC survey.
%
%There are two types of classifiers, which input light curve data directly.
%
%One classifies whether the type of SNe are Ia or not, and the other classifies them into three classes (Type Ia, Ibc and II).
%
%Hundreds of thousands of simulated light curves are used to train the classifier.
%
%By simulating the light curve data and training the machine in advance before observation, type classification of SN can be performed in a short time after observation.
%
\section{Data}
%In this section, we describe three kinds of data we use.
%First, we need a simulated photometric data to train the machine ($\S$ \ref{sec:training}).
%Second, we want to have a realistic dataset which we know the answer so that we can assess the accuracy of the machine ($\S$\ref{sec:validation}).  We would not know how good/bad the machine is without labeled answer.
%Third, we apply our well trained machine to the real observed data ($\S$\ref{sec:hscdata}).
In this section, we present dataset we use for our study.
We first show our SN datset from HSC-SSP transient survey ($\S$ \ref{sec:hscdata}).
Then we describe simulated photometric data to train the machine ($\S$ \ref{sec:training}).
Lastly, we explained the pre-processing of the above data input to the machine ($\S$ \ref{sec:preproc}).

%For the assessment of the accuracy of the machine, we need a dataset which we know the answer.
%As is described in $\S$X, we adopt a simulated photometric data which was generated for 

\subsection{Observed Data from Subaru/HSC-SSP Transient Survey}
\label{sec:hscdata}
The goal of this project is to classify Subaru/HSC observed light curves. 
The discovery of 1824 SNe from a six-month HSC-SSP Transient Survey in the period of
November 2016 through April 2017 are reported and described in \citet{yasuda19a}.
The survey is composed of Ultra-Deep (UD) and Deep layer in COSMOS \citep{scoville07a}.
The median 5$\sigma$ limiting magnitudes per epoch are 26.4, 26.3, 26.0, 25.6, and 24.6 mag (AB)
for $g$-, $r2$-, $i2$-, $z$-, and $y$- bands respectively for UD. 
For Deep layer, the depth is 0.6 mag shallower.

The SN dataset consists of time series photometric data (flux, magnitude and their errors) in each band for each SN.
Because part of the $y$ band photometric data has residuals due to improper background subtraction influenced by scattered light \citep{aihara18dr},
we exclude the $y$ band data in our study, considering the impact on classification performance.

%redshift info
The redshift information for HSC SNe is combination of our follow-up observation results and catalogs from multiple surveys of those host galaxies.
The spectral redshifts (spec-z) are adopted from the results of the follow-up spectrum observations by AAT/AAOmega performed in 2018 and that from DEIMOS \citep{DEIMOS2018}, FMOS-COSMOS \citep{FMOS-COSMOS2015}, C3R2 \citep{C3R2_2017}, PRIMUS \citep{PRIMUS2011} and COSMOS catalogs.
For those without spec-z, the photometric redshifts (photo-z) are adopted from the COSMOS catalog and those calculated from the HSC-SSP survey data \citep{HSCSSP_photo-z2018}.



\subsection{Simulated Data for Training}
\label{sec:training}
We are in need of simulating observed photometric data to train the machine.  
For normal SN~Ia, we use SALT2 \citep{guy10b} model ({\it ver} 2.4) which requires 2 input parameters: c for color and x1 for stretch.
We adopt the asymmetric Gaussian distribution of c, and x1 from \citet{mosher14a}, and generate
light curves and simulate photometric data points based on the observation schedule. 
Besides SN~Ia, we use spectral time series from \citet{kessler19b} which contains both observed
core collapse data and light curves from simulation.
We mix equal number of SN~Ia and Non-SN~Ia to approximate the observed fractions. 
These fraction does not need to be exact for our purpose of classification, but it is important
that the minority is not buried in the majority.
For three class classification, we set the ratio of SN~Ia:SN~Ibc:SN~II=10:3:7.
The redshfit distribution of galaxies is taken from COSMOS survey \citep{laigle16a}, and we distributed simulated SN accordingly from $z$=0.1 through $z=$2.0.
Throughout this paper, we use $\Lambda$CDM cosmology with $\Omega_{m}$=0.3 and h=0.7.
We use filter response including system throughput from \citet{kawanomoto18a}. 
Examples of simulated photometric data is shown in Figure \ref{fig:simLCsamples}.

One complication in simulating realistic data is that the machine does not accept expected errors, or we have not identified a good method to include errors.   
For this study, we take a path of brute force, namely, we place expected photometric error on top of the simulated data so that the simulated data point behaves like one of the many realizations.
For HSC simulation, the size of error varies night to night due to sky condition. 
We measure and derive flux vs. error relationship from the actual observed data at simulating epoch and apply that relationship to the simulated photometric data.
%For LSST simulation, we find the flux-error relation from the Deep Drilling Field (DDF) of the one used in Kaggle Competition \citep{malz19a} and applied that to the simulated photometric data.

We have tested how many light curves are needed for training.  We use "accuracy" ($\S$ \ref{hyperparametersearch}) as a guide on how many light curves to use.  
Based on the our convergence test, we conclude we need to generate more than 100,000 light curves for training as shown Figure \ref{fig:size_convergence_test}.
We dropped ones which has less than 3$\sigma$ detection at the maximum since our detection criterion is 5$\sigma$.  %Prior to the modeling, we did the convergence test
For training, 514,954 light curves are generated for HSC observed data. 
Their class ratio is SN~Ia:Ibc:II=0.59:0.07:0.34, and the peak timings of them are randomly shifted by 450 days.
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/SimLCsamples.eps}
  \end{center}
  \vspace{-6mm}
  \caption{%
  Overlay plots of simulated light curves with z between 0.1 and 1.2.
  Each panel shows the case of Ia, Ibc, II from the left, and $g$, $r2$, $i2$, $z$, $y$ band from the bottom.
  No noise component is added to these light curves.
  }%
  
  \label{fig:simLCsamples}
\end{figure}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/size_accuracy.eps}
  \end{center}
  \caption{%
%  the relation between the number of samples of training dataset and the accuracy measured with test dataset. 
A convergence test: We tested how many light curves needs to be generated to train a machine. 
The solid line shows the mean accuracy of five classifiers. The shade area shows the standard deviation of the classifiers. The classifiers are trained with 5 fold cross validation using the training dataset.  We conclude we need more than 100,000 light curves for training.
  }%
  \label{fig:size_convergence_test}
\end{figure}
%


%We used SNANA \citep{Kessler_2009} and time series SED called "template" to simulate the light curve dataset for classifier training.
%The PLAsTiCC templates published after the completion of PLAsTiCC \citep{plasticc_models} were adopted for our simulation.
%need citation
%We prepared simulated light curves of three classes of SNe, Ia and Ibc type II.
%The figure \ref{fig:simLCsamples} shows examples of the simulated light curves.
%
%
%\begin{itemize}
%\item Used model and parameters
%\item Type distribution
%\item Simulated light curve samples
%\end{itemize}
%
%Ia light curves are simulated using SALT-II light curve model \citep{Guy_2010}.
%The stretch and color values are distributed in the range of $-2<x_1<+2$ and $-0.3<c<0.3$ according to "G10'" model in \citet{Mosher_2014}.

%Non-Ia light curves are generated based on the PLAsTiCC template and using redshift as a parameter.
%The number of Ia and non Ia light curves has been adjusted to a ratio of 1:1.
%The ratio of Ibc and II is set to Ibc:II=3:7 with reference to that of the simulated dataset for PLAsTiCC \citep{plasticc_models}.
%In other words, the numbers of the three classes have a ratio of Ia:Ibc:II=0.5:0.15:0.35.
%The selection weights of individual SED time-series models in each of the Ibc and II classes are also matched to those in the PLAsTiCC dataset.
%The redshift is selected in the range of 0.1 to 2.0, and the event rate depends on the number distribution of galaxies in the COSMOS2015 catalog \citep{2016ApJS..224...24L}.
%In addition, the timing at which SNs occur is shifted by randomly setting an offset that indicates the difference between the SN phase and the observation schedule.
%Of all the light curves that are finally created, faint events with a maximum signal-to-noise ratio of less than three are rejected.
%
\subsection{Preprocessing of Input Data}
\label{sec:preproc}
% われわれはシミュレーションデータを利用した実験の中で入力xは絶対等級と最大値が1になるようにスケーリングしたfluxを組み合わせるのが良いことを発見した。
% 厳密な絶対等級の計算は複雑な計算が要求されるので、簡単のために以下の式を用いた。
% また、等級の計算は観測したfluxが負の値の場合もあるので、Luptitude(Lupton magnitude)を用いて対応した。
%We found that in experiments using simulation data, it is better to be input x follow as,
Based on our pre-experiment with simulated dataset, we found that the machines performs best if we 
chose a combination of normalized flux ($f$) and pseudo-absolute magnitude ($M$):
\begin{equation}
    x = \left( M_1^\mathrm{abs}, \ldots, M_P^\mathrm{abs}, f_{1}^{\mathrm{scale}}, \ldots, f_{P}^{\mathrm{scale}} \right)^T,
\end{equation}
where $f_{i}^{\mathrm{scale}}$ is $i$-th raw observed flux normalized by its maximum flux:
\begin{equation}
    f_{i}^{\mathrm{scale}} = \frac{f_i}{\max \left(f_1, \ldots, f_P \right)},    \label{eq:scaled_flux}
\end{equation}
and $M_i^\mathrm{abs}$ is $i$-th pseudo-absolute observed magnitude.
For a simplicity, we ignore K-correction and use distance modulus (DM(z)) based on $\Lambda$CDM with photometric redshift
from its host galaxy.
\begin{eqnarray}
    M_i^\mathrm{abs} = m_i - \mathrm{DM}\left(z\right),
\end{eqnarray}
We can justify this operation because we treat the training set and the observed dataset in the same way.
It there existed K-correction offset, both dataset would experience in the same way.
%where $m_i$ is i-th magnitude and $\mathrm{DM}\left(z\right)$ is distance modulus of the redshift value $z$.
In addition, since the observed flux could go to a negative value due to statistical fluctuation, 
we adopt hyperbolic sine to imitate the magnitude system we use \citep{lupton99a}.  
%the calculation of the magnitude was dealt with using Luptitude (Lupton magnitude). %simplified Lupton magnitude 
\begin{eqnarray}
    m_i = 27.0 - \frac{2.5}{\log 10} \sinh^{-1} \frac{f_i}{2}. \label{eq:mag} 
\end{eqnarray}
In fact, the combination of the flux and magnitudes are redundant.   If we know one, we can calculate the other
in an explicit way.   
However, based on our experiment, the score of machine gets better if we use both.  
We suspect the distribution in flux (linear) and magnitude space (log) are different, 
and it gives an extra clue to the machine.
Thus, we use pseudo-absolute magnitude ($M$) and normalized flux ($f$) as an input.

\section{Deep Neural Network Classifier}
\label{sec:DNN}
On the rise of the era of Big Data, the use of machine learning technique plays a critical role on the analysis of astronomical data.  Techniques such as Random Forest, Support Vector Machine, Convolution Neural Network are being used for photometric data analysis \citep{pasquet19a}, galaxy classifications \citep{hausen19a} and spectral 
classifications \citep{garciadias18a,muthukrishna19c,sharma20a}.

We are in need of classifying SN from photometric data.
We attempt to make use of the observed data as it is without parameterization.
It is the Deep Learning that makes this attempt possible.
On this paper, we test how good deep learning can do without extracting features such as color, light curve width and the peak magnitude.
%It has been proven among PLAsTiCC competition that a combination of features can perform very well \citep{boone19a}, but it needs the data augmentation, interpolation or extrapolation to extract features.

We push one step further and use the observed data as raw as possible, meaning our input is a simple array of magnitudes.  Such attempt would not be possible ten years ago, but thanks for the advancement of computing and deep learning technique, now it becomes reality.  Among many other machine learning methods, deep neural network is our choice which enables us to classify astronomical objects from the raw observed data. 

\subsection{Model Design}
\label{sec:model} %added by IT
In this section, we describe our design of DNN model which takes an array of observed magnitudes as an input and outputs SN classification with probabilities.  We adopted Highway Layer \citep{srivastava15a} as a core part of our network which is also called 'layer in layer'.  Compared to the plain DNN, Highway Layer can perform better when the network is deep in terms of parameter optimization \citep{srivastava15b}.   
Just like the other DNN model, this model goes through training, validation process to optimize parameters, and we describe the steps below.  The terminology we use is common in the world of DNN, but this is a new introduction to astronomical community, we spell out each step one by one.  The architecture of our model is summarized in Figure \ref{fig:dnn_model}.
At the end, each SN is given a probability of being a certain astrophysical class, in our case, SN types. 
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/model_all.eps}
  \end{center}
  \caption{\label{dnnmodel}
  The architecture of the deep neural network classifier. 
  The green boxes are parameters which are optimized by gradient descent method during training. The red boxes are hyper-parameters which are optimized by the hyper-parameter search. 
  Batch normalization layer has four variables ($\mu, \sigma^2, \gamma, \beta$). $\mu$ and $\sigma^2$ are to learn the statistics (mean and variance) of the value through the layer respectively.  $\gamma$ and $\beta$ are scale parameter and shift parameter to adjust the output. $\mu$ and $\sigma^2$ are are not updated by gradient descent method and are updated by moving average. We omit them from the figure for simplicity.
  }%
  \label{fig:dnn_model}
\end{figure}
%


{\bf Input}: Our input is an array of magnitudes and normalized flux of a SN $i$ :
\begin{equation}
      x_i = \left( M_{i1}, M_{i2}, \ldots M_{ij} \ldots , M_{iN}, f_{i1}, f_{i2}, \ldots, f_{iN} \right)^T
\end{equation}
We do not use time or filter explicitly but it is recorded as an order inside the array.  The philosophy here is that the training set which is composed of the simulated data in the same array length holds the information on filter and dates.  For example, $j$-th magnitude in the array is the data taken on a certain date and by a certain filter.  The combination of the date and filter is identical to the ones in the training set.  Therefore, $j$-th component implicitly contains unique information about dates and filters.  
%{\bf Memo Distance Modulus \& Normalized Flux needs to be written for preprocessing}
%Based on our experiments (Table \ref{tab:p_test}), the combination of absolute magnitude and normalized flux performs best, and therefore, our
If input is a combination of magnitude and normalized flux,
our input array has a size of $1\times2N$ per a SN where $N$ is the number of data points.

%The DNN model receives the total of P brightnesses observed in multiple bands as an input and outputs $C$ values, where $C$ is the number of classes in classification task, and $C = 1$ in regression task.

{\bf First Fully Connected Layer:}
We would like to make use of $d$ neurons which is also known as the number of 'hidden layer', and $d$ is greater than the number of input components (2$N$).  However, the dimension $d$ is not known in advance, and this is one of the hyperparameters we optimize later in this section.  Since the dimension of input (2N) and the number of optimized neurons $d$ could be different, we are in need of adjusting the dimensions and that is the role of this First Fully Connected Layer F(x).    
\begin{equation}
    F \left(x, \left\{W,b\right\}\right) = W x + b.
\end{equation}
F(x) is given by a linear combination of matrix W(2N$\times$ d) and an array b (1$\times$ d), and the initial value is generated by Gaussian distribution and b is initialized by zero.  We use a python wrapper library called {\it dm-sonnet} and its function called {\it linear} supplies the F(x) when we plug in '2N' and 'd'.  The initial value of the matrix $W$ is generated by Gaussian distribution and b is initialized by zero.  Later on, W and b are optimised by an open source Machine Learning package, {\it Tensorflow}.  Unless we mention otherwise, we use libraries from {\it Tensorflow}.

{\bf Dropout Layer:}
In order to have a robust result, it is always best to train the all of the neurons as an ensemble and avoid one of the neurons drags the result.  Dropout is a process which randomly drops some of the neurons from the training and the rate of dropout can be optimized as one of the hyperparameters \citep{dropout}. 

{\bf Highway layer:}
We adopted Highway Layer \citep{srivastava15a} and the number of layers is optimized through hyperparameter search.   In theory, we can design a very deep layer and the number of layers can be more than we need.   In fact, it is not trivial to find the optimized number of layers.   
%In this study, we adopted Highway Layer \citep{highway} as the basic part of the network structure.
The depth and/or layer size of the DNN model is directly related to the complexity of the features that we input, and greatly affect the performance of the task.  
However, if a model is too deep, the learning process becomes difficult and causes performance degradation.
%There are many studies to solve the problem such as optimization, initialization, and network architecture.
Highway layer is a technique that stabilizes learning process by devising the network structure.
In \citet{kimura17a}, we have used Highway Layer and tested on 2D image, and it proved a good performance.  The details of the use of Highway Layer is described in \citet{kimura17a}.  We continue to adopt Highway Layer scheme for this analysis.
%\begin{equation}
%Highway\left(h\right) = H\left(h\right) \otimes T \left(h\right) + h \otimes C\left(h\right)
%\end{equation}
%Highway layer, shown in Fig.\ref{fig:dnn_model}, has multiple paths from the input to the output.
The output of Highway Layer is calculated from the values of the multiple paths.
The output, ${\bf Highway} \left(x\right)$, is formulated as
\begin{equation}
    {\bf Highway} \left(x\right) = T \left(x\right) \cdot H \left(x\right) + C \left(x\right) \cdot x,
\end{equation}
where $H$ is a non-linear transformation layer. $T$ is the transformation gate function layer and controls the transformation of input.  $C$ is the carry gate function layer.
%The two gate functions $T$ and $C$ control how much of the outputs which are transforming the input and carrying it, respectively.
%One gate functions, $T$, controls the transformation of input and the other gate function, $C$, controls  
Highway layer includes other layers inside.  This structure is called layer in layer.  Each function is defined as follows: 
\begin{eqnarray}
    H \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_H, b_H\right\}\right) \right), \\
    T \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_T, b_T\right\}\right) \right), \\
    C \left(x\right) &=& 1 - T \left(x\right),
\end{eqnarray}
where $a$ is an activation function, namely sigmoid.
\begin{eqnarray*}
    \mathrm{a} \left(p\right) &=& \left( \sigma\left(p_1\right),\sigma\left(p_2\right), \ldots, \sigma\left(p_d\right) \right)^T, \; p \in \mathbb{R}^d, \\
    \sigma \left(p_i\right) &=& \frac{1}{1 + e^{-p_i}},
\end{eqnarray*}
Note $d$ is the number of neurons.  $T(x)$ is expected to take values between 0 and 1.  At the end values, highway layer behaves as follows:
\begin{equation}
    {\bf Highway(x)}=\left\{
    \begin{array}{@{}ll@{}}
    %\begin{cases}
      x, & \mathrm{if} \ T \left(x\right)=0 \\
      H(x), & \mathrm{if} \ T \left(x\right)=1 
    %\end{cases}
    \end{array}\right.
\end{equation}
% われわれはhighway blockというものを定め、highway blockの個数を変えることでネットワークの大きさを調整した。
We define a highway block which is composed of three layers: Highway Layer, Batch Normalization Layer and Activation Layer.
%and adjusted the size of the network by changing the number of highway blocks.
% highway blockは図2のようにhighway layerを含む最大で三つの層からなる。
%The highway block is composed of a maximum of three layers including a highway layer as shown in Figure \ref{fig:dnn_model}.
% ただし、highway layer以外の層については後述するハイパーパラメータ探索により決定した。
%However, layers other than Highway layer are determined by a hyperparameter search described later.
% モデルの大きさなどについてはハイパーパラメータ探索を行い決定した。
% 探索を行ったハイパーパラメータは、Highway Blockの個数$N_{block}$、Highway Blockの隠れ層の次元数$d$、ドロップアウトの割合、Batch Normalizationの有無、活性化関数の種類である。
This block itself is laid multiple times.
%and this number of blocks $N_{bloack}$ is one of the hyperparameters and optimized later. 
%The optimal structure of the network was determined by hyperparameter search.
Along with the hidden layer dimensions $d$, the dropout ratio, batch normalization, and the types of activation function, the number of highway blocks $N_{block}$ is one of the hyperparameters and optimized through hyperparameter search.   The details are described in $\S$ \ref{hyperparametersearch}.
%The subjects of hyperparameter search are the number of highway blocks $N_{block}$, the number of hidden layer dimensions $d$ of highway blocks, the dropout ratio, the presence of batch normalization, and the type of activation function.

{\bf Batch Normalization Layer:}
We adopt Batch Normalization \citep{batch_norm} to accelerate and stabilize the optimization.  Even if it may be a large number of parameters we need to train, Batch Normalization helps converge the training, reduce errors on the slope when we apply entropy minimization, prevents the average and dispersion goes exponentially large in deep layers, and minimize the biases on outputs \citep{understanding_batch_norm}.
%BNを使うと大きな学習係数でも収束する、精度が高くなるメリットがある。それは、勾配の誤差を減らせること、深い層になるほど平均と分散が指数的に大きくなるのを防げること、出力の偏りを解消することなどによる(\cite{understanding_batch_norm})。
Batch Normalization works well in many cases. However, if both Batch normalization and Dropout are in the model, the performance may degrade (\cite{dropout_and_batch_norm}).   
%ハイパーパラメータ$bn$=1ならBatch normalizationを使って、$bn$=0なら使わない。　実装はdm-sonnetのBatchNormV2クラスを利用した。

{\bf Activation Layer:}
Each neuron is activated through a non-linear transformation.  Non-linearity is an important component for DNN which gives us a wide variety of expression.   Note the majority of the layers, including Fully Connected Layers, are a linear transformation and even if we have a number of layers, it is equivalent of one single linear transformation.   Thus, it is essential to have a non-linear transformation so that each neuron can have a freedom to take any values necessary.
For the first iteration, we do not know what kind of transformation is the best, the transformation itself is taken as one of the hyperparameters.  The functions, 'tf.nn', in {\it Tensorflow} is being used.
%非線形な変換を行う。非線形な変換はDNNにおいてモデルの表現力を支える重要な要素である。Fully Connected layerをはじめとする多くのlayerは線形な変換になっている。線形なlayerのみの場合はいくらモデルを深くしようとも単一のlayerと等価なので、十分な表現力を得られない。そのため各ニューロンが様々な値を表現するために非線形な変換は必要である。
%変換の種類は何が良いかわからないので、恒等変換も含めてその変換の種類をハイパーパラメータとして後述の方法で最適化した。
%実装はtf.nnの中にあるそれぞれの関数を利用した。

{\bf Second Fully Connected Layer:}
%D次元からC次元に隠れ層の次元数を変換する。
After Highway Layer Block is stacked for 3-5 times, we are in need of converting the number of neurons to the number of SNe times the number of SN classes.   This is an opposite operation of the first fully connected layers.

{\bf Softmax layer:}
Softmax layer normalize the value $h$ with following formula,
\begin{equation}
    \hat{y}_j = \frac{\exp \left( h_j \right)}{\sum_{k=1}^C \exp \left( h_k \right)}.
\end{equation}
The normalized value $\hat{y}$ satisfies $\hat{y}_j \geq 0$ and $\sum_j^C \hat{y}_j =1$.
We can interpret $\hat{y}_j$ as the probability that the input $x$ belongs to  class $c=j$.
However, we note that this is a pseudo probability and it differs from the one we use in statistics.
  
\subsection{Hyperparameter Search}\label{hyperparametersearch}
% fig.\ref{fig:dnn_model}の赤色の背景の値をDNNの精度が最も高くなるように決定する。ハイパーパラメータの値とDNNの精度の関係を陽に求めることは非常に難しく計算コストも高いので、それらの間の関係はブラックボックスとしてハイパーパラメータの最適化を行う。
% 今回、われわれは最初にグリッドサーチを行い、その後、TPEアルゴリズムによる探索を行った。グリッドサーチは探索点を任意に指定できることと複数の点の評価を同時に行えるので時間的な効率が良いことから採用した。また、TPEアルゴリズムは探索履歴に基づいて次の探索点を決定するので、履歴にグリッドサーチの結果を含めることでグリッドサーチでは見つけられなかったより良い点を探索できる。
% グリッドサーチの探索グリッドは予備実験で良かった点とその周辺の点を設定した。各グリッド点の評価は並列で行った。
% ハイパーパラメータの探索にはグリッドサーチとTPE(Tree-structured Parzen Estimator)アルゴリズムを組み合わせる方法で行った。
We perform a hyperparameter search (red boxes in Figure \ref{fig:dnn_model}) by combining grid search and TPE (Tree-structured Parzen Estimator) algorithm.
% グリッドサーチは高次元空間の探索はあまり適していないが、並列に複数の点を探索できるメリットがある。
Grid search is not suitable for high-dimensional space search, but has an advantage of searching for multiple points in parallel.
% 加えて、アルゴリズムが単純なので、予備実験の結果などの知見を探索点の設定に反映させやすいメリットがある。
In addition, since the algorithm is simple, there is another advantage to convery the knowledge of preliminary experiments for parameter initialization.
% 一方で、TPEアルゴリズムは高次元空間の探索に適しているが、現在までの探索結果に基づいて次の探索点を決定するので探索が逐次的になるデメリットがある。
On the other hand, the TPE algorithm is suitable for searching a high-dimensional space, but has a disadvantage of not knowing where to start for the first time.
%he search is sequential because the next search point is determined based on the search results up to now.
% そこで今回は予備実験でよかったハイパーパラメータの周辺をグリッドサーチで探索を行い、続いてその結果に基づいてTPEアルゴリズムでの探索を行った。
Therefore, this time, we searched around the hyperparameters that were good in the preliminary experiment by grid search, and then searched by TPE algorithm based 
on the grid search result.
The ranges of hyperparameters that we searched are given in Table \ref{tb:hp}.
% 組み合わせることで、予備実験で得た知識を利用でき、探索にかかる時間を短縮できるメリットがある。
%By combining them, there are merits that the knowledge obtained in the preliminary experiment can be used and the time required for the search can be shortened.

%% DNNのハイパーパラメータはoptunaを用いて最適化を行った。
% ハイパーパラメータの最適化は、学習用データセットを訓練データとバリデーションデータに分割して、バリデーションデータでの精度が最も高くなるように最適化した。
% 最適化によって得られたハイパーパラメータを用いてモデルの最適化を行い、そのモデルの性能の評価を行った。
%% note: 参考文献にはOputunaのソースコード https://github.com/pfnet/optunaではなく、元論文の方を指定した
% We optimized the hyperparameters of the DNN model using the library, optuna(\cite{optuna}).
As a usual strategy, we divide the dataset into a training data and a validation dataset.
We use the training data for optimizing DNN, and use the validation data for measuring the accuracy.
In optimizing the hyperparameters, we evaluated the hyperparameters with validation dataset so
that the accuracy of validation dataset becomes maximum.
We iterate this process for 100 times so that the accuracy converges to its maximum (Figure \ref{fig:hp_test}).
The optimized hyperparameters are listed in Table \ref{tb:searched_hp_class}.

%We trained the DNN model using the hyper parameters obtained by optimization, and evaluated the performance of the DNN model.
%
\begin{table}[htbp]
  \tbl{Ranges of hyperparameter search for Type Classification}{
      \begin{tabular}{lcc}
        \noalign{\vskip 2mm}
        \hline
        hyper parameter     & value (grid)  & range (TPE)\\ \hline 
        D                   & \{100, 300\}  & 50, \ldots, 1000   \\
        T                   & \{1, 3, 5\}   & 1, \ldots, 5       \\
        bn                  & \{true\}      & \{true, false\}    \\
        dropout rate        & [5e-3, 0.035] & [5e-4, 0.25]       \\
        activation function & \multicolumn{2}{c}{\{identity, relu, sigmoid, tanh\}} \\
        \hline
      \end{tabular}
  }\label{tb:hp}
\end{table}

\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/hp_iterations_accuracy.eps}
  \end{center}
  \caption{%
  We iterate hyperparameter search for 100 times where it converges to its maximum performance in terms of accuracy.   What is shown is the case of a binary classification.
  }%
  \label{fig:hp_test}
\end{figure}
%
%
\begin{table*}[t]
  \tbl{Optimized hyperparameters for classification}{
      \begin{tabular}{lcccllllllllll}
      \noalign{\vskip 1mm}
\hline
%      & $M$        & $m$        & $f$        & the number of blocks T & hidden size D & drop rate & bn & type    \\ \hline
      & \multicolumn{3}{l}{Input$^*$}            & \multicolumn{5}{l}{Two-Class}      &  \multicolumn{5}{l}{Three-Class}   \\ \hline
      & $M$        & $m$        & $f$        & T & D   & drop rate & bn & type    & T & D   & drop rate & bn & type    \\ \hline      
Case 0& \checkmark &            & \checkmark & 5 & 178 & 9.47e-3   & 1  & sigmoid & 4 & 429 & 1.20e-3   & 0  & linear  \\
      & \checkmark &            &            & 3 & 247 & 9.68e-4   & 1  & sigmoid & 4 & 516 & 2.54e-3   & 0  & tanh    \\
      &            & \checkmark & \checkmark & 4 & 531 & 6.43e-3   & 0  & linear  & 4 & 608 & 1.72e-2   & 0  & linear  \\
      &            & \checkmark &            & 4 & 411 & 9.00e-2   & 1  & sigmoid & 4 & 838 & 1.36e-3   & 0  & tanh    \\ \hline
Case 1& \checkmark &            & \checkmark & 5 & 734 & 8.75e-4   & 0  & tanh    & 4 & 915 & 1.03e-2   & 0  & linear  \\
      & \checkmark &            &            & 2 & 389 & 7.17e-2   & 1  & sigmoid & 5 & 698 & 2.79e-2   & 0  & linear  \\
      &            & \checkmark & \checkmark & 2 & 647 & 7.92e-4   & 0  & tanh    & 4 & 540 & 1.45e-3   & 0  & linear  \\
      &            & \checkmark &            & 2 & 342 & 1.42e-2   & 1  & sigmoid & 2 & 520 & 9.99e-4   & 1  & sigmoid \\ \hline
Case 2& \checkmark &            & \checkmark & 2 & 368 & 1.79e-3   & 1  & sigmoid & 4 & 698 & 9.08e-4   & 0  & linear  \\
      & \checkmark &            &            & 4 & 920 & 1.44e-3   & 0  & sigmoid & 4 & 614 & 7.03e-3   & 0  & linear  \\
      &            & \checkmark & \checkmark & 5 & 572 & 4.27e-3   & 1  & sigmoid & 4 & 896 & 8.58e-3   & 0  & linear  \\
      &            & \checkmark &            & 4 & 640 & 1.12e-1   & 1  & sigmoid & 5 & 300 & 5.00e-1   & 1  & sigmoid \\ \hline
Case 3& \checkmark &            & \checkmark & 5 & 893 & 1.42e-3   & 1  & sigmoid & 4 & 522 & 5.02e-4   & 0  & tanh    \\
      & \checkmark &            &            & 4 & 880 & 1.98e-2   & 1  & sigmoid & 5 & 841 & 4.96e-2   & 1  & sigmoid \\
      &            & \checkmark & \checkmark & 3 & 300 & 5.00e-3   & 1  & linear  & 3 & 462 & 9.28e-4   & 1  & sigmoid \\
      &            & \checkmark &            & 3 & 930 & 9.77e-2   & 1  & sigmoid & 3 & 300 & 5.00e-3   & 1  & sigmoid \\ \hline
Case 4& \checkmark &            & \checkmark & 5 & 379 & 4.21e-3   & 1  & sigmoid & 3 & 484 & 2.13e-3   & 0  & linear  \\
      & \checkmark &            &            & 2 & 631 & 1.77e-2   & 0  & sigmoid & 4 & 243 & 3.21e-3   & 1  & sigmoid \\
      &            & \checkmark & \checkmark & 5 & 140 & 4.04e-3   & 1  & sigmoid & 5 & 389 & 1.23e-4   & 0  & tanh    \\
      &            & \checkmark &            & 4 & 567 & 5.77e-2   & 0  & sigmoid & 3 & 354 & 2.14e-1   & 1  & sigmoid \\ \hline
\end{tabular}
  }\label{tb:searched_hp_class}
\begin{tabnote}
* Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table*}

%
% このセクションでは超新星のタイプ分類とredshiftの推定に利用した手法について説明する。
% 続いて、モデルの学習方法とモデルのハイパーパラメータの決定方法についても説明する。
%In this section, we describe the method used for SN type classification and regression of the value of redshift.
%Next, we will explain how to learn the model and how to determine the hyperparameters of the model.

%DNN is one of machine learning methods inspired by the biological brain system.
%DNN combines a large number of elements that mimic neurons, and each element has only a simple function, but it can obtain complex functions as a whole.
% DNNは明示的に特徴を与えることなく、与えられた入力から目的のタスクを解くために必要な特徴を自動的に獲得する
%DNN model can obtain the features needed to solve the target task from the given input without explicitly giving the features.
%Our goal is to classify the type of the celestial body or to estimate the redshift value from the observed light curve.
% われわれはlight curveのタイプ分類にdeep neural network(DNN)を利用した。
% このDNNは、複数のバンドで観測された合計M個の明るさを入力として$K$個の値を出力する。ここで、$K$はクラス数である。
%We used DNN model for them.
%The DNN model receives the total of P brightnesses observed in multiple bands as an input and outputs $C$ values, where $C$ is the number of classes in classification task, and $C = 1$ in regression task.
% 本研究ではfig \ref{fig:dnn_model} に示す構造のモデルを利用した。
%We used the structural model shown in Fig.\ref{fig:dnn_model}.

% 本研究ではネットワーク構造の基本的な部分にHighway Layerを採用した
% dnnモデルの深さや層の大きさはその獲得できる特徴の複雑さに直結して、タスクに対しての性能にも大きく影響する
% 一方で、深すぎるモデルは学習が困難になり、かえって性能の低下を引き起こす
% その問題の解決のためにoptimizer、初期化、ネットワーク構造の工夫など、数多くの研究が存在する
% Highway layerはネットワークの構造を工夫することで学習を安定化させる手法である。
%In this study, we adopted Highway Layer \citep{highway} as the basic part of the network structure.
%The depth and/or layer size of the DNN model are directly related to the complexity of the features that can be obtained, and greatly affect the %performance for the task.
%However, a too deep model is difficult to learn and rather causes performance degradation.
%There are many studies to solve the problem, such as optimization, initialization, and network architecture.
%Highway layer is a technique that stabilizes learning by devising the network structure.

% highway layerは図2に示す通り、入力から出力までの経路が複数存在している。
% highway layerの出力はその複数の経路の値から作られる。
% 出力は以下の式であらわされる。

%Highway layer, shown in Fig.\ref{fig:dnn_model}, has multiple paths from the input to the output.
%The output of Highway Layer is calculated from the values of the multiple paths.
%The output, ${\bf Highway} \left(x\right)$, is formulated as
%\begin{equation}
%    {\bf Highway} \left(x\right) = T \left(x\right) \cdot H \left(x\right) + C \left(x\right) \cdot x,
%\end{equation}
%where $H$ is a non-linear transformation layer, $T$ is the transformation-gate function layer, and $C$ is the carry-gate function layer.
%The two gate functions $T$ and $C$ control how much of the outputs which are transforming the input and carrying it, respectively.
%Highway layer includes other layers inside.
%This structure is called layer in layer.

%We defined the functions as 
%\begin{eqnarray}
%    H \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_H, b_H\right\}\right) \right), \\
%    T \left(x\right) &=& \mathrm{a} \left( F \left(x, \left\{W_T, b_T\right\}\right) \right), \\
%    C \left(x\right) &=& 1 - T \left(x\right),
%\end{eqnarray}
%where $a$ is an activation function, that is,
%\begin{eqnarray*}
%    \mathrm{a} \left(z\right) &=& \left( \sigma\left(z_1\right),\sigma\left(z_2\right), \ldots, %\sigma\left(z_d\right) \right)^T, \; z \in \mathbb{R}^d, \\
%    \sigma \left(z_i\right) &=& \frac{1}{1 + e^{-z_i}},
%\end{eqnarray*}
%$d$ is the size of the dimension of the input and the output of Highway Layer, and
%$F$ is a fully connected layer parameterized by $W \in \mathbb{R}^{d \times d}$ and $b \in \mathbb{R}^d$, is formulated as
%\begin{equation}
%    F \left(x, \left\{W,b\right\}\right) = W x + b.
%\end{equation}
%Fully connected layer is a popular and basic layer of DNN.
%This formulation of Highway layer is same as (\cite{highway}).

% 二つの値の混ぜ合わせによって、highway layerの出力が作られる。
% ゲートT=1の場合は通常のfully conected layerと等しい。
% 逆にC=1の場合は恒等変換でlayerがないのと等しく、ネットワークの実質的な深さが減少する。この場合、浅いネットワークと同じで学習しやすくなる。
% ゲートの値は[0, 1]の連続値なので、実際には中間的な振る舞いになる。この時の振る舞いも中間的なものになる。
% highway layerの二つのgate function, T and Cよって出力が調整される。
% 恒等変換の出力は層がない状態と等しいのでモデルの実質的な深さを減らす効果がある。
% このゲートの働きによってHighway layerの出力は非線形は変換と恒等変換の中間的なものになる。
% layerの出力が恒等変換であればネットワークの深さが実質的に減少することになる。


%\subsubsection{Binary and Multi-type Classification}
% DNNでのクラス分類は、DNNの各出力を各クラスに対応させることで、クラス数$K$によらずに同じ方法で学習できる。
% 多クラス分類の場合は、我々の実験ではクラス数が$K=3$なので、出力数も3となる。
% 同様にbinaryクラス分類では出力数は2となる。
We can train the DNN classifier in the same way regardless of the number of classes.
In the case of multi-type classification, the number of classes is $K = 3$ in our experiment, so the number of outputs of the DNN classifier is also three.
In binary classification (SN~Ia or Non-SN~Ia), the number of the outputs is two.

% モデルの学習はクロスエントロピー誤差$CE \left( t, y \right) = -\sum_{k=1}^K t_k \log y_k$を最小化するようにした。
% $t$は$k$次元のone hotベクトルで正解のクラス$k$に対応する次元のみが1、残りは全て0のベクトルである。
% $y$はDNNの出力ベクトルである。
% モデルのパラメータの更新は勾配法でadam optimizerを利用して行った。
We train the model by minimizing the cross-entropy error: 
\begin{equation}
\mathrm{CE} \left(y, \hat{y} \right) =　-\sum_{k = 1}^K y_k \log \hat{y}_k,
\end{equation}
where $y$ is a ground truth vector which is one-hot encoding of $K$ dimension, and $\hat{y}$ is the DNN output vector.
We deploy {\it Adam optimizer} which uses a stochastic gradient method to optimize model parameters.

% 学習時のオーバーフィッティングを防ぐためにdata augmentationを行った。
We introduce data augmentation to prevent overfitting at the time of training.
% data augmentationによって見かけのデータ数を増やすことで、DNNが訓練データを暗記してしまうのを防ぐ効果がある。
By increasing the number of input data by augmentation, we prevent DNN from memorizing all of the training dataset.
% 学習データに2種類のdata augmentationを適用した。
We apply two ways of data augmentation to the training dataset.
% 一つはfluxにガウス分布からサンプリングしたノイズを加えた。
One is adding Gaussian noise (based on the expected observed uncertainty) to the simulated flux.
% もう一つはmixup (\cite{mixup})を行った。
The other is mixup (\cite{mixup}).

% ノイズの追加は、以下のように、eq.\ref{eq:scaled_flux}, eq.\ref{eq:abs_mag}において$\hat{f}_i$を$\hat{f}_i + \epsilon_i$で置き換えた。ここで、$\epsion_i \sim \mathcal{N} \left(0, e_i\right)$、$\mathcal{N}\left(\bullet, \bullet\right)$は標準正規分布、$e_i$は$\hat{f}_i$に対応するflux eroorである。
%To add noise to flux, as follows, we replaced $f_i$ with $f_i + \epsilon_i$ in eq.\ref{eq:scaled_flux} and eq.\ref{eq:mag},
%where $\epsilon_i$ is a random noise drawn from the Gaussian distribution with zero mean and $e_i^2$ variance, and $e_i$ is the flux error corresponding to $f_i$.
Mixup generates a new virtual training dataset as follows:
\begin{eqnarray*}
    \tilde{x} &=& \lambda x_s + \left( 1-\lambda \right) x_t, \\
    \tilde{y} &=& \lambda y_s + \left( 1-\lambda \right) y_t,
\end{eqnarray*}
where $\left(x_s, y_s\right)$ and $\left(x_t, y_t\right)$ are samples drawn at random from the training dataset, $x$ is input vector, $y$ is one-hot label vector, 
and a mixing ratio $\lambda \in \left[0, 1\right]$ is drawn from a random distribution which is low density around 0 and 1 and higher density around 0.5. 
The generated datasets are suitable for DNN to learn the classification boundaries.

% 前述のとおり、モデルのハイパーパラメータは探索を行い決定した。
As described above, the hyperparameters (red boxes in Figure \ref{fig:dnn_model}) of the model are optimized by maximizing the accuracy, while 
the model parameters (green boxes in Figure \ref{fig:dnn_model}) are optimized by minimizing cross-entropy error.
% グリッドサーチの設定は表1の通りで、TPEアルゴリズムでの探索の設定は表2のとおりである。
%Table\ref{tb:hp_grid} shows the grid search settings, and Table\ref{tb:hp_search} shows the search settings for the TPE algorithm.
%Table\ref{tb:hp} shows the grid search settings and the search settings for the TPE algorithm.
% われわれはハイパーパラメータ探索によってクラス分類に最適なDNNの大きさを決定した。
% モデルの良さを測る尺度には学習に用いなかったバリデーションデータの精度とした。
% われわれはハイパーパラメータ探索の第一段階として表?の値のデカルト積で作られる各要素についてグリッド探索を行った。
% 続いて表?の範囲についてTPEアルゴリズムで探索を行った。
% We determined the optimal DNN model size for classification by hyperparameter search.
% The scale for measuring the goodness of the model was the accuracy of the validation data, which was not used for training.
% As the first step of hyperparameter search, we performed a grid search for each element created by Cartesian product of the values in Table \ref{tb:hp_grid}.
% Subsequently, we searched in the range in Table \ref{tb:hp_search} using the TPE algorithm.

% DNNの学習は5-fold cross validationで行い、その5つの学習器による予測結果の平均で評価した。
% DNNの予測ベクトルのうちで最も値が高い要素に対応するクラスをDNNが予測したクラスとする。
% DNNの予測したクラスと正解ラベルの一致率で精度を評価する。
% The performance evaluation of the models was performed by 5-fold cross validation.
% Let the class corresponding to the element with the highest value among the prediction vector be the class predicted by DNN.
% The accuracy is evaluated by the matching rate of DNN predicted class and the ground truth.


\subsection{Testing DNN Model with PLAsTiCC Dataset} 
\label{sec:p}

%\subsection{Simulated Data for ML Validation}
%\subsubsection{Simulated Data for ML Evaluation}
%\label{sec:validation}
%To evaluate our machine, we need a dataset which is labled by true identities. 
%In nature, we should expect to have a variety of transients besides SN~Ia, SN~Ibc, and SN~II.
%LSST team has compiled a series of variables including AGNs, variable stars, microlensing events,and class 99 whose identity was not revealed at the time of competition \citep{malz19a,kessler19b}.   
%We take their test dataset as a pseudo-real observed data and assess our accuracy of classification.
%In 2018, the data classification challenge was invoked by Kaggle \citep{malz19a}, called PLAsTiCC, and they simulated a realistic photometric dataset for LSST.
%Inside the PLAsTiCC data, simulated data for Deep Drilling Field is the most similar dataset to HSC survey data, and we extract their 2,297 objects from PLAsTiCC \citep{malz19a} dataset to test our machine.
%Kaggle competition provided us with the training dataset, but the number of objects are too 


%
%We verify the performance of the classifier described in the previous section using the PLAsTiCC dataset \citep{plasticc_dataset}.
Before we apply our model to the HSC observed data, we test it with LSST simulated classification challenge dataset, PLAsTiCC dataset \citep{allam18a,malz19a} which is composed of realistic photometric dataset with errors on time-variable objects.
To evaluate our model, we are in need of dataset with labels of true identity. 
PLAsTiCC Deep Drilling Field (DDF) dataset is similar to HSC-SSP Transient Survey, and we took advantage of it.
However, %our primary purpose is to extract SN~Ia, and 
we generate the training set by ourselves 
%($\S$ \ref{sec:training}) 
and do not use the training set PLAsTiCC team has provided because we knew that the number of training dataset is not good enough to achieve the maximum performance (Figure \ref{fig:size_convergence_test}).

%PLAsTiCC data is mainly divided into two components, Wide-Fast-Deep (WFD) and Deep-Drilling-Fields (DDF), based on survey styles with different cadences.
%WFD data is not suitable for classification by our proposed method, because it consists of a group of light curves simulated assuming a wide area survey of almost half of the whole sky, and have various observation schedules.
%In contrast, DDF data is suitable for the verification of our method because it has photometric data that simulates the observation of specific areas with the same cadence as the HSC survey.
%Therefore, we validated the classifier using DDF data which have the fixed observation schedule for each object.
%A total of 2,297 events extracted from PLAsTiCC DDF data were used for the test set.
%These are three types of SNe, Ia, Ibc, and II, which are simulated to occur in the COSMOS field \citep{scoville2007cosmos}, the target region of the HSC survey.
%For these events, we extracted only the light curves for about one year during which SNe occur from the original three-year data and input them into the classifier.
%
%We prepared multiple classifiers with different inputs for comparison.
%Specifically, there are two types of inputs, magnitude or absolute magnitude, and a total of four types of classifiers including cases where normalized flux is added to the input. 
%Two cases of classification tasks were explored in this study: two-class classification (Ia or non Ia) and three-class classification (Ia, Ibc or II).
%The performance of each classifier is evaluated based on the probabilities of each class, which are the outputs from the classifier, described in section \ref{sec:model}.

%
%\subsubsection{PLAsTiCC data for Validation \mt{(this should be "test")}}
%\label{sec:p}
%\begin{itemize}
%\item Validation using a part of samples with answers
%\item ROC curve and Precision-Recall curve in binary classification(figure)
%\item Confusion matrix in 3 class classification(figure)
%\item Comparison with the 1st model in PLAsTiCC (If possible)
%\end{itemize}
%Whereas we extracted and classified a part of the simulated data for learning in the previous subsection, here we verify the classification performance for PLAsTiCC data.
%We used simulated data for learning, and compared the classification result for PLAsTiCC data with true label.

%A total of 2,297 events extracted from PLAsTiCC DDF data were used for the test set.
%These are three types of SNe, Ia, Ibc, and II, which are simulated to occur in the COSMOS field \citep{scoville2007cosmos}, the target region of the HSC survey.
%For these events, we extracted only the light curves for about one year during which SNe occur from the original three-year data and input them into the classifier.
%Inside the PLAsTiCC data, simulated data for Deep Drilling Field is the most similar dataset to HSC survey data, and we extract their 2,297 objects from PLAsTiCC \citep{malz19a} dataset to test our machine.

%For training, 370,345 light curves are generated for LSST DDF.
%We use filter response including system throughput from \citet{ivezic19a}. 
For training, we generated 370,345 light curves based on the method described in $\S$ \ref{sec:training} using filter response and photometric zero-point for LSST \citep{ivezic19a}.
These light curves are composed of SN types by SN~Ia:Ibc:II=0.60:0.06:0.34, and their peaks are randomly shifted in time.
%In addition, the timing at which SNs occur is shifted by randomly setting an offset that indicates the difference between the SN phase and the observation schedule.
%
For test dataset, we extract 2,297 light curves from PLAsTiCC dataset.
%, and applied our model to it.
These are labeled three types of SNe (Ia, Ibc and II), and simulated to occur in COSMOS.
%, the target region of HSC survey.

We use the area under the curve (AUC) of receiver operating characteristic (ROC) curve, precision-recall curve in two-class classification, and the accuracy from confusion matrix in three-class classification as a metric to evaluate our model.
We test which combination of inputs performs best for PLAsTiCC dataset. Our input could be a combination of arrays of normalized flux ($f$), magnitude ($m$), pseudo-absolute magnitude ($M$).
Tables \ref{tab:p_test} show the AUC in two-class classification and accuracy in three-class classification in PLAsTiCC data, respectively.
We find that the combination of normalized flux ($f$) and pseudo-absolute magnitude ($M$) performs best, although the information is redundant, we suspect the different distribution of the data gives extra clue to the machine.
%As with the case of the simulated data, the classifier with absolute magnitudes and normalized flux provides the best classification results for PLAsTiCC data.
The AUC for the ROC curve and precision-recall curve are 0.996 and 0.995 respectively.
%, and these curves in this model are shown in figure\ \ref{fig:plasticc_2class_ROC} and figure\ \ref{fig:plasticc_2class_PreRec}.
Figure\ \ref{fig:plasticc_3class_CM} shows the confusion matrix in the three-class classification, and the total accuracy is calculated as 95.3\%.
As is always the case in the real world, it is hard to classify SN~Ibc, but the effect on total accuracy is relatively small.
%PLAsTiCC, test(v1.2.1)
%
\begin{table}[htbp]
\tbl{Classification performance of each input for the PLAsTiCC dataset.}{
\begin{tabular}{cccccc}
\noalign{\vskip 2mm}
\hline
\multicolumn{3}{c}{Input$^*$}   & \multicolumn{2}{c}{AUC} & Accuracy \\
\hline
%\begin{tabular}{c}absolute\\magnitude\end{tabular} & magnitude & \begin{tabular}{c}normalized\\flux\end{tabular} &  ROC &  PreRec &\\
$M$ & $m$ & $f$ &  ROC &  Pre.-Rec. &\\
\hline
\checkmark &            & \checkmark &    0.996 &       0.995 &     0.953\\
\checkmark &            &            &    0.995 &       0.993 &     0.952\\
           & \checkmark & \checkmark &    0.995 &       0.993 &     0.948\\
           & \checkmark &            &    0.995 &       0.991 &     0.940\\
\hline
\end{tabular}
}\label{tab:p_test}
\begin{tabnote}
* Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table}
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/06_CM_abs-mag_scaled-flux_w-mixup_predictions_test_2.eps}
  \end{center}
  \caption{%
  Normalized confusion matrix in three-class classification of PLAsTiCC dataset. The input to the classifier is pseudo-absolute magnitude and normalized flux. The proportions in each row sum to 1. The numbers in parentheses represent the raw numbers.
  }%
  \label{fig:plasticc_3class_CM}
\end{figure}
%

In the three-class classification of the PLAsTiCC dataset,
107 SNe is misclassified and have the following characteristics:
\begin{itemize}
\item 45\% (46/107) of them are "incomplete events" that do not include the SN peak phase in the photometric data, while they are only 22\% of all events.
\item Of the rest, 28\% (17/61) are on the boundary where the difference in probability between the correct class and the predicted class is less than 0.1.
\item More than half of the remaining 44 events misclassify Ibc as Ia, II
\end{itemize}
Figure\ \ref{fig:misclass_rate_3class} shows the accuracy against redshift for the PLAsTiCC dataset.
%We found that 
The accuracy for Ibc is lower than that of other classes.
%and that the accuracy dropped significantly in the vicinity of redshift 1.0, and the accuracy also decreased for nearby Ibc SNe.
%The accuracy for SN~Ibc is 
It is greatly reduce at redshifts beyond 1.0, and also decreased at redshifts of 0.1 to 0.2.
As a result of individual check, while some misclassified SNe are so dark and indeterminate that they are difficult to classify even with conventional methods, there are also bright SNe that are completely misclassified.
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/misclass_rate_plastic_3class.eps}
  \end{center}
  \caption{%
  Accuracy against redshift in the three-class classification of the PLAsTiCC dataset.
  }%
  \label{fig:misclass_rate_3class}
\end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to HSC-SSP Transient Survey}
\label{sec:h}
%
We applied the developed classifier to the light curve data of 1824 SNe discovered in HSC-SSP Transient Survey.
As described in \citet{yasuda19a}, the survey %is part of HSC-SSP survey, where observations were performed in two areas with different 
%observation frequencies,
was conducted in two kinds of layers with different depths and cadence, 
"Deep" and "Ultra-Deep".
Therefore, the number of photometric data of SN in each layer is different.
There are also SNe in which part of the light curve data is missing.
To correspond these SNe, we prepared five input cases of light curve data and classifiers for each case.
The number of SNe for each case are summarized in Table\ \ref{tab:class_flag}.
For example, the number of SNe in Case 0 using photometric data of all observation epochs in the survey is 705.
They are detected in the Ultra-Deep layer, and have 42 epochs of photometric data in four bands of $g$, $r2$, $i2$ and $z$ band.
The number of epochs and the schedule for Case 0 SNe are summarized in Table \ref{tab:HSCsurvey_schedule}.
The combination of these classifiers enables the classification of 1812 SNe corresponding to 99.3\% of all target SNe.
The remaining 12 SNe are excluded in this classification because they have "data missing" in all of the schedule patterns from Case 0 to Case 4, and there is no new pattern common to them.

%
%v190829
\begin{table}[htbp]
\tbl{Number of SNe for each pattern.}{
%\begin{tabular}{lrrrr}
%\hline
%Case & Epoch & Number & Cumulate & Cumulate ratio \\
%\hline
%0     & 42      & 705        & 705        & 0.387 \\
%1     & 26      & 651        & 1356       & 0.743 \\
%2     & 19      & 271        & 1627       & 0.892 \\
%3     & 10      & 121        & 1748       & 0.958 \\
%4     & 4        & 64        & 1812       & 0.993 \\
\begin{tabular}{lrrrr}
\noalign{\vskip 1mm}
\hline
Case & Epoch & Number & Fraction \\
\hline
0     & 42      & 705        & 0.387 \\
1     & 26      & 651        & 0.357 \\
2     & 19      & 271        & 0.149 \\
3     & 10      & 121        & 0.066 \\
4     & 4        & 64        & 0.035 \\
\hline
\end{tabular}
}\label{tab:class_flag}
\end{table}
%
%
\begin{table}[htbp]
\tbl{Number of input epochs and the schedule for Case 0 SNe.}{
\begin{tabular}{llp{15em}}
\noalign{\vskip 2mm}
\hline
Filter & Epochs & Elapsed day\\
\hline
$g$ & 8 & 2, 40, 63, 70, 92, 119, 126, 154\\
$r2$ & 9 & 5, 32, 61, 71, 92, 103, 122, 129, 151\\
$i2$ & 13 & 2, 6, 32, 40, 61, 68, 71, 94, 101, 120, 127, 154, 155\\
$z$ & 12 & 0, 6, 30, 40, 59, 68, 90, 101, 119, 126, 151, 157\\
%Y  & 10 & -59, -26, -17, 4, 13, 37, 45, 59, 68, 89 \\
\hline
\end{tabular}
}\label{tab:HSCsurvey_schedule}
\end{table}
%
%
%training set 
We applied both two-class and three-class classification tasks to HSC data, as with PLAsTiCC data.
The following sections ($\S$ \ref{sec:h2} and $\S$ \ref{sec:h3}) describe the Performance evaluation for each classification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Binary Classification}
\label{sec:h2}
%
In binary classification, we prepared four versions of classifiers with different inputs as in PLAsTiCC dataset, and compared their performance.
%The performance for each input of binary classification are summarized in Table\ \ref{tab:h2_validation},\ref{tab:h2_test_gold} and \ref{tab:h2_test_all}.
The performance for each input of binary classification are summarized in Table\ \ref{tab:h2_AUC}.

For the validation dataset, which is a part of the simulated dataset, a higher number of input dimensions results in the better the results, and any classifier can classify them with very high AUCs.
The best AUCs for all classified events are 0.993 and 0.995 for the ROC curve and precision-recall curve, respectively.
%Figure\ \ref{fig:h2_validation} show ROC curves and precision-recall curves in the best performing classifier for the validation set.

For test dataset, the classification performance is verified using 1332 HSC SNe (1256 with redshift) labeled by the SALT2 light curve fitter \citep{guy2007,guy10b}, one of the conventional classification methods.
The verification label for HSC SNe conforms to \citet{yasuda19a}, which defines SN~Ia as SNe that satisfy all the following four criteria for the SALT2 fitting results:
(1)color ($c$), and stretch ($x_1$) within the $3\sigma$ range of \citet{scolnickessler2016} "All G10" distribution, 
(2)absolute magnitude in B band $M_B$ brighter than −18.5 mag, 
(3)reduced $\chi ^{2}$ of less than 10,
(4)number of degrees of freedom (dof) is greater than or equal to 5.
Other candidates that satisfy the looser set of conditions above are labeled "Ia?".
Specifically, the range of (1) is expanded to within 5 sigma, and the thresholds of (2) and (3) are set to -17.5 mag and 20 respectively.
On the other hand, we define non Ia in the HSC classification as a SN that does not satisfy the conditions of "Ia" and "Ia?", and whose number of dof is 5 or more.
In this performance evaluation, only those labeled Ia and non Ia are used.
The numbers labeled Ia and non Ia are 428 and 904 (409 and 847, with redshift), respectively.
Those with the number of dof less than 5 are labeled as "unclassified", which is not classified by SALT2 fit, and are not used for the evaluation.

We also extracted 441 "light curve verified SNe" that have spec-z and photometric information before and after peak, and verified their classification results.
Figures\ \ref{fig:h2_test_all} and \ref{fig:h2_test_gold} show the AUCs of the best classifier for all labeled HSC SNe and the light curve verified HSC SNe respectively.
The confusion matrices for each case are shown in Figure\ \ref{fig:h2_test_CM}.
%need reference
The best performing classifier shows the same classification results as the conventional method in 84.9\% of 1256 labeled SNe, with 91.4\% accuracy for 441 light curve verified SNe.
%The final classification result of the HSC SNe is the combination of outputs from the classifier using the absolute magnitude for those with redshift information, and for that using the magnitude for those without redshift information.
%The numbers of SNe with Ia probabilities above 0.5, 0.7 and 0.9 are 530, 386 and 170 out of 1332, respectively.
%The numbers of SNe with Ia probabilities above 0.5, 0.7 and 0.9 are 613, 437 and 186 out of 1540, respectively. (in old ver.)
%
%
%
%
%2class AUC
%Ver. 1.2.0 
\begin{table*}[htbp]
\tbl{AUC of each input in the HSC binary classification.}{
\begin{tabular}{c|ccc|p{3em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}|p{3em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}}
\noalign{\vskip 1mm}
\hline
Dataset & \multicolumn{3}{c}{Input$^*$} & \multicolumn{6}{|c|}{ROC} & \multicolumn{6}{c}{Pre.-Rec.} \\
\hline
 & $M$ & $m$ & $f$ & Case 0 & 1 & 2 & 3 & 4 & All & Case 0 & 1 & 2 & 3 & 4 & All \\
\hline
Validation &\checkmark &            & \checkmark &       1.000&       0.990 &       0.987 &       0.976 &       0.887 &        0.993 &          1.000 &          0.993 &          0.991 &          0.983 &          0.917 &           0.995 \\
& \checkmark &            &            &       0.999&       0.980 &       0.975 &       0.959 &       0.845 &        0.987 &          0.999 &          0.986 &          0.982 &          0.972 &          0.886 &           0.991 \\
&           & \checkmark & \checkmark &       0.999&       0.983 &       0.979 &       0.963 &       0.817 &        0.988 &          0.999 &          0.988 &          0.985 &          0.973 &          0.863 &           0.992 \\
&           & \checkmark &            &       0.996&       0.971 &       0.966 &       0.938 &       0.790 &        0.980 &          0.997 &          0.980 &          0.976 &          0.955 &          0.841 &           0.985 \\
\hline
Test& \checkmark &            & \checkmark &       0.972 &       0.973 &       0.946 &       0.889 &       1.000 &        0.966 &          0.918 &          0.956 &          0.860 &          0.838 &          1.000 &           0.919 \\
(Light curve verified)& \checkmark &            &            &       0.979 &       0.974 &       0.977 &       0.900 &       0.944 &        0.972 &          0.938 &          0.942 &          0.960 &          0.870 &          0.908 &           0.936 \\
&           & \checkmark & \checkmark &       0.934 &       0.928 &       0.886 &       0.711 &       0.889 &        0.918 &          0.798 &          0.852 &          0.791 &          0.605 &          0.579 &           0.795 \\
&           & \checkmark &            &       0.926 &       0.887 &       0.868 &       0.822 &       0.611 &        0.894 &          0.757 &          0.775 &          0.775 &          0.693 &          0.374 &           0.751 \\
\hline
Test& \checkmark &            & \checkmark &       0.938 &       0.921 &       0.906 &       0.896 &       0.865 &        0.921 &          0.842 &          0.845 &          0.809 &          0.849 &          0.672 &           0.830 \\
(All labeled)& \checkmark &            &            &       0.941 &       0.921 &       0.893 &       0.892 &       0.879 &        0.918 &          0.861 &          0.829 &          0.757 &          0.870 &          0.729 &           0.810 \\
&           & \checkmark & \checkmark &       0.898 &       0.883 &       0.867 &       0.721 &       0.706 &        0.874 &          0.719 &          0.743 &          0.744 &          0.574 &          0.362 &           0.709 \\
&           & \checkmark &            &       0.891 &       0.845 &       0.847 &       0.700 &       0.677 &        0.853 &          0.693 &          0.693 &          0.689 &          0.519 &          0.377 &           0.676 \\
\hline
\end{tabular}
}\label{tab:h2_AUC}
\begin{tabnote}
* Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table*}
\begin{comment}
%
%Validation
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/08_abs-mag_scaled-flux_w-mixup_remove-y_predictions_validation_ROC.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/08_abs-mag_scaled-flux_w-mixup_remove-y_predictions_validation_PreRec.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \caption{%
  ROC curves and precision Recall curves in two-class classification of validation dataset.
}%
    \label{fig:h2_validation}
\end{figure*}
\end{comment}
%
% all samples
%
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_abs-mag_scaled-flux_w-mixup_remove-y_predictions_test_ROC_all.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_abs-mag_scaled-flux_w-mixup_remove-y_predictions_test_PreRec_all.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \vspace{2mm}
    \caption{%
    ROC curves and precision Recall curves in two-class classification of all labeled HSC SNe.
    The input to the classifier is pseudo-absolute magnitude and normalized flux.
    Each colored line represents the performance for each of the five classifiers with different input cases, and the that for all of their outputs.
    }
    \label{fig:h2_test_all}
\end{figure*}
%
%
%spec-z \& w/o incomplete samples
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_abs-mag_scaled-flux_w-mixup_remove-y_predictions_test_ROC_noedge_spec.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_abs-mag_scaled-flux_w-mixup_remove-y_predictions_test_PreRec_noedge_spec.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \vspace{2mm}
    \caption{%
  As in Figure \ref{fig:h2_test_all}, but for the light curve verified HSC SNe. 
  %Recall curves in two-class classification of light curve verified HSC SNe. 
  %The input to the classifier is absolute magnitude and normalized flux.
  %Each colored line represents the performance for each of the five classifiers with different input cases, and the that for all of their outputs.
}%
    \label{fig:h2_test_gold}
\end{figure*}
%
%
\begin{comment}
%2class Test Accuracy
%Ver. 1.2.0
%
%% spec-z & w/o incomplete samples
%
\begin{table*}[htbp]
\tbl{Accuracy of each input in the HSC binary classification.}{
\begin{tabular}{c|ccc|p{2em}p{2em}p{2em}p{2em}p{2em}p{2em}}
\noalign{\vskip 2mm}
\hline
Dataset & \multicolumn{3}{|c}{Input} & \multicolumn{6}{|c}{Accuracy} \\
\hline
 & $M$ & $m$ & $f$  &  Case 0 &  Case 1 &  Case 2 &  Case 3 & Case 4 &  All \\
\hline
Test& \checkmark &            & \checkmark &        0.947 &        0.902 &        0.877 &        0.789 &        0.923 &         0.914 \\
(Light curve verified)& \checkmark &            &            &        0.931 &        0.902 &        0.912 &        0.789 &        0.769 &         0.907 \\
&           & \checkmark & \checkmark &        0.878 &        0.847 &        0.825 &        0.632 &        0.538 &         0.839 \\
&           & \checkmark &            &        0.868 &        0.816 &        0.789 &        0.842 &        0.462 &         0.825 \\
\hline
Test& \checkmark &            & \checkmark &        0.859 &        0.816 &        0.757 &        0.778 &        0.745 &         0.818 \\
(All)& \checkmark &            &            &        0.843 &        0.778 &        0.734 &        0.697 &        0.618 &         0.784 \\
&           & \checkmark & \checkmark &        0.798 &        0.782 &        0.767 &        0.636 &        0.508 &         0.765 \\
&           & \checkmark &            &        0.790 &        0.744 &        0.737 &        0.645 &        0.393 &         0.740 \\
\hline
\end{tabular}
}\label{tab:h2_test_CM}
\end{table*}
\end{comment}
%
%
%2class Test CM
%Ver. 1.2.0
%
%% spec-z & w/o incomplete samples
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_CM_abs-mag_scaled-flux_w-mixup_remove-y_predictions_test_2_Flagall_all.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/10_CM_abs-mag_scaled-flux_w-mixup_remove-y_predictions_test_2_Flagall_noedge_spec.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \caption{%
  Normalized confusion matrices in binary classification of 1256 labeled HSC SNe (left) and 441 light curve verified SNe (right).
  The inputs for both classifications are pseudo-absolute magnitude and normalized flux.
}%
    \label{fig:h2_test_CM}
\end{figure*}
%

In the binary classification of 1256 labeled HSC SNe, 190 of them are misclassified.
The misclassification rate for each case is different, and tends to increase as the number of input dimensions decreases.
Although the rate is 11\% for Case 0, it is 22\% for Case 4.
As with the PLAsTiCC data, incomplete events are the majority of misclassified events in HSC data, accounting for 44\% (83/190) of them.
The second most common cause of misclassification is boundary events with an Ia probability of 40 to 60\%, corresponding to the remaining 28\% (30/107).
%As a result of individually checking the light curves of other misclassified SNe, we found that most of them have outlier values or systematic flux offset in photometric data.
Results of individual check for the light curves of other misclassified SNe show that most of them have outlier values or systematic flux offset in photometric data.
In addition, there are some cases where SALT2 fitting does not work well during labeling.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Multi-type Classification}
\label{sec:h3}
%
In this paper, we show the classification performance only for the validation dataset with three-class classifier, because three type classification is not available for the HSC transients.
%because we are currently labeling more than three classes for all SNe detected in HSC-SSP Transient Survey using conventional light curve fitting.
%It is necessary to write the reason why there is no answer of multi-class classification.
%
% Classification performance of each model (3class, HSC, validation)
The accuracy for each input in the three-class classification of the validation dataset are summarized in Table\ \ref{tab:h3_validation}.
The best accuracy for the validation dataset is 94.0\%.
The confusion matrix of the best classifier is shown in Figure \ref{fig:h3_validation_CM}.
It represents that our classifier has a very high sensitivity to SN~Ia while it is not good at classifying SN~Ibc.

In addition, we describe the predicted classes of actual SNe classified by the three-class classifier.
Figure \ref{fig:hsc3_type_frac_alongz} shows the fractions of each type predicted by the three-class classifier in each redshift from 0.1 to 1.5.
All 1812 classified HSC SNe are used to calculate the fraction.
These SN types are a combination of the outputs from the two classifiers with different inputs depending on the presence of redshift information: (1)pseudo-absolute magnitude and normalized flux, (2)magnitude and normalized flux.
%Figure \ref{fig:h3_class_ratio} shows the predicted class ratio.
%Our machine classifies 1812 SN classes with a ratio of SN~Ia:Ibc:II=0.43:0.06:0.51, which is an integrated value throughout the redshift and includes incomplete sampling of faint SNe.

%
%
%%However, it should be noted that this ratio is calculated including misclassificaion.
%Table\  summarizes the classification results of all the actual SN.
%
%Need to write that "ALL" accuracy is weighted
%
%3class Validation
%Ver. 1.2.1
\begin{table}[htbp]
\tbl{Accuracy of each input in the HSC three class classification for validation dataset.}{
\begin{tabular}{ccc|p{3em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}p{1.8em}}
\noalign{\vskip 2mm} 
\hline
\multicolumn{3}{c}{Input$^*$} & \multicolumn{6}{|c}{Accuracy$^{**}$} \\
\hline
$M$ & $m$ & $f$  &  Case 0 & 1 & 2 & 3 & 4 &  All \\
\hline
\checkmark &            & \checkmark &        0.985 &        0.926 &        0.920 &        0.890 &        0.774 &         0.940 \\
\checkmark &            &            &        0.971 &        0.894 &        0.886 &        0.844 &        0.729 &         0.914 \\
           & \checkmark & \checkmark &        0.970 &        0.907 &        0.897 &        0.860 &        0.724 &         0.921 \\
           & \checkmark &            &        0.952 &        0.871 &        0.861 &        0.818 &        0.701 &         0.892 \\
\hline
\end{tabular}
}\label{tab:h3_validation}
\begin{tabnote}
* Input to classifier is displayed as check mark. $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.

** %Accuracy for samples extracted from each case according to the number ratio in the actual HSC dataset.
Accuracy of samples extracted from each case according to the fractions in Table \ref{tab:class_flag}.
\end{tabnote}
\end{table}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/13_CM_abs-mag_scaled-flux_w-mixup_remove-y_predictions_validation_2_Flagall_weighted.eps}
  \end{center}
  \caption{%
  Normalized confusion matrix for validation dataset in the HSC three-class classification.
  The input is pseudo-absolute magnitude and normalized flux.
  The proportions in each row sum to 1 (within rounding error).
  }%
  \label{fig:h3_validation_CM}
\end{figure}
%
%
\begin{comment}
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/14_piechart_hybrid.eps}
  \end{center}
  \caption{%
  Type fractions in the three-class classification of 1812 HSC SNe.
  The numbers in parentheses represent the raw numbers.
  }%
  \label{fig:h3_class_ratio}
\end{figure}
%
\end{comment}
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/SNfrac_alongz.eps}
  \end{center}
  \caption{%
  Type fractions along redshift in HSC three-class classification.
  }%
  \label{fig:hsc3_type_frac_alongz}
\end{figure}
%



%
\subsection{Classification of HSC SNe}
%
We here publish the classification results of 1824 HSC SNe by the proposed classifiers at (url).
A part of this classification list is shown in Table\ \ref{tab:h_results} as an example.
This list summarizes the probabilities predicted by the two-class and three-class classifiers for each SN, along with redshifts of host galaxies and classification labels by SALT2 fit.
The probabilities in this list are calculated from the output of the classifier with the normalized flux added to the input.
Each classification performance shown in $\S$ \ref{sec:h2} and $\S$ \ref{sec:h3} is calculated based on the probabilities in this list.
\begin{table*}[htbp]
\tbl{Example of classification result list for HSC SNe.}{
%\tiny
\scriptsize
\begin{tabular}{p{4em}p{1em}p{4.0em}p{2.1em}|p{0.6em}p{4.0em}p{4.5em}|p{2.9em}|p{1.2em}p{1.2em}p{1.2em}p{0.6em}|p{2.9em}|p{1.2em}p{1.2em}p{1.2em}p{0.6em}}
\noalign{\vskip 1mm}
%\multicolumn{6}{|c}{Accuracy}
\hline
Name  &  Case &        z &  z\_src$^*$ &  \multicolumn{3}{p{10.0em}}{SALT2 fitting} &\multicolumn{5}{|p{13.5em}}{Classifier (Input$^{**}$: $M+f$)} & \multicolumn{5}{|p{12.0em}}{Classifier (Input$^{**}$: $m+f$)}\\
\hline
      &       &          &          &  dof & label1 & label2  & 2-class &\multicolumn{4}{p{10.0em}|}{3-class}&2-class &    \multicolumn{4}{p{10.0em}}{3-class}\\
\hline
      &       &          &          &      &        &        &    Ia &    Ia &   Ibc &    II  &class &   Ia &    Ia &   Ibc &    II &class\\
\hline
HSC16aaau &     1 &    $0.370_{-0.072}^{+0.110}$ &         3 &    7 &    Ia? &   incomplete &    0.399 &    0.414 &    0.041 &    0.545 &      II &    0.401 &    0.447 &    0.013 &    0.541 &      II \\
HSC16aaav &     1 &    $3.280_{-2.423}^{+0.167}$ &         4 &   17 &  nonIa &     OK &    0.157 &    0.074 &    0.001 &    0.925 &      II &    0.269 &    0.314 &    0.168 &    0.518 &      II \\
HSC16aabj &     0 &    $0.361_{-0.008}^{+0.007}$ &         2 &    8 &  nonIa &   incomplete &    0.641 &    0.680 &    0.010 &    0.310 &      Ia &    0.574 &    0.583 &    0.020 &    0.396 &      Ia \\
HSC16aabk &     1 &      nan &         0 &    9 &    Ia? &   incomplete &      nan &      nan &      nan &      nan &     NaN &    0.421 &    0.445 &    0.317 &    0.237 &      Ia \\
HSC16aabp &     1 &    $1.477_{-0.032}^{+0.037}$ &         2 &   19 &  nonIa &   incomplete &    0.965 &    0.972 &    0.001 &    0.027 &      Ia &    0.882 &    0.888 &    0.033 &    0.079 &      Ia \\
\vdots & & & & & & & & & & & & & & & &\\
HSC17bjrb &     1 &    $0.560_{-0.036}^{+0.024}$ &         3 &    1 &  unclassified &   incomplete &    0.004 &    0.005 &    0.006 &    0.988 &      II &    0.009 &    0.003 &    0.004 &    0.993 &      II \\
HSC17bjwo &     0 &    $1.449_{-0.063}^{+0.080}$ &         2 &   26 &     Ia &     OK &    0.893 &    0.919 &    0.003 &    0.078 &      Ia &    0.890 &    0.933 &    0.006 &    0.061 &      Ia \\
HSC17bjya &     0 &    $1.128_{-0.000}^{+0.000}$ &         1 &   22 &  nonIa &     OK &    0.241 &    0.191 &    0.070 &    0.738 &      II &    0.311 &    0.177 &    0.083 &    0.740 &      II \\
HSC17bjyn &     0 &    $0.626_{-0.000}^{+0.000}$ &         1 &   24 &     Ia &     OK &    0.881 &    0.881 &    0.028 &    0.091 &      Ia &    0.961 &    0.921 &    0.007 &    0.072 &      Ia \\
HSC17bjza &     1 &    $1.350_{-0.156}^{+1.142}$ &         4 &   13 &  nonIa &     OK &    0.012 &    0.030 &    0.023 &    0.947 &      II &    0.060 &    0.040 &    0.004 &    0.956 &      II \\
HSC17bkbn &     0 &    $0.863_{-0.012}^{+0.036}$ &         2 &   23 &  nonIa &     OK &    0.010 &    0.009 &    0.000 &    0.991 &      II &    0.013 &    0.010 &    0.002 &    0.988 &      II \\
HSC17bkcz &     0 &    $0.795_{-0.000}^{+0.000}$ &         1 &   27 &     Ia &     OK &    0.666 &    0.656 &    0.050 &    0.293 &      Ia &    0.637 &    0.767 &    0.022 &    0.211 &      Ia \\
HSC17bkef &     0 &    $2.940_{-0.087}^{+0.119}$ &         2 &    0 &   fail &    NaN &    0.237 &    0.446 &    0.000 &    0.554 &      II &    0.942 &    0.935 &    0.012 &    0.052 &      Ia \\
HSC17bkem &     2 &    $0.609_{-0.000}^{+0.000}$ &         1 &   17 &     Ia &     OK &    0.906 &    0.873 &    0.001 &    0.126 &      Ia &    0.904 &    0.869 &    0.022 &    0.108 &      Ia \\
HSC17bkfv &     0 &    $0.670_{-0.035}^{+0.035}$ &         3 &   23 &     Ia &     OK &    0.909 &    0.920 &    0.004 &    0.076 &      Ia &    0.943 &    0.934 &    0.020 &    0.047 &      Ia \\
\vdots & & & & & & & & & & & & & & & &\\
HSC17dskd &     0 &    $0.630_{-0.000}^{+0.000}$ &         1 &    3 &  unclassified &   incomplete &    0.882 &    0.855 &    0.081 &    0.063 &      Ia &    0.859 &    0.858 &    0.086 &    0.055 &      Ia \\
HSC17dsng &     0 &    $1.331_{-0.048}^{+0.048}$ &         2 &    7 &    Ia? &   incomplete &    0.964 &    0.963 &    0.009 &    0.029 &      Ia &    0.939 &    0.906 &    0.010 &    0.084 &      Ia \\
HSC17dsoh &     0 &    $1.026_{-0.000}^{+0.000}$ &         1 &    2 &  unclassified &   incomplete &    0.967 &    0.970 &    0.012 &    0.018 &      Ia &    0.901 &    0.918 &    0.024 &    0.058 &      Ia \\
HSC17dsox &     0 &    $1.137_{-0.034}^{+0.041}$ &         2 &    2 &  unclassified &   incomplete &    0.769 &    0.787 &    0.156 &    0.057 &      Ia &    0.899 &    0.898 &    0.027 &    0.076 &      Ia \\
HSC17dspl &     0 &    $0.624_{-0.000}^{+0.000}$ &         1 &    9 &  nonIa &   incomplete &    0.095 &    0.048 &    0.106 &    0.846 &      II &    0.074 &    0.054 &    0.078 &    0.868 &      II \\
\hline
\end{tabular}
}\label{tab:h_results}
\begin{tabnote}
* Code for redshift source.
1: spec-z, 2: COSMOS photo-z, 3: HSC photo-z ultra-deep, 4: HSC photo-z deep, 0: hostless.

** $M$: pseudo-absolute magnitude, $m$: magnitude, $f$: normalized flux.
\end{tabnote}
\end{table*}
%
%
\subsection{Dependence on the Number of Epochs}
%
When using our classification method, the number of photometric data points given to the classifier increases as the survey progresses.
Therefore, we investigated the transition of performance against the number of epochs.
We classified the HSC dataset by increasing the number of input data points one by one, and examined the relationship between the number of epochs and the classification performance.
Binary classifiers are used for classification, and the accuracy calculated from each confusion matrix is used for evaluation.
Figure\ \ref{fig:n_observations} shows the transition of classification performance along with the number of epochs.
Although the Ia accuracy is as low as 0.6 to 0.7 in the early stage of the survey with less than five epochs, it exceeds 0.8 when the number of epochs increases to 20.
The partial decrease in accuracy is thought to be due to the new SN found at the added photometric point.

We also investigated the classification performance during each SN phase by regrouping all events according to the length of period since the first detection.
%examples of probability
Figure\ \ref{fig:lcps} illustrates the light curves and Ia probability transitions since the first detection of three HSC SNe.
We define "first detection" as the first day when the SN is detected with 5$\sigma$ confidence in flux, and flagged as a real object by the real-bogus classifier using convolutional neural network \citep{yasuda19a}.
The probability is updated at each new epoch.
While there are events where the probability increases as the SN phase progresses, there are also some events where the probabilities fluctuate around 0.5 even as the observation progresses and cannot be clearly classified.
Figure\ \ref{fig:n_observations_SNphase} shows the relationship between the number of days from first detection to classification, accuracy and the cumulative number of SNe classified as Ia with high probability.
The calculations for each performance are based on the classification results for different input epoch numbers of 470 Case 0 SNe observed before the rising phase.
This figure presents how long SN photometric data is needed to classify with high accuracy using our classifier.
The classification performance is low if only the initial rising phase before the SN peek is included in the input photometric data, but the accuracy increases to 85\% when we input two weeks of SN data from the first detection.
%Since the observation interval in the HSC survey is not constant, the number of inputs is different even for SNe with the same detected period.
%Due to this fact, there is a variation in the average input dimension number of samples in each phase, which causes a fluctuation in accuracy.
However, due to events whose probabilities fluctuate with the progress of the SN phase illustrated in Figure \ref{fig:lcps}, there are some cases where the accuracy slightly decreases even if the period until classification becomes long.
%In addition, considering the cumulative number of SNe classified as Ia with high probability from six months data, it is estimated that our classifier can classify at least four or more Ia SNe candidate to follow-up in one month observation of the Ultra-deep area.
In addition, the number of follow-up SNe classified by the classifier can be estimated from the cumulative number in Figure \ref{fig:n_observations_SNphase}.
Using data within one month from the first detection, 28 SNe with z $>$ 1 are classified with Ia probability of 95\% or more.
Since the number of these SNe is a cumulative number for six months, dividing this by six corresponds to the number of follow-up SNe classified during a one-month survey, which is 4 to 5 events.

Lastly, we studied the transition of the Ia probability output by the classifier along SN phase.
Each time the SN photometric data input to the classifier increases, Ia probability, the output of the classifier, is updated.
For each of 100 SNe labeled Ia and non-Ia with SALT2 fitting, we extracted its Ia probability for each duration from the various classification results, and plotted its transition as Figure\ \ref{fig:visualized_Ia_prob}.
Although the Ia probability fluctuates greatly in the earliest phase, it begins to divide in about two weeks, and it can be clearly separated in one month.
%Figure \ref{fig:HSTIaprob} shows the transitions of Ia probability along the days from the peak for 16 SNe included in Case 0 data, among 26 SNe selected as HST targets in the HSC survey, and the average of these transitions.
Figure \ref{fig:HSTIaprob} shows the transitions of Ia probability along the days from the peak for 16 SNe selected as HST targets in the HSC survey, and the average of these transitions.
The Ia probability for the average of the candidates is greater than 0.8 two weeks before the peak.
This means that our classifier accomplishes the task described in $\S$ \ref{sec:tasks} to find good candidates only with information before the SN peak.
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/n_observations_v2.eps}
  \end{center}
  \caption{%
  Relationship between the number of epochs and classification performance in binary classification for HSC dataset. 
  The horizontal axis is the elapsed days of the HSC survey, and the vertical dot line shows the scale of input photometric point number. 
  The color of each mark in accuracy indicates the band of the added photometric point.
  }%
  \label{fig:n_observations}
\end{figure}
%
%
\begin{figure*}[htbp]
    \begin{tabular}{ccc}
        \begin{minipage}{0.33\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/lcp_aqfi.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.33\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/lcp_apsw.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.33\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/lcp_aboi.eps}
            \end{center}
        \end{minipage}
    \end{tabular}
    \vspace{3mm}
    \caption{%
    Examples of light curves and probability transitions. The title of each plot shows the name of the SN in the HSC survey and the label classified by SALT2 fitting.
    }%      
    \label{fig:lcps}
\end{figure*}
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/n_observations_SNphase_v200206.eps}
  \end{center}
  \caption{%
  Classification accuracy and cumulative number of type Ia SNe classified with high probability against SN phase.
  The orange line indicates the cumulative number of SNe with Ia probability $>$ 95\%, and green line is that for distant SNe at z $>$ 1. %Average number of observation points with red lines.
  }%
  \label{fig:n_observations_SNphase}
\end{figure}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/n_observations_visualized_Ia_probability_200206.eps}
  \end{center}
  \caption{%
  Transition of Ia probability with increasing elapsed time of input SNe.
  Each line corresponds to one of SNe.
  The difference in color indicates the difference in label, with red being Ia and blue being non Ia.
  }%
  \label{fig:visualized_Ia_prob}
\end{figure}
%
%
\begin{figure}[htbp]
  \begin{center}
     \includegraphics[width=\columnwidth]{figures/HST_DaysFromPeak_vs_IaProbability_200108.eps}
  \end{center}
  \caption{%
  Ia probability transitions of 16 HST targets. Each of the lighter lines represents the variation for the individual HST targets, and the red line is those average.
  }%
  \label{fig:HSTIaprob}
\end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%
\subsection{Performance of Classifier}
%\subsection{Simulated data versus observed data}
%
In this paper we not only evaluate our classifier with simulated data, but also apply it to actual HSC survey data to evaluate classification performance.
What we find out by classifying actual data with our method is that the performance difference between the validation and test set is larger than that for the PLAsTiCC data.
One possible reason is the uncertainty in the labeling of real data using conventional methods.
In the confusion matrix for all labeled SNe in Figure \ref{fig:h2_test_CM}, 44\% of the 190 misclassified SNe are incomplete events.
%As with the classification of the PLAsTiCC data, many misclassified incomplete events, which should be only 27\% in all classified SNe, suggest that they reduce the performance of the classifier.
They are only 27\% of all classified SNe.
The high percentage of incomplete events among misclassified events suggests that they reduce classification performance. %, as with classification of PLAsTiCC data.
However, the misclassification rate of incomplete events is clearly increased to 6\% (83/1256) in HSC data compared to 2\% (46/2297) in PLAsTiCC data.
SALT2 fitting is also not good at fitting incomplete events due to its specification, and errors in fitting parameters are significantly larger.
These facts suggests the existence of uncertainty in labeling with conventional classification methods.
Another source of uncertainty in labeling is due to the uncertainty of redshift information used in SALT2 fitting.
In at least nine of the other misclassified events that have no other misclassification factors, the fitting determined color parameters are far outside the criteria for Ia, and the wrong redshift is probably used for fitting.
In fact, as shown in the right panel of Figure \ref{fig:h2_test_CM}, the performance for light curve verified SNe, except for incomplete events and whose spec-z is known, is better than for all classified events.

Another cause of the large difference in the performance of the validation and test set is the inability to completely simulate the observed data.
Outlier values and systematic flux offsets, which are considered to be one of the causes of misclassification described in $\S$ \ref{sec:h2}, are found only in observed data.
As described in \citet{yasuda19a}, the photometric data of the SN in the HSC survey is measured from the difference image obtained by subtracting the reference image from the observed image.
We believe that the unsimulated residual in this subtraction creates a difference between the photometric data for training and observation, and increases the misclassification rate of the classifier in the actual data.
In fact, for the SN HSC16aboi in the right panel of Figure \ref{fig:lcps}, it can be seen that flux offsets exist in the decline phase after the peak, and misclassification occurs in that part.
It will be necessary to reduce or simulate these outlier values as much as possible to improve the performance of the classifier.
%The presence of anomalous events also reduces the performance of actual survey classifications.
%
%Of course, the labeling of the actual data class by the conventional method includes uncertainty, but the difference in the results should be minimized by making the learning data more similar to the actual data.
%
%
%\subsection{Comparison with the best classifier in PLAsTiCC}
\subsection{Comparison with Other Classifiers}
%
When comparing the classification results of the classifiers, it is difficult to make a direct comparison if there are differences in the method and number of training sets used, the number of input dimensions and output classes. 
Here, we simply compare the recent SN type classifier based on machine learning and our classifier with AUC of ROC in the binary classification of Ia or not.
For comparison, we use the classification results of the simulated SN light curves with redshift information from \citet{Lochner_2016}, \citet{charnock17a} and \citet{Muthukrishna_2019}.
\citet{Lochner_2016} obtained AUC of 0.984 by classification with boosted decision trees (BDTs) using the SALT2 fitting parameters as input features.
\citet{charnock17a} reported AUC of 0.986 for classification of SNPCC data using deep recurrent neural networks (RNN) with unidirectional long short-term memory (LSTM) units.
\citet{Muthukrishna_2019} used deep RNN with Gated Recurrent Units (GRUs) to classify simulated ZTF light curves and archived an AUC of 0.99 40 days after the trigger.
The AUC of our classifier, 0.996, is is more than comparable to those of these recent classifiers in binary classification.

Next, we compare the results of our classifiers and those ranked first place in the PLAsTiCC Kaggle competition \citep{malz19a}.
The best classifier in the competition \citep{boone19a} is based on Light-GBM and trained with features extracted from photometric data modeled by Gaussian process regression.
The training set of this classifier includes a total of 591,410 light curves by augmenting 100 new light curves under different observation conditions and different redshifts for each light curve of the original PLAsTiCC training set.
It classifies events into 15 classes including variable objects other than SN such as microlensing events and active galactic nuclei.
We use 2,297 PLAsTiCC SN predictions, classified by both classifier, for comparison.
Since the number of classified classes is different between our classifier and the best classifier, we divide the classification results into Ia and the other, and compare as binary classification.
Figure\ \ref{fig:comp_plasticc_1st} shows the confusion matrix of each classifier.
While our classifier and PLAsTiCC 1st classifier cannot be strictly compared because of the different training datasets and number of classification classes,
%only for the compared SNe, our classifier is about 6\% better in total accuracy than the PLAsTiCC 1st classifier and 6\% better in Ia accuracy.
our classifier has a comparable capability to the PLAsTiCC 1st classifier.
%Considering that our classifier only uses the middle one year data out of the original three year data as input, it classifies PLAsTiCC data with better accuracy with less data than that of PLAsTiCC 1st.
However, since our method fixes the observation schedule to input, it is impossible to apply to all PLAsTiCC data with one classifier.
%How many patterns
Our method is not useful for surveys that sweep a wide area like LSST survey, but for surveys that keep observing at the same field of view like HSC-SSP Transient Survey.
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/07_CM_PLAsTiCC-1st_submission_aug22_2class_2.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/03_CM_abs-mag_scaled-flux_w-mixup_predictions_test_2.eps}
            \end{center}
        \end{minipage}
    \end{tabular}  \caption{%
    Normalized confusion matrices of two-class classification results for the PLAsTiCC test set by \citet{boone19a} (left) and our best classifier given pseudo-absolute magnitude and normalized flux (right).
    }%                                                                                           
    \label{fig:comp_plasticc_1st}
\end{figure*}
%
%
%
%
\subsection{Data Uncertainty}
%
The flux error information is also important for the classification of distant Ia, which is the main target of the HSC survey and has a small signal-to-noise ratio.
Here, we discuss how to incorporate errors for classification, including our method.
\citet{charnock17a} adds the flux error itself as part of the input vector.
In the case of feature-based classification, the flux error is used in fitting for calculating the features, and affects those errors.
In addition, the flux error is used to calculate statistical features such as "Standard Deviation / Mean", which is one of the features shown in \citet{narayan18a} and \citet{Muthukrishna_2019}.
On the other hand, as described in $\S$ \ref{sec:training}, we calculate the flux error of the simulated training data based on the relationship between the flux and the flux error for each epoch of the observed data.
Then, random numbers that follow a normal distribution defined by the original fluxes and flux errors are input to the classifier as processed fluxes.

The processing of time series data with uncertainty is an issue in many application domains, not only in the astronomy field.
For example, in similarity matching of time series data with uncertainty, it is suggested as a promising method to take into account continuous correlation in time series such as measuring distance after filtering by Uncertain Moving Average (UMA)\citep{Dallachiesa_2012}.
UMA filtering uses the correlation of neighboring points and reduces the contribution of observation points with large errors.
We believe that the development of a method to efficiently incorporate such error information is necessary for improving the classification performance.
%
\subsection{Missing Data}
%
Our method is specialized in the HSC survey in which each observed transient tends to have the same cadence by observing a certain region deeply.
It can use all photometric information and classify with only a small amount of pre-processing, compared to the method of reducing dimensions and extracting features. 
However, this method is not good at dealing with small differences in observation schedules for each classification object, and missing data.
In other classification methods, missing data is interpolated by linear or Gaussian processes \citep{Lochner_2016,Muthukrishna_2019}, or replaced with reference to the flux of adjacent observation points \citep{charnock17a}.
In the case of the HSC survey, most of all samples can be roughly classified into two types of observation schedules corresponding to Ultra-Deep and Deep layer.
As shown in Table \ref{tab:class_flag}, 99.3\% of SNe are classified by only five types of classifiers with different numbers of inputs, including those for samples whose input epochs are reduced due to missing data.
%To classify all objects with our method, we need to prepare multiple classifiers.
%In order to support ultra-wide-range surveys such as LSST, it is necessary to input features that are less affected by the magnitude of the cadence for each transient, and to add processing to interpolate missing values.
%
%
\subsection{Type Fractions of HSC SNe}
The elemental abundance of the solar system \citep{grevesse98a} originates from the cosmic history of SNe \citep{maraston05a,kobayashi00a}.  
Recent studies show that the solar abundance pattern is observed in other systems \citep{ramirez09a} and clusters \citep{mernier18a}. 
It is important to investigate the origin of elements in the context of cosmic evolution \citep{fukugita04a}. 

It is now well established that star formation rate is peaking at z$\sim$2, and the chemical composition of our system is a mixture of SN~Ia and Core Collapse SNe \citep{tsujimoto95a,kobayashi11a}.
Deriving SN rate needs a careful analysis \citep{dilday08a,brown19a,frohmaier19a} and it is beyond the scope of this paper, but at least we can check the consistency with previous work on relative fraction.
Lick Observatory Supernova Survey \citep{li11a} reports the relative ratio between SN~Ia:Ibc:II to be 0.24:0.19:0.57, while we have 0.24:0.17:0.59 at z$\lesssim$0.2. 

Based on our survey depth, we have a complete sampling of SN~Ia up to z$\sim$1.1 while we lose
completeness of core collapse SN in much lower redshift given the fact that the magnitudes of SN~Ibc and SN~II are fainter by 2$\sim$3 mag at maximum.
The trend of SN~II fraction drops while SN~Ia fraction rises towards high redshift as shown in Figure \ref{fig:hsc3_type_frac_alongz}. 
It is simply because of this completeness effect due the the magnitude difference and does not reflect the cosmological SN rates.
If we adopt the SN~Ia rate from \citet{graur14a} and Core-Collapse SN rate from \citet{strolger15a}, we can estimate the completeness of Core-Collapse SN. 
At z$\sim$0.3 Core-Collapse SN completeness is 72\%, and it drops to be 28\% at z$\sim$0.5. 
The reason why SN~II fraction does not go to zero at z$\sim$1 in Figure \ref{fig:hsc3_type_frac_alongz} is that the magnitude dispersion of Core-Collapse SN \citep[$\sigma$ $\sim$ 1.2 mag]{li11a,kessler19b} is much larger than that of SN~Ia \citep[$\sigma$ $\sim$ 0.5 mag]{rubin15a}, and they are factor of 4-5 more abundant at z$\sim$1 \citep{madau98a,hounsell18a}. 
More careful investigation is needed, but we deduce completeness of Core-Collapse SN at z$\sim$1 is 16\%.

%Cosmological origin of elements depends of the cosmic history of SNe.
%show solar abundance 
%
%
%
\section{Conclusions}
%\begin{itemize}
%\item 
%\item 
%\item our new method(2DGP+LGBM)
%\end{itemize}
%
In this paper, we present a model of a classifier that classifies SN types by directly inputting photometric data and its classification performance.
Our DNN classifier is trained with simulated SN photometric data and classifies PLAsTiCC data and HSC survey actual SN data with high accuracy of 95.3\% and 0.84.9\% respectively.
The study on the number of input dimensions also shows that our classifier can classify the HSC survey data with sufficient accuracy even with only pre-peak data about two weeks after the first detection.
From these facts, we conclude that this classifier has sufficient classification performance for subsequent type-specific studies and selection of follow-up targets even in actual HSC surveys.
%Furthermore, our proposed method is easy to apply to estimation models such as redshift because of simple input. The application of our model to redshift estimation is useful for the selection of host galaxies by comparing with the photo-z information of galaxies.
%
\begin{ack}
The Hyper Suprime-Cam (HSC) collaboration includes the astronomical communities of Japan, Taiwan, and Princeton University. The HSC instrumentation and software were developed by the National Astronomical Observatory of Japan (NAOJ), the Kavli Institute for the Physics and Mathematics of the Universe (Kavli IPMU), the University of Tokyo, the High Energy Accelerator Research Organization (KEK), the Academia Sinica Institute for Astronomy and Astrophysics in Taiwan (ASIAA), and Princeton University. Funding was contributed by the FIRST program from the Japanese Cabinet Office, the Ministry of Education, Culture, Sports, Science and Technology (MEXT), the Japan Society for the Promotion of Science (JSPS), the Japan Science and Technology Agency (JST), the Toray Science Foundation, NAOJ, Kavli IPMU, KEK, ASIAA, and Princeton University.
This paper makes use of software developed for the Large Synoptic Survey Telescope. We thank the LSST Project for making their code available as free software at  http://dm.lsst.org
The Pan-STARRS1 Surveys (PS1) have been made possible through contributions of the Institute for Astronomy, the University of Hawaii, the Pan-STARRS Project Office, the Max-Planck Society and its participating institutes, the Max Planck Institute for Astronomy, Heidelberg and the Max Planck Institute for Extraterrestrial Physics, Garching, The Johns Hopkins University, Durham University, the University of Edinburgh, Queen’s University Belfast, the Harvard-Smithsonian Center for Astrophysics, the Las Cumbres Observatory Global Telescope Network Incorporated, the National Central University of Taiwan, the Space Telescope Science Institute, the National Aeronautics and Space Administration under Grant No. NNX08AR22G issued through the Planetary Science Division of the NASA Science Mission Directorate, the National Science Foundation under Grant No. AST-1238877, the University of Maryland, and Eotvos Lorand University (ELTE) and the Los Alamos National Laboratory.
%
IT, NS and NY acknowledge financial support from JST CREST (JPMHCR1414). NY is supported by the Grant-in-Aid for Scientific Research program of MEXT (18H04345). NS is supported by JSPS (18K03696).
%
This research is based in part on data collected at the Subaru Telescope and retrieved from the HSC data archive system, which is operated by the Subaru Telescope and Astronomy Data Center at NAOJ.
\end{ack}
%
%
\appendix 
\section*{Redshift Estimation}
%\subsection{Estimating Redshift}
\label{sec:est_redshift}
% 観測した明るさからその天体のredshiftを推定した
% DNNの出力がスカラーになって最後のsoftmat layerがないことを除けば、図2と同じ構造のモデルを利用した。
% モデルの最適化は正解のredshiftの値と出力値の二乗誤差を小さくするようにした。
% モデルのハイパーパラメータについてはクラス分類の時と同様に行った。
% 表?の値のデカルト積で作られる要素をグリッドサーチ、続いて表?の範囲の中をTPEアルゴリズムで探索を行った。
% ハイパーパラメータの最適化は$R^2$決定係数を最大にするようにした。
In HSC-SSP transient survey, there are SNe without redshift because their host galaxies are not clearly identified, called hostless SNe.
In the redshift list of HSC SNe used for type classification, 6\% (108/1824) correspond to them.
To cope with such SNe and the possibility of host galaxy misidentification,
we can estimate the redshift $z$ of a SN from the photometric data.
%There exists cases that we may not have a photometric redshift (hostless supernvoae) or photometric redshift is not accurate.
%For these cases, we can estimate the redshift $z$ of a SN from the observed brightness.
%We cannot use absolute magnitude to estimate the redshift value because redshift value is required to calculate absolute magnitude.
%Therefore, we used magnitude instead of absolute magnitude as the input of the DNN.

We use a model with the same structure as in Figure \ref{fig:dnn_model} except that the DNN output is a scalar, and place no final softmax layer for redshift estimation.
The objective function to optimize is the squared error between the ground truth $z$ and the output value $\hat{y}$.
We measure the accuracy of the model with the coefficient of determination $R^2$, that is,
\begin{eqnarray*}
    R^2 = 1 - \frac{\sum_n \left| z_n - \hat{y}_n \right|^2}{\sum_n \left| z_n - \bar{z} \right|^2}, 
\end{eqnarray*}
where $z_n$ is the redshift value of $n$-th sample, $\hat{y}_n$ is the output of $n$-th sample, 
and $\bar{z}$ is the mean redshift of the dataset.
% $\mathrm{SE} \left(z, y\right) = \frac{1}{2} \left| z - y \right|^2 $
We perform the same hyperparameter search in the model as in $\S$ \ref{hyperparametersearch} 
including the data augmentation.
The accuracy of this estimator is discussed below.
%The element created by the Cartesian product of the values in Table \ref{tb:hp_grid_redshift} was searched by grid search, and then the range in Table \ref{tb:hp_search_redshift} was searched by the TPE algorithm.
%The search values, ranges and candidates are surmalized in Table \ref{tb:hp_redshift}.
% 学習時にfluxにノイズを加えるdata augmentationを行った。一方で、クラス分類の場合とは異なりmixupは行っていない。
%We performed data augmentation that adds noise to the flux in training time.
%On the other hand, unlike the case of classification, no mixup is performed.

%\subsection{Redshift estimation}
%using v1.1.0 results
%
%In addition to the SN type classification,
%we also made a redshift estimate using same training dataset for type classification.
%
%As described in section 4.2.2, for SNe without redshift information, we can also estimate their redshift using a model that has largely the same structure as the type classifier.
\begin{comment}
In HSC-SSP transient survey, there are SNe without redshift because their host galaxies are not clearly identified.
They are called hostless SNe.
In the redshift list of HSC SNe used for type classification, 6\% (108/1824) correspond to them.
To cope with such SNe and the possibility of host galaxy misidentification,
we also apply the model described in $\S$ \ref{sec:est_redshift}, which estimates redshift directly from photometric data of SNe.
The training and input data are common to the type classification.
In this section, we discuss the accuracy of this estimator.
\end{comment}

We apply the standard deviation of normalized residual $(z_{\rm pred}-z_{\rm spec})/(1+z_{\rm spec})$ \citep{Salvato_2009,Salvato_2019}, used in galaxy photo-z estimation, to the performance criteria for redshift estimation.
In verification with Case 0 simulated data, that for Ia and non Ia objects are 0.022 and 0.076, respectively.
For the actual HSC SNe, we used observationally obtained redshifts of host galaxies (see section \ref{sec:h}) including spec-z and photo-z for comparison with estimates.
Figure\ \ref{fig:redshift_estimation} shows the comparison between the estimated redshifts and the observed redshifts for the HSC SNe classified as Ia.
The two classes labeled by SALT2 fitting have different distributions, and the events labeled non Ia are less accurate in redshift estimation than Ia.
The normalized residuals for each of Ia and non Ia Case 0 SNe labeled with SALT2 fitting are normally distributed with standard deviations of 0.066 and 0.138, respectively.
If the comparison is limited to SNe in the host galaxy with spec-z information, they are 0.056 and 0.130.
Although these estimation accuracy is worse than the template fitting accuracy using host galaxy photometric data, it is useful not only for hostless SNe, but also to select host galaxy by comparing the redshift estimated from the SN light curve itself with those of galaxies.
Furthermore, this distributional difference of residuals leads to the fact that the Ia accuracy can be further improved by excluding events with large residuals in the estimated redshift of the SN and host galaxy.
%
\begin{figure*}[htbp]
    \begin{tabular}{cc}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/redshift_pred_Ia_w_true_label_flagall.eps}
            \end{center}
        \end{minipage}
        \begin{minipage}{0.5\hsize}
            \begin{center}
                \includegraphics[width=\columnwidth]{figures/redshift_pred_Ia_w_true_label_diff_flag0.eps}
            \end{center}
        \end{minipage}
    \end{tabular}  \caption{%
    (Left) Relation between the machine predictions and the observed redshifts for HSC SNe.
    The dashed line is the line of equality.
    The color difference represents SALT2 fit label.
    (Right) Normalized residual distribution for Case 0 SNe.
    }%
    \label{fig:redshift_estimation}
\end{figure*}

%
\bibliographystyle{myaasjournal}
\bibliography{hsc,archive}
%


\end{document}
